{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to the NOMAD onboarding workshop \u00b6 This is the work in progress to prepare training materials for upcomming NOMAD onboarding workshop. Current Materials \u00b6 At the moment, just empty and test documents are added for the test purposes.","title":"Welcome to the NOMAD onboarding workshop"},{"location":"index.html#welcome-to-the-nomad-onboarding-workshop","text":"This is the work in progress to prepare training materials for upcomming NOMAD onboarding workshop.","title":"Welcome to the NOMAD onboarding workshop"},{"location":"index.html#current-materials","text":"At the moment, just empty and test documents are added for the test purposes.","title":"Current Materials"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html","text":"Getting started with creating materials for the NOMAD Onboarding Workshop Materials \u00b6 This guide will help us get started with contributing content using MkDocs, which utilizes the Markdown format. NOTE: This guide assumes you are not working on a tracked git repository. Creating Markdown Files \u00b6 Among many alternatives for creating Markdown files, Visual Studio Code (VSCode) is a popular option that offers real-time Markdown preview. To use VSCode for Markdown: Open VSCode and create a new file. Add the .md extension to your filename. To see a live preview, click on the Open Preview to the Side icon located at the top right of the editor. To create a new documentation page: \u00b6 NOTE: This guide assumes you are not working on a tracked git repository. Create a .md file or copy an existing one into a directory inside the docs directory of your project. Add the filename with its extention and its path to the navigation by updating the mkdocs.yml configuration file under the nav section. Here is a generic example entry in mkdocs.yml : nav : - Home : index.md - Your New Page : path/to/your-new-page.md Make sure your the newly made file is placed in correct Section or SubSection. For instance, after copying a newly made Section3_mdfile.md into a directory named Section3 inside the docs directory, the mkdocs.yml file should be modified as follows: nav : - Home : index.md - Section1 : Section1/Section1_mdfile.md - Section2 : - Section2/Section2_mdfile.md - Section2_SubSection1 : Section2/Section2_SubSection1/Section2_SubSection1_mdfile.md - Section2_SubSection2 : Section2/Section2_SubSection2/Section2_SubSection2_mdfile.md - Section3 : Section3/Section3_mdfile.md Referencing Webpages \u00b6 You can create links to other webpages using the following Markdown syntax: [FAIRmat website](https://www.fairmat-nfdi.eu/fairmat/) which gives: FAIRmat website Adding Images \u00b6 To add images to your .md ' file: Save your image in the docs/assets folder. Reference the image in your Markdown file using the syntax: ![A sample text for the image](relative_path_to_your_image/image-filename.png) Be sure to replace image-filename.png with the actual file name and correct relative path to your image, and provide suitable alternative text. For writing relative paths in markdown: \u00b6 Same directory: ![Image](image.png) Subdirectory: ![Image](subdirectory/image.png) Parent directory: ![Image](../image.png) Best practices for file naming \u00b6 Avoid spaces: Use hyphens (-) or underscores (_) instead. Use lowercase: Prevents issues with case sensitivity. Formatting Text \u00b6 Here's how you can format text in Markdown: Bold : **Bold Text** Italic : *Italic Text* Inline Code : `Inline Code` FAIRmat website : [FAIRmat website](https://www.fairmat-nfdi.eu/fairmat/) Adding Tables \u00b6 Create tables using pipes and dashes: | Header One | Header Two | Header Three | |------------|------------|--------------| | Row One | Data | Data | | Row Two | Data | Data | gives Header One Header Two Header Three Row One Data Data Row Two Data Data Code Blocks \u00b6 Include code blocks with syntax highlighting by specifying the language: ```python def example_function(): print(\"Let's be FAIR\") example_function() ``` gives: def example_function (): print ( \"Let's be FAIR\" ) example_function () Further Resources \u00b6 For complete MkDocs documentation, visit MkDocs Official Documentation . For more on Markdown syntax, visit Markdown Guide . Thank you for contributing to our documentation project!","title":"Getting started with creating materials for the NOMAD Onboarding Workshop Materials"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#getting-started-with-creating-materials-for-the-nomad-onboarding-workshop-materials","text":"This guide will help us get started with contributing content using MkDocs, which utilizes the Markdown format. NOTE: This guide assumes you are not working on a tracked git repository.","title":"Getting started with creating materials for the NOMAD Onboarding Workshop Materials"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#creating-markdown-files","text":"Among many alternatives for creating Markdown files, Visual Studio Code (VSCode) is a popular option that offers real-time Markdown preview. To use VSCode for Markdown: Open VSCode and create a new file. Add the .md extension to your filename. To see a live preview, click on the Open Preview to the Side icon located at the top right of the editor.","title":"Creating Markdown Files"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#to-create-a-new-documentation-page","text":"NOTE: This guide assumes you are not working on a tracked git repository. Create a .md file or copy an existing one into a directory inside the docs directory of your project. Add the filename with its extention and its path to the navigation by updating the mkdocs.yml configuration file under the nav section. Here is a generic example entry in mkdocs.yml : nav : - Home : index.md - Your New Page : path/to/your-new-page.md Make sure your the newly made file is placed in correct Section or SubSection. For instance, after copying a newly made Section3_mdfile.md into a directory named Section3 inside the docs directory, the mkdocs.yml file should be modified as follows: nav : - Home : index.md - Section1 : Section1/Section1_mdfile.md - Section2 : - Section2/Section2_mdfile.md - Section2_SubSection1 : Section2/Section2_SubSection1/Section2_SubSection1_mdfile.md - Section2_SubSection2 : Section2/Section2_SubSection2/Section2_SubSection2_mdfile.md - Section3 : Section3/Section3_mdfile.md","title":"To create a new documentation page:"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#referencing-webpages","text":"You can create links to other webpages using the following Markdown syntax: [FAIRmat website](https://www.fairmat-nfdi.eu/fairmat/) which gives: FAIRmat website","title":"Referencing Webpages"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#adding-images","text":"To add images to your .md ' file: Save your image in the docs/assets folder. Reference the image in your Markdown file using the syntax: ![A sample text for the image](relative_path_to_your_image/image-filename.png) Be sure to replace image-filename.png with the actual file name and correct relative path to your image, and provide suitable alternative text.","title":"Adding Images"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#for-writing-relative-paths-in-markdown","text":"Same directory: ![Image](image.png) Subdirectory: ![Image](subdirectory/image.png) Parent directory: ![Image](../image.png)","title":"For writing relative paths in markdown:"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#best-practices-for-file-naming","text":"Avoid spaces: Use hyphens (-) or underscores (_) instead. Use lowercase: Prevents issues with case sensitivity.","title":"Best practices for file naming"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#formatting-text","text":"Here's how you can format text in Markdown: Bold : **Bold Text** Italic : *Italic Text* Inline Code : `Inline Code` FAIRmat website : [FAIRmat website](https://www.fairmat-nfdi.eu/fairmat/)","title":"Formatting Text"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#adding-tables","text":"Create tables using pipes and dashes: | Header One | Header Two | Header Three | |------------|------------|--------------| | Row One | Data | Data | | Row Two | Data | Data | gives Header One Header Two Header Three Row One Data Data Row Two Data Data","title":"Adding Tables"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#code-blocks","text":"Include code blocks with syntax highlighting by specifying the language: ```python def example_function(): print(\"Let's be FAIR\") example_function() ``` gives: def example_function (): print ( \"Let's be FAIR\" ) example_function ()","title":"Code Blocks"},{"location":"Creating_Workshop_Materials/creating_workshop_materials.html#further-resources","text":"For complete MkDocs documentation, visit MkDocs Official Documentation . For more on Markdown syntax, visit Markdown Guide . Thank you for contributing to our documentation project!","title":"Further Resources"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html","text":"Git Collaboration Workflow \u00b6 Introduction \u00b6 This document describes our Git collaboration workflow. I suggest using GitHub GUI instead of the command line for issue tracking and branch management, which allows for a relatively more straightforward integration of everyone's contributions. Workflow Steps \u00b6 1. Start with Fetching Updates \u00b6 Begin each day by synchronizing your local repository with the remote repository: git fetch --all 2. Issue Creation and Branching \u00b6 For every new task or bug fix: Create an Issue on GitHub: Navigate to the 'Issues' tab in our GitHub repository. Click 'New Issue', provide a descriptive title and a detailed description. Submit the issue. Create a Corresponding Branch: After creating the issue, make a new branch, e.g., branch_123 . Keep the 'Repository destination' as it is. Choose 'Checkout locally' Click on 'Create branch' You may now follow the GitHub instructions. For this, you need to open a termiinal, navigate to your local repository directory, and run the commands provided by GitHub: git fetch origin git checkout branch_123 3. Implementing Changes \u00b6 Make Changes: Work on the task on your local. Commit Changes: Regularly commit your work with clear and descriptive messages: git add . git commit -m \"Describe your changes\" Push Changes: Once the work is done and you ready for review, push your branch to the remote repository: git push origin branch_123 4. Pull Requests for Review \u00b6 Create a Pull Request using the GitHub GUI: Go to the GitHub repository webpage You will typically see a prompt to create a pull request for your recently pushed branch. If not, go to the 'Pull requests' tab and click New pull request . Select branch_123 to compare with the 'main' branch. Fill in the details of the pull request with a title and description of the changes you made. Submit the pull request. 5. Review and Merge \u00b6 Review Process: Team members review the Pull Request, provide feedback or approve. Merge the Pull Request: After approval, the owner or a maintainer will merge the Pull Request into the main branch. 6. Post-Merge Updates \u00b6 Update Local Repository: After your Pull Request is merged: git checkout main git pull origin main git branch -d branch_123 git fetch --all --prune The --prune option removes remote-tracking references in your local repository that no longer exist on the remote,his helps in cleaning up old branches that are no longer in use, keeping your local repository tidy and in sync with the remote repository. 7. Conflict Resolution \u00b6 In case of merge conflicts, seek assistance. Resolving conflicts is imprtant for maintaining the repository and sometimes tricky. 8. Conclusion \u00b6 Best Practices and Notes \u00b6 Write clear and detailed issue descriptions and commit messages. At least once a day update your local repository to stay in sync. Delete branches post-merge to maintain a clean repository. Keep the team informed about the issues you are working on. Make small, frequent commits to simplify tracking changes and resolving conflicts.","title":"Git Collaboration Workflow"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#git-collaboration-workflow","text":"","title":"Git Collaboration Workflow"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#introduction","text":"This document describes our Git collaboration workflow. I suggest using GitHub GUI instead of the command line for issue tracking and branch management, which allows for a relatively more straightforward integration of everyone's contributions.","title":"Introduction"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#workflow-steps","text":"","title":"Workflow Steps"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#1-start-with-fetching-updates","text":"Begin each day by synchronizing your local repository with the remote repository: git fetch --all","title":"1. Start with Fetching Updates"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#2-issue-creation-and-branching","text":"For every new task or bug fix: Create an Issue on GitHub: Navigate to the 'Issues' tab in our GitHub repository. Click 'New Issue', provide a descriptive title and a detailed description. Submit the issue. Create a Corresponding Branch: After creating the issue, make a new branch, e.g., branch_123 . Keep the 'Repository destination' as it is. Choose 'Checkout locally' Click on 'Create branch' You may now follow the GitHub instructions. For this, you need to open a termiinal, navigate to your local repository directory, and run the commands provided by GitHub: git fetch origin git checkout branch_123","title":"2. Issue Creation and Branching"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#3-implementing-changes","text":"Make Changes: Work on the task on your local. Commit Changes: Regularly commit your work with clear and descriptive messages: git add . git commit -m \"Describe your changes\" Push Changes: Once the work is done and you ready for review, push your branch to the remote repository: git push origin branch_123","title":"3. Implementing Changes"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#4-pull-requests-for-review","text":"Create a Pull Request using the GitHub GUI: Go to the GitHub repository webpage You will typically see a prompt to create a pull request for your recently pushed branch. If not, go to the 'Pull requests' tab and click New pull request . Select branch_123 to compare with the 'main' branch. Fill in the details of the pull request with a title and description of the changes you made. Submit the pull request.","title":"4. Pull Requests for Review"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#5-review-and-merge","text":"Review Process: Team members review the Pull Request, provide feedback or approve. Merge the Pull Request: After approval, the owner or a maintainer will merge the Pull Request into the main branch.","title":"5. Review and Merge"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#6-post-merge-updates","text":"Update Local Repository: After your Pull Request is merged: git checkout main git pull origin main git branch -d branch_123 git fetch --all --prune The --prune option removes remote-tracking references in your local repository that no longer exist on the remote,his helps in cleaning up old branches that are no longer in use, keeping your local repository tidy and in sync with the remote repository.","title":"6. Post-Merge Updates"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#7-conflict-resolution","text":"In case of merge conflicts, seek assistance. Resolving conflicts is imprtant for maintaining the repository and sometimes tricky.","title":"7. Conflict Resolution"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#8-conclusion","text":"","title":"8. Conclusion"},{"location":"Creating_Workshop_Materials/git_collaboration_training_materials.html#best-practices-and-notes","text":"Write clear and detailed issue descriptions and commit messages. At least once a day update your local repository to stay in sync. Delete branches post-merge to maintain a clean repository. Keep the team informed about the issues you are working on. Make small, frequent commits to simplify tracking changes and resolving conflicts.","title":"Best Practices and Notes"},{"location":"Misc/Miscx_x_glossary.html","text":"Glossary \u00b6 This glossary provides definitions of key terms frequently used in NOMAD. For a comprehensive list, please refer to the NOMAD Documentation Glossary . Key Terms \u00b6 Archive : Processed data stored in NOMAD. The entirety of all processed data is referred to as the Archive. Sometimes used to refer to the processed data of a particular entry. Dataset : Users can organize entries into datasets. Datasets can be compared to albums, labels, or tags on other platforms. Users can obtain a DOI for their datasets. ELN (Electronic Lab Notebook) : Editable entries in NOMAD for documenting research activities, offering form fields and widgets to modify contents. Entry : A database entry in NOMAD, identified by an entry id. Entries are associated with raw files and have individual pages on the NOMAD GUI. Mainfile : Each entry has one raw file that defines it. This is called the mainfile of that entry. Metadata : A specific technical subset of processed data, including ids, timestamps, authors, datasets, references, and schema information. Metainfo : The sum of all schemas. It includes all pre-defined schemas used to represent processed data in a standardized way. Parser : A program that converts raw data files into NOMAD's structured format, transforming information from a specific source format. Processed data : The outcome of parsing and normalizing raw data files. Processed data always follows a schema. Quantity : The smallest referable unit of processed data, which can have many types and shapes, such as strings, numbers, lists, or matrices. Raw file : Any file provided by a NOMAD user. Raw files always belong to an upload and might be associated with an entry. Results (section results ) : A particular section of processed data comprising a summary of the most relevant data for an entry. Schema : Defines possible data structures for processed data, organizing data hierarchically in sections and subsections. Section and Subsection : Sections provide hierarchy and organization for processed data, while quantities refer to the actual pieces of data. Upload : Raw files organized and submitted to NOMAD, which can be shared and published. Uploads consist of raw files and a list of respective entries. For more detailed definitions, visit the NOMAD Documentation Glossary .","title":"NOMAD Glossary"},{"location":"Misc/Miscx_x_glossary.html#glossary","text":"This glossary provides definitions of key terms frequently used in NOMAD. For a comprehensive list, please refer to the NOMAD Documentation Glossary .","title":"Glossary"},{"location":"Misc/Miscx_x_glossary.html#key-terms","text":"Archive : Processed data stored in NOMAD. The entirety of all processed data is referred to as the Archive. Sometimes used to refer to the processed data of a particular entry. Dataset : Users can organize entries into datasets. Datasets can be compared to albums, labels, or tags on other platforms. Users can obtain a DOI for their datasets. ELN (Electronic Lab Notebook) : Editable entries in NOMAD for documenting research activities, offering form fields and widgets to modify contents. Entry : A database entry in NOMAD, identified by an entry id. Entries are associated with raw files and have individual pages on the NOMAD GUI. Mainfile : Each entry has one raw file that defines it. This is called the mainfile of that entry. Metadata : A specific technical subset of processed data, including ids, timestamps, authors, datasets, references, and schema information. Metainfo : The sum of all schemas. It includes all pre-defined schemas used to represent processed data in a standardized way. Parser : A program that converts raw data files into NOMAD's structured format, transforming information from a specific source format. Processed data : The outcome of parsing and normalizing raw data files. Processed data always follows a schema. Quantity : The smallest referable unit of processed data, which can have many types and shapes, such as strings, numbers, lists, or matrices. Raw file : Any file provided by a NOMAD user. Raw files always belong to an upload and might be associated with an entry. Results (section results ) : A particular section of processed data comprising a summary of the most relevant data for an entry. Schema : Defines possible data structures for processed data, organizing data hierarchically in sections and subsections. Section and Subsection : Sections provide hierarchy and organization for processed data, while quantities refer to the actual pieces of data. Upload : Raw files organized and submitted to NOMAD, which can be shared and published. Uploads consist of raw files and a list of respective entries. For more detailed definitions, visit the NOMAD Documentation Glossary .","title":"Key Terms"},{"location":"Module1/1_RDM_with_NOMAD5.html","text":"The NOMAD Graphical User Interface (GUI) \u00b6 This is a placeholder for the NOMAD GUI section.","title":"The NOMAD Graphical User Interface (GUI)"},{"location":"Module1/1_RDM_with_NOMAD5.html#the-nomad-graphical-user-interface-gui","text":"This is a placeholder for the NOMAD GUI section.","title":"The NOMAD Graphical User Interface (GUI)"},{"location":"Module1/1_RDM_with_NOMAD6.html","text":"Uploading and Publishing Data \u00b6 This is a placeholder for the Uploading and Publishing Data section.","title":"Uploading and Publishing Data"},{"location":"Module1/1_RDM_with_NOMAD6.html#uploading-and-publishing-data","text":"This is a placeholder for the Uploading and Publishing Data section.","title":"Uploading and Publishing Data"},{"location":"Module1/1_RDM_with_NOMAD7.html","text":"Sharing and Collaboration \u00b6 This is a placeholder for the Sharing and Collaboration section.","title":"Sharing and Collaboration"},{"location":"Module1/1_RDM_with_NOMAD7.html#sharing-and-collaboration","text":"This is a placeholder for the Sharing and Collaboration section.","title":"Sharing and Collaboration"},{"location":"Module1/1_RDM_with_NOMAD8.html","text":"Exploring Data \u00b6 This is a placeholder for the Exploring Data section.","title":"Exploring Data"},{"location":"Module1/1_RDM_with_NOMAD8.html#exploring-data","text":"This is a placeholder for the Exploring Data section.","title":"Exploring Data"},{"location":"Module1/M1_1_RDM_with_NOMAD.html","text":"Research Data Management with NOMAD \u00b6 Towards FAIR Research Data with NOMAD \u00b6 Materials science researchers often struggle with managing the diverse and vast amounts of data generated by their work. This is where NOMAD comes in. NOMAD is a free, open-source data management platform designed specifically for materials science. Its main mission is to ensure that research data adhere to the FAIR principles: Findable, Accessible, Interoperable, and Reusable. What are the FAIR Principles? The FAIR principles, introduced by Wilkinson et al. in 2016 , outline a framework for making research data more Findable, Accessible, Interoperable, and Reusable (FAIR). These principles emphasize the importance of improving data management to maximize the value of data in research. Findable : Data should be easy to locate, both by humans and machines, using persistent identifiers and descriptive metadata. This includes making data searchable in registries or repositories with unique identifiers. Accessible : Data and metadata should be retrievable by identifiers using standard protocols that are open and free to ensure widespread access. While not all data must be openly available, clear accessibility conditions should be provided. Interoperable : To integrate with other datasets or tools, data should use standard formats and vocabularies, facilitating cross-domain data usage and interoperability. Reusable : Data should have clear and accessible licenses, detailed provenance, and should be well-described to enable reuse in different contexts and by different communities. Together, these principles help ensure that data can be found, accessed, combined, and reused effectively across different projects and disciplines. Embracing FAIR doesn\u2019t just make data management smoother\u2014it opens doors to collaboration, accelerates discovery, and gives your data a meaningful, extended life in the research community! NOMAD provides a comprehensive set of tools that go beyond simple storage to support the entire research data lifecycle. With NOMAD you actively control the following areas of RDM: Data Management : Structure and organize your data for clarity and ease of access. Data Sharing : Collaborate securely with colleagues by sharing your data. Data Publishing : Publish your research data for broader access and use by the scientific community. NOMAD addresses the challenge of managing large amounts of heterogeneous research data, benefiting individual research groups by improving data organization, analysis, collaboration, and knowledge sharing. It ultimately builds a centralized archive of FAIR research data for the entire materials-science community. NOMAD and the Research Data Lifecycle \u00b6 The data lifecycle refers to the journey data take throughout their existence. This journey typically involves several stages: data generation or collection, processing for accuracy and usability, analysis to extract insights, secure storage for later access, and eventual sharing with colleagues or the broader scientific community. Following these steps ensures that valuable data are not lost but preserved and fully utilized. NOMAD will support you throughout the research data lifecycle stages: Planning : Organize and manage data from the start of your project. Collection : Use NOMAD's Electronic Lab Notebook (ELN) functionality for structured data documentation. NOMAD CAMELS enhances data collection by generating FAIR-compliant data directly from instruments. Analysis : Post-process and analyze data with NOMAD Remote Tools Hub (NORTH) , using tools like Jupyter notebooks for flexible data exploration. Preservation : Keep data secure with NOMAD\u2019s centralized infrastructure, or use NOMAD Oasis for local storage, offering flexibility for your preferred data storage solution. Sharing : Share your data securely with colleagues. DOIs ensure data visibility and proper citation in publications. Reuse : NOMAD promotes data reuse with rich metadata and advanced search capabilities. The powerful API allows programmatic access for automating data exploration.","title":"Introduction"},{"location":"Module1/M1_1_RDM_with_NOMAD.html#research-data-management-with-nomad","text":"","title":"Research Data Management with NOMAD"},{"location":"Module1/M1_1_RDM_with_NOMAD.html#towards-fair-research-data-with-nomad","text":"Materials science researchers often struggle with managing the diverse and vast amounts of data generated by their work. This is where NOMAD comes in. NOMAD is a free, open-source data management platform designed specifically for materials science. Its main mission is to ensure that research data adhere to the FAIR principles: Findable, Accessible, Interoperable, and Reusable. What are the FAIR Principles? The FAIR principles, introduced by Wilkinson et al. in 2016 , outline a framework for making research data more Findable, Accessible, Interoperable, and Reusable (FAIR). These principles emphasize the importance of improving data management to maximize the value of data in research. Findable : Data should be easy to locate, both by humans and machines, using persistent identifiers and descriptive metadata. This includes making data searchable in registries or repositories with unique identifiers. Accessible : Data and metadata should be retrievable by identifiers using standard protocols that are open and free to ensure widespread access. While not all data must be openly available, clear accessibility conditions should be provided. Interoperable : To integrate with other datasets or tools, data should use standard formats and vocabularies, facilitating cross-domain data usage and interoperability. Reusable : Data should have clear and accessible licenses, detailed provenance, and should be well-described to enable reuse in different contexts and by different communities. Together, these principles help ensure that data can be found, accessed, combined, and reused effectively across different projects and disciplines. Embracing FAIR doesn\u2019t just make data management smoother\u2014it opens doors to collaboration, accelerates discovery, and gives your data a meaningful, extended life in the research community! NOMAD provides a comprehensive set of tools that go beyond simple storage to support the entire research data lifecycle. With NOMAD you actively control the following areas of RDM: Data Management : Structure and organize your data for clarity and ease of access. Data Sharing : Collaborate securely with colleagues by sharing your data. Data Publishing : Publish your research data for broader access and use by the scientific community. NOMAD addresses the challenge of managing large amounts of heterogeneous research data, benefiting individual research groups by improving data organization, analysis, collaboration, and knowledge sharing. It ultimately builds a centralized archive of FAIR research data for the entire materials-science community.","title":"Towards FAIR Research Data with NOMAD"},{"location":"Module1/M1_1_RDM_with_NOMAD.html#nomad-and-the-research-data-lifecycle","text":"The data lifecycle refers to the journey data take throughout their existence. This journey typically involves several stages: data generation or collection, processing for accuracy and usability, analysis to extract insights, secure storage for later access, and eventual sharing with colleagues or the broader scientific community. Following these steps ensures that valuable data are not lost but preserved and fully utilized. NOMAD will support you throughout the research data lifecycle stages: Planning : Organize and manage data from the start of your project. Collection : Use NOMAD's Electronic Lab Notebook (ELN) functionality for structured data documentation. NOMAD CAMELS enhances data collection by generating FAIR-compliant data directly from instruments. Analysis : Post-process and analyze data with NOMAD Remote Tools Hub (NORTH) , using tools like Jupyter notebooks for flexible data exploration. Preservation : Keep data secure with NOMAD\u2019s centralized infrastructure, or use NOMAD Oasis for local storage, offering flexibility for your preferred data storage solution. Sharing : Share your data securely with colleagues. DOIs ensure data visibility and proper citation in publications. Reuse : NOMAD promotes data reuse with rich metadata and advanced search capabilities. The powerful API allows programmatic access for automating data exploration.","title":"NOMAD and the Research Data Lifecycle"},{"location":"Module1/M1_2_NOMAD_workflow.html","text":"NOMAD Structures Your Research Data \u00b6 Structured data is a key component in the journey towards making research data FAIR. NOMAD supports this process by providing predefinec structure for collected reasearch data as well as converting raw files into structured and FAIR-compliant data. This transformation is done through the use of schemas, parsers, and ELNs . What are structured data? Structured data refers to information that is organized and formatted in a way that makes it easily searchable, understandable, and usable by both humans and machines . It typically adheres to a predefined model or schema, which dictates how the data is arranged\u2014such as in tables, rows, and columns in a database, or within specific fields in a file format like XML, JSON or HDF5. At the core of the structuring process in NOMAD is the schema . Schemas define possible structures for processed data and metadata, organizing this data hierarchically in sections and subsections. They also define the relationships between different pieces of data and their associated metadata. For example, in a schema, the units of a measured quantity are also defined\u2014such that one cannot assign units of length to a value of mass measurement. In essence, schemas are based on ontologies, as they define possible relationships between data and metadata organized within them. To simplify, a schema is a design that defines how the structure and relationship of the data should be. An analogy would be to think of a schema as the blueprint of a toolbox, where the design specifies the location for the tools, once a template based on this schema is created. Some placeholders in the template must be filled with data, while others are optional. For instance, when uploading data to NOMAD, it is mandatory to specify when , and which user is uploading the data. However, certain fields, such as whether the data includes a DOI, are optional. The NOMAD Metainfo All data in NOMAD follows a schema that defines how the data is hierarchically structured and cross-referenced. This schema is called the NOMAD Metainfo . It defines a general domain-independent super structure, as well as highly detailed, specialized data from specific methods, tools, and programs. Based on the chosen or a newly created schema, NOMAD structures data into sections and subsections , where each section can contain data and additional sections. Without selecting a schema when uploading files to NOMAD, the system treats the files as a block of binary information, functioning merely as a storage service without the organizational benefits; similar to how a service like Dropbox would work. Creating a schema enables the development of ELNs that exploit the schema to organize data in a FAIR-compliant manner. Returning to the analogy, the schema defines placeholders in the ELN, guiding users on what data or metadata to provide. For example, in an experiment measuring the heating and evaporation rates of water at 120\u00b0C, the schema would require temperature units for the temperature field and mass units for the water mass field. The same principle applies when uploading a file to NOMAD with an available parser for this specific file format. The parser is a computer script that goes through your data file and collects the information to fill the placeholders defined by the schema. Therefore, the primary effort lies in preparing a proper schema for your experiment. Once the schema is prepared, the data can be placed in the correct placeholders either manually via an ELN or automatically upon uploading a file using a parser. The result is the same: in both scenarios data will be stored in a structured manner as you defined it in the schema. In summary, the transition from raw files to FAIR data in NOMAD is achieved by careful design and implementation of schemas and parsers .","title":"From Raw Files to FAIR Data"},{"location":"Module1/M1_2_NOMAD_workflow.html#nomad-structures-your-research-data","text":"Structured data is a key component in the journey towards making research data FAIR. NOMAD supports this process by providing predefinec structure for collected reasearch data as well as converting raw files into structured and FAIR-compliant data. This transformation is done through the use of schemas, parsers, and ELNs . What are structured data? Structured data refers to information that is organized and formatted in a way that makes it easily searchable, understandable, and usable by both humans and machines . It typically adheres to a predefined model or schema, which dictates how the data is arranged\u2014such as in tables, rows, and columns in a database, or within specific fields in a file format like XML, JSON or HDF5. At the core of the structuring process in NOMAD is the schema . Schemas define possible structures for processed data and metadata, organizing this data hierarchically in sections and subsections. They also define the relationships between different pieces of data and their associated metadata. For example, in a schema, the units of a measured quantity are also defined\u2014such that one cannot assign units of length to a value of mass measurement. In essence, schemas are based on ontologies, as they define possible relationships between data and metadata organized within them. To simplify, a schema is a design that defines how the structure and relationship of the data should be. An analogy would be to think of a schema as the blueprint of a toolbox, where the design specifies the location for the tools, once a template based on this schema is created. Some placeholders in the template must be filled with data, while others are optional. For instance, when uploading data to NOMAD, it is mandatory to specify when , and which user is uploading the data. However, certain fields, such as whether the data includes a DOI, are optional. The NOMAD Metainfo All data in NOMAD follows a schema that defines how the data is hierarchically structured and cross-referenced. This schema is called the NOMAD Metainfo . It defines a general domain-independent super structure, as well as highly detailed, specialized data from specific methods, tools, and programs. Based on the chosen or a newly created schema, NOMAD structures data into sections and subsections , where each section can contain data and additional sections. Without selecting a schema when uploading files to NOMAD, the system treats the files as a block of binary information, functioning merely as a storage service without the organizational benefits; similar to how a service like Dropbox would work. Creating a schema enables the development of ELNs that exploit the schema to organize data in a FAIR-compliant manner. Returning to the analogy, the schema defines placeholders in the ELN, guiding users on what data or metadata to provide. For example, in an experiment measuring the heating and evaporation rates of water at 120\u00b0C, the schema would require temperature units for the temperature field and mass units for the water mass field. The same principle applies when uploading a file to NOMAD with an available parser for this specific file format. The parser is a computer script that goes through your data file and collects the information to fill the placeholders defined by the schema. Therefore, the primary effort lies in preparing a proper schema for your experiment. Once the schema is prepared, the data can be placed in the correct placeholders either manually via an ELN or automatically upon uploading a file using a parser. The result is the same: in both scenarios data will be stored in a structured manner as you defined it in the schema. In summary, the transition from raw files to FAIR data in NOMAD is achieved by careful design and implementation of schemas and parsers .","title":"NOMAD Structures Your Research Data"},{"location":"Module1/M1_3_NOMAD_datamodel.html","text":"NOMAD Data Model \u00b6 NOMAD organizes data into sections , where each section can contain data and additional subsections . This hierarchical organization allows users to browse complex data similarly to browsing files and directories on a computer. Each section follows a specific definition, ensuring that all contained data and subsections have precise names, descriptions, types, shapes, and units. This structured approach not only makes it easier for users themselves to explore the data but also keeps the data and metadata consistent and interoperable, i.e., machine-readable. To define the structures for data files, NOMAD adopts a data model based on an entity-activity framework . Essentially, each section that is intended to become an entry in NOMAD is a subsection of one of the two main base sections: entities or activities . Entities are objects that persist over time, such as material systems, collections, or instruments. Activities are actions involving entities, including processes, measurements, analyses, and experiments. Sections in NOMAD are interrelated through inheritance and composition . Inheritance allows sections to inherit properties, attributes, and functions from other sections, building on predefined structures. Composition enables sections to include other sections as subsections, organizing complex data hierarchically. This flexible design makes it possible to incorporate new data types by extending and reusing existing sections. What are Base Sections? \u00b6 Base sections are reusable building blocks in NOMAD's data model. The top-most parent base section, which is called BaseSection , provides a set of four global quantities for basic information about entries: name : A short human readable and descriptive name. datetime : The date and time associated with this section. lab_id : An ID string that is unique at least for the lab that produced this data. description : Any information that cannot be captured in the other fields. Both the Entity and Activity base sections, inherit these four global quantities from BaseSection . What are Entities? \u00b6 Entities are objects that persist over time while maintaining their identity. In NOMAD, the Entity base section is composed of the three sections System , Collection , and Instrument . System : Represents any material system, from atomic to device scale. It includes properties like elemental composition, which tracks the elements and their atomic fractions within the system. CompositeSystem and PureSubstance are two sections that inherit from the System base section. Collection : Used to group entities together, such as a batch of substrates. Instrument : Describes tools used for material processing or characterization. What are Activities? \u00b6 Activities are actions that occur over time and depend on some entity. The Activity section is subclassed by Process , Measurement , Analysis , and Experiment sections. These subclasses are intended to cover all types of activities and should be used instead of inheriting directly from Activity base section. Process : Represents a planned process that results in physical changes to a specific material, such as sample preparation or material transformation. Measurement : A planned process aimed at producing information about the material entity. Analysis : Involves data processing and analysis, producing output data from input data. Experiment : Groups activities together, useful for organizing multiple samples or processes under a single experimental framework. Please note, all sections mentioned here are base sections that inherit from either Entity or Activity , which are themselves subclasses of BaseSection . For further details, you may refer to the NOMAD's documentation page on How to use base sections . The built-in NOMAD module datamodel.metainfo.basesections contains a set of base sections based on the entity-activity model. There are more base sections available for reuse that are not listed in this module.","title":"NOMAD Data Model"},{"location":"Module1/M1_3_NOMAD_datamodel.html#nomad-data-model","text":"NOMAD organizes data into sections , where each section can contain data and additional subsections . This hierarchical organization allows users to browse complex data similarly to browsing files and directories on a computer. Each section follows a specific definition, ensuring that all contained data and subsections have precise names, descriptions, types, shapes, and units. This structured approach not only makes it easier for users themselves to explore the data but also keeps the data and metadata consistent and interoperable, i.e., machine-readable. To define the structures for data files, NOMAD adopts a data model based on an entity-activity framework . Essentially, each section that is intended to become an entry in NOMAD is a subsection of one of the two main base sections: entities or activities . Entities are objects that persist over time, such as material systems, collections, or instruments. Activities are actions involving entities, including processes, measurements, analyses, and experiments. Sections in NOMAD are interrelated through inheritance and composition . Inheritance allows sections to inherit properties, attributes, and functions from other sections, building on predefined structures. Composition enables sections to include other sections as subsections, organizing complex data hierarchically. This flexible design makes it possible to incorporate new data types by extending and reusing existing sections.","title":"NOMAD Data Model"},{"location":"Module1/M1_3_NOMAD_datamodel.html#what-are-base-sections","text":"Base sections are reusable building blocks in NOMAD's data model. The top-most parent base section, which is called BaseSection , provides a set of four global quantities for basic information about entries: name : A short human readable and descriptive name. datetime : The date and time associated with this section. lab_id : An ID string that is unique at least for the lab that produced this data. description : Any information that cannot be captured in the other fields. Both the Entity and Activity base sections, inherit these four global quantities from BaseSection .","title":"What are Base Sections?"},{"location":"Module1/M1_3_NOMAD_datamodel.html#what-are-entities","text":"Entities are objects that persist over time while maintaining their identity. In NOMAD, the Entity base section is composed of the three sections System , Collection , and Instrument . System : Represents any material system, from atomic to device scale. It includes properties like elemental composition, which tracks the elements and their atomic fractions within the system. CompositeSystem and PureSubstance are two sections that inherit from the System base section. Collection : Used to group entities together, such as a batch of substrates. Instrument : Describes tools used for material processing or characterization.","title":"What are Entities?"},{"location":"Module1/M1_3_NOMAD_datamodel.html#what-are-activities","text":"Activities are actions that occur over time and depend on some entity. The Activity section is subclassed by Process , Measurement , Analysis , and Experiment sections. These subclasses are intended to cover all types of activities and should be used instead of inheriting directly from Activity base section. Process : Represents a planned process that results in physical changes to a specific material, such as sample preparation or material transformation. Measurement : A planned process aimed at producing information about the material entity. Analysis : Involves data processing and analysis, producing output data from input data. Experiment : Groups activities together, useful for organizing multiple samples or processes under a single experimental framework. Please note, all sections mentioned here are base sections that inherit from either Entity or Activity , which are themselves subclasses of BaseSection . For further details, you may refer to the NOMAD's documentation page on How to use base sections . The built-in NOMAD module datamodel.metainfo.basesections contains a set of base sections based on the entity-activity model. There are more base sections available for reuse that are not listed in this module.","title":"What are Activities?"},{"location":"Module1/M1_4_NOMAD_users.html","text":"NOMAD Users \u00b6 The NOMAD software offers a wide range of extendable functionalities to support FAIR research data management. Similarly, individuals with a diverse and dynamic spectrum of needs and skills interact with NOMAD. Therefore, understanding your roles and responsibilities with respect to RDM in a research project is important for the effective utilization of NOMAD. In general, individuals who interact with NOMAD can be categorized into three groups: 'Users', 'Application Administrators', and 'System Administrators'. This classification is not absolute and can have overlaps, depending on the RDM needs of researchers, research groups, or institutes. User (Researcher) \u00b6 Users are scientists who directly interact with the NOMAD software. They upload datasets, search for data, and use applications within the NOMAD software. The majority of NOMAD functionalities are developed to support this user group with their RDM needs. Application Administrator \u00b6 Application administrators, also known as data stewards or data scientists, are responsible for implementing or creating use-case applications within NOMAD. This role requires an understanding of NOMAD, as well as proficiency in Python programming and tools like Jupyter. They build applications for the researchers to address their specific RDM requirements. System Administrator \u00b6 System administrators set up local NOMAD installations (NOMAD Oasis) on servers of a university or a research institute. Their responsibilities include configuring and maintaining the server and managing access. In addition to the requirements described for the Application Administrators, this role also requires knowledge of Docker and Keycloak. In this onboarding workshop, our main target group will be NOMAD users (researchers), with a tendency to extend it towards the needs of application administrators or advanced users in different parts to ensure a comprehensive understanding and effective use of NOMAD functionalities. The NOMAD Documentation , the ongiong FAIRmat and NOMAD Tutorial series , archived both on YouTube and Zenodo , are designed to address the needs of researchers, application adminstrators and system adminstrators.","title":"Levels of NOMAD Users"},{"location":"Module1/M1_4_NOMAD_users.html#nomad-users","text":"The NOMAD software offers a wide range of extendable functionalities to support FAIR research data management. Similarly, individuals with a diverse and dynamic spectrum of needs and skills interact with NOMAD. Therefore, understanding your roles and responsibilities with respect to RDM in a research project is important for the effective utilization of NOMAD. In general, individuals who interact with NOMAD can be categorized into three groups: 'Users', 'Application Administrators', and 'System Administrators'. This classification is not absolute and can have overlaps, depending on the RDM needs of researchers, research groups, or institutes.","title":"NOMAD Users"},{"location":"Module1/M1_4_NOMAD_users.html#user-researcher","text":"Users are scientists who directly interact with the NOMAD software. They upload datasets, search for data, and use applications within the NOMAD software. The majority of NOMAD functionalities are developed to support this user group with their RDM needs.","title":"User (Researcher)"},{"location":"Module1/M1_4_NOMAD_users.html#application-administrator","text":"Application administrators, also known as data stewards or data scientists, are responsible for implementing or creating use-case applications within NOMAD. This role requires an understanding of NOMAD, as well as proficiency in Python programming and tools like Jupyter. They build applications for the researchers to address their specific RDM requirements.","title":"Application Administrator"},{"location":"Module1/M1_4_NOMAD_users.html#system-administrator","text":"System administrators set up local NOMAD installations (NOMAD Oasis) on servers of a university or a research institute. Their responsibilities include configuring and maintaining the server and managing access. In addition to the requirements described for the Application Administrators, this role also requires knowledge of Docker and Keycloak. In this onboarding workshop, our main target group will be NOMAD users (researchers), with a tendency to extend it towards the needs of application administrators or advanced users in different parts to ensure a comprehensive understanding and effective use of NOMAD functionalities. The NOMAD Documentation , the ongiong FAIRmat and NOMAD Tutorial series , archived both on YouTube and Zenodo , are designed to address the needs of researchers, application adminstrators and system adminstrators.","title":"System Administrator"},{"location":"Module1/M1_5_getting_support.html","text":"Getting Support \u00b6 The following resources are available to provide support and assistance: 1. Official Documentation \u00b6 The NOMAD Documentation is a rich resource that explains all the aspects of NOMAD in great detail and provides valuable how-to guides on advanced topics. 2. Tutorials \u00b6 Explore the FAIRmat and NOMAD Tutorial series, which cover various aspects of using NOMAD. These tutorials are available on multiple platforms: A list of past and upcoming tutorials can be found on our website under FAIRmat and NOMAD Tutorial Series . Also feel free to subscribe to our email newsletter to be the first to know about the latest news, upcoming events, new videos as well as general FAIRmat updates throughout the year. YouTube Channel: The NOMAD Laboratory Zenodo: FAIRmat NFDI Community 3. Interactive Communication on Discord \u00b6 Join the NOMAD Discord Channel for interactive and casual communication with other users and support from our developers. There you may find several sub-channels focusing on specific topics. 4. Additional Support Channels \u00b6 NOMAD GitHub Repository: Check the NOMAD GitHub for issues, discussions, and community support. You can open an issue on NOMAD's main GitHub project . This way, we can track your problem and start implementing a solution. Community Forum: Participate in the NOMAD User Forum to ask questions and share experiences with other users. Email Support: If the options mentioned above are not appropriate for your problem, feel free to contact us via email. We will respond as soon as possible: support@nomad-lab.eu.","title":"Getting Support"},{"location":"Module1/M1_5_getting_support.html#getting-support","text":"The following resources are available to provide support and assistance:","title":"Getting Support"},{"location":"Module1/M1_5_getting_support.html#1-official-documentation","text":"The NOMAD Documentation is a rich resource that explains all the aspects of NOMAD in great detail and provides valuable how-to guides on advanced topics.","title":"1. Official Documentation"},{"location":"Module1/M1_5_getting_support.html#2-tutorials","text":"Explore the FAIRmat and NOMAD Tutorial series, which cover various aspects of using NOMAD. These tutorials are available on multiple platforms: A list of past and upcoming tutorials can be found on our website under FAIRmat and NOMAD Tutorial Series . Also feel free to subscribe to our email newsletter to be the first to know about the latest news, upcoming events, new videos as well as general FAIRmat updates throughout the year. YouTube Channel: The NOMAD Laboratory Zenodo: FAIRmat NFDI Community","title":"2. Tutorials"},{"location":"Module1/M1_5_getting_support.html#3-interactive-communication-on-discord","text":"Join the NOMAD Discord Channel for interactive and casual communication with other users and support from our developers. There you may find several sub-channels focusing on specific topics.","title":"3. Interactive Communication on Discord"},{"location":"Module1/M1_5_getting_support.html#4-additional-support-channels","text":"NOMAD GitHub Repository: Check the NOMAD GitHub for issues, discussions, and community support. You can open an issue on NOMAD's main GitHub project . This way, we can track your problem and start implementing a solution. Community Forum: Participate in the NOMAD User Forum to ask questions and share experiences with other users. Email Support: If the options mentioned above are not appropriate for your problem, feel free to contact us via email. We will respond as soon as possible: support@nomad-lab.eu.","title":"4. Additional Support Channels"},{"location":"Module1/M1_x_NOMAD_and_oasis.html","text":"Towards Building a Federated FAIR Data Infrastructure with NOMAD and NOMAD Oasis \u00b6 NOMAD is a free, open-source web-based software designed for managing, sharing, and publishing materials-science data. It provides a comprehensive suite of tools to help researchers organize, analyze, and publish their data. Research data in NOMAD are organized in well-defined structures described by a formal schema known as the NOMAD Metainfo. This schema ensures that data are hierarchically structured and cross-referenced, enabling efficient searching, accessibility via APIs, interoperability, and reusability. NOMAD offers important features for effective research data management including: FAIR Data : RDM with NOMAD ensures data are Findable, Accessible, Interoperable, and Reusable by extracting machine-actionable data from various file formats. With NOMAD you can provide rich and organized metadata for custom theoretical and experimental research data. Organizing in Hierarchical Format : NOMAD organizes your files as you would on your hard drive, allowing for file-by-file or batch uploads using drag-and-drop. All data are hierarchically structured and interconnected. NOMAD uses a file-based archival system, meaning that you access all your raw files, in case a migration or archival in other data management systems are desired. API Access : NOMAD allows you to programmatically manage your research data. Everything you might need for an effective research data management is accessible via API, including raw files, data, metadata, processed data, etc. Extensible Schema : With NOMAD, you have highest flexibility for your RDM needs. The existing NOMAD schema can be extended to suit your specific needs. Practical User Control : Using NOMAD, you can manage your data privately or collaborate within your group before making it public. For even higher privacy, e.g., due to restrictions within an institute or research group, NOMAD has further solutions, the NOMAD Oasis. Integrated Analysis Tools : NOMAD offers a programming environment for your advanced analysis processes and visualizations, e.g., you can run Jupyter notebooks and other tools directly on NOMAD. Access and modify your data, and publish your Jupyter notebooks. DOI Assignment : With NOMAD, you can publish and archive your data for free, with the ability to assign DOIs to datasets for efficient referencing in publications. Comprehensive Database : NOMAD Combines data from popular sources like the Materials Project, AFLOW, and OQMD, making it the largest database of its kind. The central (or public) NOMAD uses computer resources from the Max-Planck-Society. For specific needs where more resources or greater privacy are required, NOMAD Oasis is the solution. NOMAD Oasis is containerized. You can run it on a single computer with Docker Compose or on a large cluster with Kubernetes. NOMAD can be used to manage data for small groups as well as large research institutes, allowing to manage data under your own rules and extensions. NOMAD Oasis lets you create your own local NOMAD instance, providing all the features of the public NOMAD service while allowing you to implement your specific data management needs and policies. With NOMAD Oasis, you get an instant overview of all your group's data, increase productivity, and realize your research data management plans with ease. NOMAD Oasis allows you to manage your data locally, while still being prepared to publish and share your data via NOMAD. Most important features of NOMAD Oasis include: Easy Installation and Scalability : Install with a quick guide on any computer with Docker, and utilize unlimited scalability on Kubernetes clusters when needed. Local Operation : NOMAD Oasis can be customized to operate behind your firewall and inside your VPN. Control over Published Data : NOMAD Oasis allows managing data without publishing them. Soon, NOMAD Oasis will be connected to the public NOMAD service, allowing you to publish selected data with one click. Custom ELNs : Extend and customize NOMAD's schema to create specialized ELNs for documenting your work and data. Easy Backup : All data are organized in a conventional file system, easily integrated into existing storage and backup solutions. You can export all the files related to your projects, or publish them using the central NOMAD service. Achieving a Federated FAIR Data Infrastructure \u00b6 Combining NOMAD and NOMAD Oasis allows for the creation of a federated FAIR data infrastructure. This setup enables local control and customization while still benefiting from the centralized, comprehensive features of the public NOMAD. Research institutions can manage sensitive data locally with NOMAD Oasis and selectively share and publish data to NOMAD. By leveraging both platforms, research groups can achieve a robust, scalable, and secure data management environment that supports collaboration, data sharing, and advanced analysis, all while adhering to the highest standards of data stewardship.","title":"M1 x NOMAD and oasis"},{"location":"Module1/M1_x_NOMAD_and_oasis.html#towards-building-a-federated-fair-data-infrastructure-with-nomad-and-nomad-oasis","text":"NOMAD is a free, open-source web-based software designed for managing, sharing, and publishing materials-science data. It provides a comprehensive suite of tools to help researchers organize, analyze, and publish their data. Research data in NOMAD are organized in well-defined structures described by a formal schema known as the NOMAD Metainfo. This schema ensures that data are hierarchically structured and cross-referenced, enabling efficient searching, accessibility via APIs, interoperability, and reusability. NOMAD offers important features for effective research data management including: FAIR Data : RDM with NOMAD ensures data are Findable, Accessible, Interoperable, and Reusable by extracting machine-actionable data from various file formats. With NOMAD you can provide rich and organized metadata for custom theoretical and experimental research data. Organizing in Hierarchical Format : NOMAD organizes your files as you would on your hard drive, allowing for file-by-file or batch uploads using drag-and-drop. All data are hierarchically structured and interconnected. NOMAD uses a file-based archival system, meaning that you access all your raw files, in case a migration or archival in other data management systems are desired. API Access : NOMAD allows you to programmatically manage your research data. Everything you might need for an effective research data management is accessible via API, including raw files, data, metadata, processed data, etc. Extensible Schema : With NOMAD, you have highest flexibility for your RDM needs. The existing NOMAD schema can be extended to suit your specific needs. Practical User Control : Using NOMAD, you can manage your data privately or collaborate within your group before making it public. For even higher privacy, e.g., due to restrictions within an institute or research group, NOMAD has further solutions, the NOMAD Oasis. Integrated Analysis Tools : NOMAD offers a programming environment for your advanced analysis processes and visualizations, e.g., you can run Jupyter notebooks and other tools directly on NOMAD. Access and modify your data, and publish your Jupyter notebooks. DOI Assignment : With NOMAD, you can publish and archive your data for free, with the ability to assign DOIs to datasets for efficient referencing in publications. Comprehensive Database : NOMAD Combines data from popular sources like the Materials Project, AFLOW, and OQMD, making it the largest database of its kind. The central (or public) NOMAD uses computer resources from the Max-Planck-Society. For specific needs where more resources or greater privacy are required, NOMAD Oasis is the solution. NOMAD Oasis is containerized. You can run it on a single computer with Docker Compose or on a large cluster with Kubernetes. NOMAD can be used to manage data for small groups as well as large research institutes, allowing to manage data under your own rules and extensions. NOMAD Oasis lets you create your own local NOMAD instance, providing all the features of the public NOMAD service while allowing you to implement your specific data management needs and policies. With NOMAD Oasis, you get an instant overview of all your group's data, increase productivity, and realize your research data management plans with ease. NOMAD Oasis allows you to manage your data locally, while still being prepared to publish and share your data via NOMAD. Most important features of NOMAD Oasis include: Easy Installation and Scalability : Install with a quick guide on any computer with Docker, and utilize unlimited scalability on Kubernetes clusters when needed. Local Operation : NOMAD Oasis can be customized to operate behind your firewall and inside your VPN. Control over Published Data : NOMAD Oasis allows managing data without publishing them. Soon, NOMAD Oasis will be connected to the public NOMAD service, allowing you to publish selected data with one click. Custom ELNs : Extend and customize NOMAD's schema to create specialized ELNs for documenting your work and data. Easy Backup : All data are organized in a conventional file system, easily integrated into existing storage and backup solutions. You can export all the files related to your projects, or publish them using the central NOMAD service.","title":"Towards Building a Federated FAIR Data Infrastructure with NOMAD and NOMAD Oasis"},{"location":"Module1/M1_x_NOMAD_and_oasis.html#achieving-a-federated-fair-data-infrastructure","text":"Combining NOMAD and NOMAD Oasis allows for the creation of a federated FAIR data infrastructure. This setup enables local control and customization while still benefiting from the centralized, comprehensive features of the public NOMAD. Research institutions can manage sensitive data locally with NOMAD Oasis and selectively share and publish data to NOMAD. By leveraging both platforms, research groups can achieve a robust, scalable, and secure data management environment that supports collaboration, data sharing, and advanced analysis, all while adhering to the highest standards of data stewardship.","title":"Achieving a Federated FAIR Data Infrastructure"},{"location":"Module2/M2_x_NOMAD_GUI.html","text":"NOMAD GUI \u00b6 Navigating to NOMAD \u00b6 NOMAD website is the starting point for accessing NOMAD, where you can upload, manage, and explore data. To access the latest stable version of NOMAD (official release), click on the blue OPEN NOMAD button on the top right of the page. This will open the NOMAD Graphical User Interface (GUI) directly in your web browser. The GUI is designed provide quick access to a range of features for managing your research data. This section provides a brief overview, with detailed guides for each functionality available separately. NOMAD Landing Page \u00b6 The NOMAD Landing Page is where you see all entries that have been published in NOMAD. You can always reach this page by choosing Entries in EXPLORE menu. The main menus on top, are always displayed in all NOMAD pages. Main Menus in NOMAD GUI \u00b6 PUBLISH : This menu is relevant for publishing your data. It allows you to: Upload or publish new data. Manage datasets, which may include multiple uploads. Search within the data that you have uploaded. EXPLORE : This menu allows you to explore all the data that have been published in NOMAD, or shared with you if you are logged in. Data is organized for exploration into several categories such as theoretical, experimental, and based on their use cases, e.g., solar cells. ANALYZE : This menu provides links to tools for data analysis directly from the GUI. This includes: Accessing data on NOMAD using its API. Browsing the NOMAD Metainfo Schema to search for specific quantities and locate data or metadata of interest. Utilizing a programming environment to run analysis codes, including Jupyter Notebooks and a Linux terminal. ABOUT : This menu provides information about NOMAD, including: General information and an overview of NOMAD. A link to a public forum, the matsci.org , where topics relevant to NOMAD are discussed among NOMAD users and developers. A link to the Frequently Asked Questions (FAQ). (This link is currently not functional.) The NOMAD Documentation, the main available source for informative materials about NOMAD, such as tutorials, how-to guides, domain-specific examples, and explainations. The main NOMAD GitLab Project, allowing access to the source code NOMAD. Terms of use of NOMAD. At the top right of the GUI, you will find the LOGIN / REGISTER button. There is also a UNITS button allows you to select the units you would like to see for the quantities in the GUI.","title":"NOMAD GUI"},{"location":"Module2/M2_x_NOMAD_GUI.html#nomad-gui","text":"","title":"NOMAD GUI"},{"location":"Module2/M2_x_NOMAD_GUI.html#navigating-to-nomad","text":"NOMAD website is the starting point for accessing NOMAD, where you can upload, manage, and explore data. To access the latest stable version of NOMAD (official release), click on the blue OPEN NOMAD button on the top right of the page. This will open the NOMAD Graphical User Interface (GUI) directly in your web browser. The GUI is designed provide quick access to a range of features for managing your research data. This section provides a brief overview, with detailed guides for each functionality available separately.","title":"Navigating to NOMAD"},{"location":"Module2/M2_x_NOMAD_GUI.html#nomad-landing-page","text":"The NOMAD Landing Page is where you see all entries that have been published in NOMAD. You can always reach this page by choosing Entries in EXPLORE menu. The main menus on top, are always displayed in all NOMAD pages.","title":"NOMAD Landing Page"},{"location":"Module2/M2_x_NOMAD_GUI.html#main-menus-in-nomad-gui","text":"PUBLISH : This menu is relevant for publishing your data. It allows you to: Upload or publish new data. Manage datasets, which may include multiple uploads. Search within the data that you have uploaded. EXPLORE : This menu allows you to explore all the data that have been published in NOMAD, or shared with you if you are logged in. Data is organized for exploration into several categories such as theoretical, experimental, and based on their use cases, e.g., solar cells. ANALYZE : This menu provides links to tools for data analysis directly from the GUI. This includes: Accessing data on NOMAD using its API. Browsing the NOMAD Metainfo Schema to search for specific quantities and locate data or metadata of interest. Utilizing a programming environment to run analysis codes, including Jupyter Notebooks and a Linux terminal. ABOUT : This menu provides information about NOMAD, including: General information and an overview of NOMAD. A link to a public forum, the matsci.org , where topics relevant to NOMAD are discussed among NOMAD users and developers. A link to the Frequently Asked Questions (FAQ). (This link is currently not functional.) The NOMAD Documentation, the main available source for informative materials about NOMAD, such as tutorials, how-to guides, domain-specific examples, and explainations. The main NOMAD GitLab Project, allowing access to the source code NOMAD. Terms of use of NOMAD. At the top right of the GUI, you will find the LOGIN / REGISTER button. There is also a UNITS button allows you to select the units you would like to see for the quantities in the GUI.","title":"Main Menus in NOMAD GUI"},{"location":"Module2/M2_x_create_account.html","text":"Create a NOMAD user account \u00b6 Creating a NOMAD account is quick: Visit nomad-lab.eu : Click on OPEN NOMAD in the top right corner. Click on LOGIN / REGISTER and select Register . Fill out the form. You are asked to enter a username (e.g., firstname.lastname), create a password, and provide your first name, last name, affiliation, and email address. Confirm your registration using the link sent to your Email.","title":"Create Account"},{"location":"Module2/M2_x_create_account.html#create-a-nomad-user-account","text":"Creating a NOMAD account is quick: Visit nomad-lab.eu : Click on OPEN NOMAD in the top right corner. Click on LOGIN / REGISTER and select Register . Fill out the form. You are asked to enter a username (e.g., firstname.lastname), create a password, and provide your first name, last name, affiliation, and email address. Confirm your registration using the link sent to your Email.","title":"Create a NOMAD user account"},{"location":"Module2/M2_x_customized_search_dashboard.html","text":"Custom Widgets for Advanced Searches in NOMAD \u00b6 One of the strengths of NOMAD is that it allows you to search entries based on rich metadata. There are different types of metadata: some are extracted automatically by NOMAD upon uploading the data (e.g., upload ID, date, and author), others are generated by NOMAD parsers, and some are provided directly by the user. Once data and metadata are connected through schemas in NOMAD, it becomes possible to search within these datasets using NOMAD's search filters. In NOMAD, the author has the choice to provide rich metadata or simply use the software as a basic storage platform. If rich metadata is provided according to schemas, it can be effectively used for searching. On the other hand, if (meta)data are provided not according to schemas (e.g., in free text fields) or stored externally, it cannot be utilized for searchability within NOMAD. Once (meta)data are stored using schemas in NOMAD they become machine-actionable. NOMAD has built-in customizable widgets that allow users to create advanced dashboards for exploring data. The buttons to create customizable widgets are located just below the search bar, once you are in any NOMAD page selected from the EXPLORE menu. Here are the main four widgets: TERMS: Visualize and explore categorical data based on user-defined terms and keywords. HISTOGRAM: Display the distribution of a specific numerical quantity within data. SCATTER PLOT: Generate scatter plots to visualize relationships between different quantities. PERIODIC TABLE: Filter data by selecting elements directly from an interactive periodic table. Create Customizable Widgets For ETL Materials in Solar Cells \u00b6 In the following, we'll walk you through an example to help you better understand how to use these widgets. Imagine we are working on solar cell research and have fabricated solar cell devices using the absorber material 'CsPbBrI' (Cesium Lead Bromine Iodide), a mixed halide perovskite. The device's structure is illustrated below: The Device: \u00b6 Contact: Au HTL (Hole Transport Layer): Spiro-OMeTAD (C81\u200bH68\u200bN4\u200bO8\u200b) Perovskite Absorber: CsPbBrI ETL (Electron Transport Layer): TiO2-c (compact Titanium Dioxide) Contact: FTO (Fluorine-doped Tin Oxide) Substrate: SLG (Soda Lime Glass) Now, we want to answer the following research question: Research Question: \u00b6 What ETL materials can replace TiO2-c to improve Voc (open circuit voltage) in perovskite solar cells? To gain insights into this question, we can utilize NOMAD's widgets to explore relevant data: Start with the Periodic Table : Click on the PERIODIC TABLE widget button and use the (+) button to pin it to the dashboard. <!-- this is not comprehinsive - clicking on the button opens a dialog box. There is no Plus icon> Select the elements of the absorber from the periodic table: Cs, Pb, Br, and I. After selecting these elements, you should see approximately 7,500 entries matching your search filters. Use the TERMS Widget : To find out what ETL and HTL materials are used in the available data, click on the TERMS widget button. For the X-axis, type 'electron transport layer'. As you type, suggestions will appear. Choose results.properties.optoelectronic.solar_cell.electron_transport_layer . <!-- screenshots or short gifs will be helpful> Set the statistics scaling to linear, give the widget a descriptive title like \"ETL\", and pin it to the dashboard. Repeat the process for the HTL materials. Create a Scatter Plot : Click on the SCATTER PLOT widget button to visualize the relationship between open circute voltage (Voc), short circuit current density (Jsc), and efficiency. Set the X-axis to \"Open Circuit Voltage (V)\", the Y-axis to \"Efficiency\", and use the marker color to represent \"Short Circuit Current Density\". The scatter plot will allow you to explore the data interactively. Interpreting the Results \u00b6 With the widgets configured, you can now explore the relationships between different ETL and HTL materials and their impact on efficiency, Voc, and short circuit current density. The scatter plot is interactive, allowing you to zoom in on specific data points. Hovering over a data point will display the exact values for the chosen properties. Clicking on a data point will take you to the corresponding entry page, where you can view detailed metadata, check if the data belongs to a dataset, and see if it has been published in a journal or has an associated DOI. NOMAD's customizable widgets offer a powerful way to gain insights from vast datasets, helping you to answer complex research questions and discover new patterns in data in short time.","title":"Customized Dashboards"},{"location":"Module2/M2_x_customized_search_dashboard.html#custom-widgets-for-advanced-searches-in-nomad","text":"One of the strengths of NOMAD is that it allows you to search entries based on rich metadata. There are different types of metadata: some are extracted automatically by NOMAD upon uploading the data (e.g., upload ID, date, and author), others are generated by NOMAD parsers, and some are provided directly by the user. Once data and metadata are connected through schemas in NOMAD, it becomes possible to search within these datasets using NOMAD's search filters. In NOMAD, the author has the choice to provide rich metadata or simply use the software as a basic storage platform. If rich metadata is provided according to schemas, it can be effectively used for searching. On the other hand, if (meta)data are provided not according to schemas (e.g., in free text fields) or stored externally, it cannot be utilized for searchability within NOMAD. Once (meta)data are stored using schemas in NOMAD they become machine-actionable. NOMAD has built-in customizable widgets that allow users to create advanced dashboards for exploring data. The buttons to create customizable widgets are located just below the search bar, once you are in any NOMAD page selected from the EXPLORE menu. Here are the main four widgets: TERMS: Visualize and explore categorical data based on user-defined terms and keywords. HISTOGRAM: Display the distribution of a specific numerical quantity within data. SCATTER PLOT: Generate scatter plots to visualize relationships between different quantities. PERIODIC TABLE: Filter data by selecting elements directly from an interactive periodic table.","title":"Custom Widgets for Advanced Searches in NOMAD"},{"location":"Module2/M2_x_customized_search_dashboard.html#create-customizable-widgets-for-etl-materials-in-solar-cells","text":"In the following, we'll walk you through an example to help you better understand how to use these widgets. Imagine we are working on solar cell research and have fabricated solar cell devices using the absorber material 'CsPbBrI' (Cesium Lead Bromine Iodide), a mixed halide perovskite. The device's structure is illustrated below:","title":"Create Customizable Widgets For ETL Materials in Solar Cells"},{"location":"Module2/M2_x_customized_search_dashboard.html#the-device","text":"Contact: Au HTL (Hole Transport Layer): Spiro-OMeTAD (C81\u200bH68\u200bN4\u200bO8\u200b) Perovskite Absorber: CsPbBrI ETL (Electron Transport Layer): TiO2-c (compact Titanium Dioxide) Contact: FTO (Fluorine-doped Tin Oxide) Substrate: SLG (Soda Lime Glass) Now, we want to answer the following research question:","title":"The Device:"},{"location":"Module2/M2_x_customized_search_dashboard.html#research-question","text":"What ETL materials can replace TiO2-c to improve Voc (open circuit voltage) in perovskite solar cells? To gain insights into this question, we can utilize NOMAD's widgets to explore relevant data: Start with the Periodic Table : Click on the PERIODIC TABLE widget button and use the (+) button to pin it to the dashboard. <!-- this is not comprehinsive - clicking on the button opens a dialog box. There is no Plus icon> Select the elements of the absorber from the periodic table: Cs, Pb, Br, and I. After selecting these elements, you should see approximately 7,500 entries matching your search filters. Use the TERMS Widget : To find out what ETL and HTL materials are used in the available data, click on the TERMS widget button. For the X-axis, type 'electron transport layer'. As you type, suggestions will appear. Choose results.properties.optoelectronic.solar_cell.electron_transport_layer . <!-- screenshots or short gifs will be helpful> Set the statistics scaling to linear, give the widget a descriptive title like \"ETL\", and pin it to the dashboard. Repeat the process for the HTL materials. Create a Scatter Plot : Click on the SCATTER PLOT widget button to visualize the relationship between open circute voltage (Voc), short circuit current density (Jsc), and efficiency. Set the X-axis to \"Open Circuit Voltage (V)\", the Y-axis to \"Efficiency\", and use the marker color to represent \"Short Circuit Current Density\". The scatter plot will allow you to explore the data interactively.","title":"Research Question:"},{"location":"Module2/M2_x_customized_search_dashboard.html#interpreting-the-results","text":"With the widgets configured, you can now explore the relationships between different ETL and HTL materials and their impact on efficiency, Voc, and short circuit current density. The scatter plot is interactive, allowing you to zoom in on specific data points. Hovering over a data point will display the exact values for the chosen properties. Clicking on a data point will take you to the corresponding entry page, where you can view detailed metadata, check if the data belongs to a dataset, and see if it has been published in a journal or has an associated DOI. NOMAD's customizable widgets offer a powerful way to gain insights from vast datasets, helping you to answer complex research questions and discover new patterns in data in short time.","title":"Interpreting the Results"},{"location":"Module2/M2_x_explore_tab.html","text":"Explore Data in NOMAD \u00b6 The EXPLORE menu allows you to navigate and search through a vast amount of materials-science data. Currently, the this menu offers several options for exploring data including: Entries : Search entries across all domains. Theory : Focus on calculations and materials data derived from theoretical models. Experiment : Explore data from experimental sources, such as ELNs (Electronic Lab Notebooks) or characterization techniques e.g., EELS (Electron Energy Loss Spectroscopy). Tools : Explore among several AI toolkit notebooks. Use Cases : Search data tailored to specific use cases, such as Solar Cells or Metal-Organic Frameworks (MOFs). In this section, we focus on searching data in NOMAD using the Entries option within the EXPLORE menu. You will see an overview of USE CASES , using Solar Cells example, in a later section . Explore NOMAD Using Entries \u00b6 To begin exploring NOMAD using entries, select Entries from the EXPLORE menu. What are Entries in NOMAD? Entries are individual simulations, workflows, or measurements that have been uploaded to NOMAD. Each entry contains detailed metadata, making it possible to be searched,analyzed, and reused. This page contains all entries available in NOMAD, showing the top 20 latest uploaded entries. Without logging in, only publicly available entries are visible. Once logged in, you can also access entries you created or those shared with you. Main Search Interface \u00b6 With millions of entries, mostly consisting of electronic structure calculations, NOMAD is expanding to include a broader range of materials science and physics data. This now includes sample data, measurements, characterization, and theoretical data at different scales. In fact the search interface is already revised to allow the increasingly complex filter options that are required for a wider range of entry types. The filter menu on the left allows you to create complex searches based on different perspectives, including: Material : Filter by elements, chemical formula, or structure/symmetry. Method : Filter by the scientific method used, such as various DFT codes. Properties : Filter by desired properties, such as electronic properties. Use Cases : Filter data relevant to specific applications, like solar cells. Origin : Filter based on the data's origin, such as the author or date of upload. Material Filter \u00b6 Let's start with a search based on material structure. The Elements / Formula filter menu allows you to filter entries by specific elements or chemical formulas. For instance, if you are interested in hexagonal boron nitride, select the B and N from the periodic table. As you select elements, the user interface dynamically updates, narrowing down the search results and altering the heatmap accordingly. The applied filters are displayed in their respective submenus, and you can remove individual filters or clear all filters to reset your search. The principle remains the same across different filter types \u2014 whether you are using the periodic table, formula input fields, structure/symmetry filters, or properties filters, your search is refined progressively. For example, you could filter down to materials with only two atoms of B and N, and further narrow it to those with hexagonal symmetry. The available filter options will adjust based on your selections, ensuring that the filters remain relevant to the data you are exploring. To be more clear, once you select the hexagonal symmetry, the Bravais lattice options relevant to the cubic symmetry disappear. Method Filter \u00b6 Under the Method menu, you can control the applied physics method. This allows you to specify whether you are looking for simulation data (e.g., DFT) or experimental data (e.g., EELS). For example, you could filter h-BN data down to only those entries created using VASP. Properties Filter \u00b6 The Properties filter menu allows you to focus on specific calculated or measured properties. For example, if you are interested in electronic properties, you can filter entries to display only those with available band structure or density of states data etc. Adding the Filters to the Search Interface \u00b6 You can customize your search interface by pinning filter items from any filter (sub)menu to the search page. Clicking the (+) button adds the selected filter to your custom search dashboard, allowing you to use them side by side. This feature helps uncover relationships within the data on NOMAD. For example, selecting Boron and fixing the symmetry to hexagonal might reveal a pattern where many entries involve elements from group 5 in the periodic table, reflecting the abundance of III-V semiconductor data in NOMAD. Important Note: The relationships you observe through the filters are dependent on the data published in NOMAD. Interpret them within the context of the available data, as they do not necessarily carry an inherent physical or scientific meaning. The Search Bar \u00b6 A very useful tool to explore entries in NOMAD is the search bar, which allows you to quickly find and apply filters. When you type anything in the search bar, NOMAD searches within various places and suggests matching filters. This is particularly helpful if you are unsure wheter a filter exist at all, or where it is located. For instance, entering a program name, functional name, or colleague\u2019s name will reveal corresponding filters. Do you think a bandgap filter exist? How will you find out if there is a filter for bandgap of materials in NOMAD? Does it give the bandgap value or the direct/indirect characteristic? Hint Try different terms or combinations of the words that seem logical or intuitive to you in the search bar. In this case it might includng \"bandgap\", \"band gap\", \"band_gap\" etc. Look at the search bar suggestions, to see if you can find the one you were looking for. Do the same for \"direct\" or \"indirect\".","title":"Explore Menu"},{"location":"Module2/M2_x_explore_tab.html#explore-data-in-nomad","text":"The EXPLORE menu allows you to navigate and search through a vast amount of materials-science data. Currently, the this menu offers several options for exploring data including: Entries : Search entries across all domains. Theory : Focus on calculations and materials data derived from theoretical models. Experiment : Explore data from experimental sources, such as ELNs (Electronic Lab Notebooks) or characterization techniques e.g., EELS (Electron Energy Loss Spectroscopy). Tools : Explore among several AI toolkit notebooks. Use Cases : Search data tailored to specific use cases, such as Solar Cells or Metal-Organic Frameworks (MOFs). In this section, we focus on searching data in NOMAD using the Entries option within the EXPLORE menu. You will see an overview of USE CASES , using Solar Cells example, in a later section .","title":"Explore Data in NOMAD"},{"location":"Module2/M2_x_explore_tab.html#explore-nomad-using-entries","text":"To begin exploring NOMAD using entries, select Entries from the EXPLORE menu. What are Entries in NOMAD? Entries are individual simulations, workflows, or measurements that have been uploaded to NOMAD. Each entry contains detailed metadata, making it possible to be searched,analyzed, and reused. This page contains all entries available in NOMAD, showing the top 20 latest uploaded entries. Without logging in, only publicly available entries are visible. Once logged in, you can also access entries you created or those shared with you.","title":"Explore NOMAD Using Entries"},{"location":"Module2/M2_x_explore_tab.html#main-search-interface","text":"With millions of entries, mostly consisting of electronic structure calculations, NOMAD is expanding to include a broader range of materials science and physics data. This now includes sample data, measurements, characterization, and theoretical data at different scales. In fact the search interface is already revised to allow the increasingly complex filter options that are required for a wider range of entry types. The filter menu on the left allows you to create complex searches based on different perspectives, including: Material : Filter by elements, chemical formula, or structure/symmetry. Method : Filter by the scientific method used, such as various DFT codes. Properties : Filter by desired properties, such as electronic properties. Use Cases : Filter data relevant to specific applications, like solar cells. Origin : Filter based on the data's origin, such as the author or date of upload.","title":"Main Search Interface"},{"location":"Module2/M2_x_explore_tab.html#material-filter","text":"Let's start with a search based on material structure. The Elements / Formula filter menu allows you to filter entries by specific elements or chemical formulas. For instance, if you are interested in hexagonal boron nitride, select the B and N from the periodic table. As you select elements, the user interface dynamically updates, narrowing down the search results and altering the heatmap accordingly. The applied filters are displayed in their respective submenus, and you can remove individual filters or clear all filters to reset your search. The principle remains the same across different filter types \u2014 whether you are using the periodic table, formula input fields, structure/symmetry filters, or properties filters, your search is refined progressively. For example, you could filter down to materials with only two atoms of B and N, and further narrow it to those with hexagonal symmetry. The available filter options will adjust based on your selections, ensuring that the filters remain relevant to the data you are exploring. To be more clear, once you select the hexagonal symmetry, the Bravais lattice options relevant to the cubic symmetry disappear.","title":"Material Filter"},{"location":"Module2/M2_x_explore_tab.html#method-filter","text":"Under the Method menu, you can control the applied physics method. This allows you to specify whether you are looking for simulation data (e.g., DFT) or experimental data (e.g., EELS). For example, you could filter h-BN data down to only those entries created using VASP.","title":"Method Filter"},{"location":"Module2/M2_x_explore_tab.html#properties-filter","text":"The Properties filter menu allows you to focus on specific calculated or measured properties. For example, if you are interested in electronic properties, you can filter entries to display only those with available band structure or density of states data etc.","title":"Properties Filter"},{"location":"Module2/M2_x_explore_tab.html#adding-the-filters-to-the-search-interface","text":"You can customize your search interface by pinning filter items from any filter (sub)menu to the search page. Clicking the (+) button adds the selected filter to your custom search dashboard, allowing you to use them side by side. This feature helps uncover relationships within the data on NOMAD. For example, selecting Boron and fixing the symmetry to hexagonal might reveal a pattern where many entries involve elements from group 5 in the periodic table, reflecting the abundance of III-V semiconductor data in NOMAD. Important Note: The relationships you observe through the filters are dependent on the data published in NOMAD. Interpret them within the context of the available data, as they do not necessarily carry an inherent physical or scientific meaning.","title":"Adding the Filters to the Search Interface"},{"location":"Module2/M2_x_explore_tab.html#the-search-bar","text":"A very useful tool to explore entries in NOMAD is the search bar, which allows you to quickly find and apply filters. When you type anything in the search bar, NOMAD searches within various places and suggests matching filters. This is particularly helpful if you are unsure wheter a filter exist at all, or where it is located. For instance, entering a program name, functional name, or colleague\u2019s name will reveal corresponding filters. Do you think a bandgap filter exist? How will you find out if there is a filter for bandgap of materials in NOMAD? Does it give the bandgap value or the direct/indirect characteristic? Hint Try different terms or combinations of the words that seem logical or intuitive to you in the search bar. In this case it might includng \"bandgap\", \"band gap\", \"band_gap\" etc. Look at the search bar suggestions, to see if you can find the one you were looking for. Do the same for \"direct\" or \"indirect\".","title":"The Search Bar"},{"location":"Module2/M2_x_explore_use_cases.html","text":"Exploring Data through Use Cases (Apps) in NOMAD \u00b6 The USE CASES option in the EXPLORE menu in NOMAD is designed to help researchers discover domain-specific data tailored to their research needs. By focusing on predefined categories, use cases provide an efficient approach to exploring datasets relevant to specific scientific domains, ensuring that researchers can find the most relevant data and use the dashboards readily designed for this purpose. What are Use Cases? \u00b6 Use cases in NOMAD refer to predefined categories of data that have been uploaded to NOMAD according to specifically designed schemas to meet the needs of a target research community. These categories are designed to help researchers quickly locate data that is most relevant to their field of study, without the need to explore large amounts of unrelated data. The schemas for use cases have been carefully and meticulously designed by domain experts in FAIRmat in tight collaboration with PIs in that specific field coming from a global research community. Currently Available Use Cases \u00b6 NOMAD currently offers two use cases: Solar Cells: Focuses on data related to the development and analysis of solar cell technologies. This includes data on photovoltaic materials, device performance, and related simulations. Metal-Organic Frameworks (MOFs): Covers data relevant to the study of MOFs, which are materials composed of metal ions coordinated to organic ligands. This use case includes structural data, adsorption properties, and related chemical simulations. The FAIRmat community is actively working on expanding/advancing the use cases to other domains. The use cases with expected releases in the near future are: Heterogeneous Catalysis: Focuses on data related to catalytic processes, including catalyst design, reaction mechanisms, and efficiency. Batteries: Encompasses data on battery materials, electrochemical performance, and simulations relevant to energy storage technologies. How to Explore Data Using the USE CASES Page \u00b6 The USE CASES page in the EXPLORE menu is designed to facilitate exploring data in that specific field. Once you choose a use case, a predefined dashboard containing various important widgets which offer key information and visualizations to explore and analyze the data (that comply with that use case schema) is presented. Each research topic may have details and intricacies that the predefined dashboard does not address. However, NOMAD offers high flexibility, allowing you to edit the dashboard to meet your needs or add your own visualizations. Advanced Tips for Using the USE CASES Page \u00b6 To make the most out of the use cases feature, consider the following tips: Combine Filters: Use multiple filters to narrow down your search results more precisely. For instance, you can combine material filters with performance metrics to find specific data points within the Solar Cells use case. Customize Your Search: Pin frequently used filters to your search interface for quicker access in future sessions. This can be particularly useful if you regularly search within the same use case. Interpret Data Relationships: Use the visual tools provided in NOMAD, such as histograms and scatter plots, to discover relationships between different data points within a use case. This can provide deeper insights into the data and help guide your research. An Example for Solar Cells \u00b6 Let's proceed with an example to address a specific research question within the domain of solar cell. Suppose we are working on Sn-based solar cell research, and the project is about using C60 as an electron transport layer in solar cells. We fabricated solar cell devices with an absorber material that contains Sn. The device's structure is illustrated below: Now, we want to answer the following research question: Which hole transport layer (HTL) materials improve efficiency in Sn-based solar cells with C60 as the electron transport layer (ETL)? Start by navigating to the EXPLORE menu in the NOMAD GUI. From here, select Solar Cells under USE CASES . Once you have selected the Solar Cells use case, you will be presented with a predefined search dashboard optimized for research in solar cells. This interface allows you to apply filters readily and easily narrow down the data to what is most relevant for your research. Dashboard Overview \u00b6 Periodic Table: Used to filter the elements of your main absorber material. For the HTL and ETL, other filters should be used. Scatter Plot: Visualizes the solar cell efficiency (y-axis), open circuit voltage (x-axis), and the short circuit current density (color map). This important plot provides insights on the relationship between efficiency and these electrical properties. Scatter Plot: Visualizes the solar cell efficiency (y-axis), open circuit voltage (x-axis), and the solar cell architecture, either p-i-n (pin) or n-i-p (nip) or Schottky. If it is not defined, it is marked as unknown. Histogram: Displays the bandgap of the solar cells data uploaded to NOMAD. It is interactive, meaning that you can use it to show the range that you are looking for. Histogram: Shows the illumination intensity, providing insights into the testing conditions for the solar cells. Interactive TERMS Plot: Categorizes the method for the fabrication of the absorber layer. Interactive TERMS Plot: Categorizes the device stack, providing information on the configuration of the solar cell layers. Interactive TERMS Plot: Categorizes the materials used for the ETL. Interactive TERMS Plot: Categorizes the materials used for the HTL. Apply Filters: Use the filters provided to refine your search. In our example, we are looking for Sn-based solar cells with C60 as the ETL. So let's choose Sn in the periodic table and fix the ETL on C60 in the interactive TERMS plot. These filters narrow down the results to about 400 entries. Add more filters if your research question can be better answered. For instance, if you are looking for absorber materials with band gaps greater than 1.3 eV, you can use the slider on the interactive bandgap histogram to narrow down your search further. Or, if you are only looking for pin architecture, click on pin on the respective scatter plot to filter down the results only to the ones with pin. Modify or Add Customized Widgets: All the widgets on each dashboard are customizable. By clicking on the pen icon on the top right of each widget, you can access the customizable quantities of that widget. For example, you can change the entity plotted on a specific axis, the color map, or the units in which that specific quantity is presented. Explore Results: Review the search results to find data entries that match your criteria. Here you can also look for specific entries in different visualizations. For example, you can zoom in on the scatter plots and hover your mouse pointer over each data point to read the value of the plotted entities. Clicking on each data point will bring you to the entry page. Each entry provides detailed metadata and links to the associated datasets, making it accessible for researchers to find and use the most relevant data for their work. In summary, the USE CASES (or apps) option in NOMAD EXPLORE menu offer a powerful way to explore domain-specific data, making it easier for researchers to find and use the most relevant data for their work. By leveraging these predefined categories and advanced search tools, you can streamline your data discovery process and focus more on advancing your research.","title":"Use Cases"},{"location":"Module2/M2_x_explore_use_cases.html#exploring-data-through-use-cases-apps-in-nomad","text":"The USE CASES option in the EXPLORE menu in NOMAD is designed to help researchers discover domain-specific data tailored to their research needs. By focusing on predefined categories, use cases provide an efficient approach to exploring datasets relevant to specific scientific domains, ensuring that researchers can find the most relevant data and use the dashboards readily designed for this purpose.","title":"Exploring Data through Use Cases (Apps) in NOMAD"},{"location":"Module2/M2_x_explore_use_cases.html#what-are-use-cases","text":"Use cases in NOMAD refer to predefined categories of data that have been uploaded to NOMAD according to specifically designed schemas to meet the needs of a target research community. These categories are designed to help researchers quickly locate data that is most relevant to their field of study, without the need to explore large amounts of unrelated data. The schemas for use cases have been carefully and meticulously designed by domain experts in FAIRmat in tight collaboration with PIs in that specific field coming from a global research community.","title":"What are Use Cases?"},{"location":"Module2/M2_x_explore_use_cases.html#currently-available-use-cases","text":"NOMAD currently offers two use cases: Solar Cells: Focuses on data related to the development and analysis of solar cell technologies. This includes data on photovoltaic materials, device performance, and related simulations. Metal-Organic Frameworks (MOFs): Covers data relevant to the study of MOFs, which are materials composed of metal ions coordinated to organic ligands. This use case includes structural data, adsorption properties, and related chemical simulations. The FAIRmat community is actively working on expanding/advancing the use cases to other domains. The use cases with expected releases in the near future are: Heterogeneous Catalysis: Focuses on data related to catalytic processes, including catalyst design, reaction mechanisms, and efficiency. Batteries: Encompasses data on battery materials, electrochemical performance, and simulations relevant to energy storage technologies.","title":"Currently Available Use Cases"},{"location":"Module2/M2_x_explore_use_cases.html#how-to-explore-data-using-the-use-cases-page","text":"The USE CASES page in the EXPLORE menu is designed to facilitate exploring data in that specific field. Once you choose a use case, a predefined dashboard containing various important widgets which offer key information and visualizations to explore and analyze the data (that comply with that use case schema) is presented. Each research topic may have details and intricacies that the predefined dashboard does not address. However, NOMAD offers high flexibility, allowing you to edit the dashboard to meet your needs or add your own visualizations.","title":"How to Explore Data Using the USE CASES Page"},{"location":"Module2/M2_x_explore_use_cases.html#advanced-tips-for-using-the-use-cases-page","text":"To make the most out of the use cases feature, consider the following tips: Combine Filters: Use multiple filters to narrow down your search results more precisely. For instance, you can combine material filters with performance metrics to find specific data points within the Solar Cells use case. Customize Your Search: Pin frequently used filters to your search interface for quicker access in future sessions. This can be particularly useful if you regularly search within the same use case. Interpret Data Relationships: Use the visual tools provided in NOMAD, such as histograms and scatter plots, to discover relationships between different data points within a use case. This can provide deeper insights into the data and help guide your research.","title":"Advanced Tips for Using the USE CASES Page"},{"location":"Module2/M2_x_explore_use_cases.html#an-example-for-solar-cells","text":"Let's proceed with an example to address a specific research question within the domain of solar cell. Suppose we are working on Sn-based solar cell research, and the project is about using C60 as an electron transport layer in solar cells. We fabricated solar cell devices with an absorber material that contains Sn. The device's structure is illustrated below: Now, we want to answer the following research question: Which hole transport layer (HTL) materials improve efficiency in Sn-based solar cells with C60 as the electron transport layer (ETL)? Start by navigating to the EXPLORE menu in the NOMAD GUI. From here, select Solar Cells under USE CASES . Once you have selected the Solar Cells use case, you will be presented with a predefined search dashboard optimized for research in solar cells. This interface allows you to apply filters readily and easily narrow down the data to what is most relevant for your research.","title":"An Example for Solar Cells"},{"location":"Module2/M2_x_explore_use_cases.html#dashboard-overview","text":"Periodic Table: Used to filter the elements of your main absorber material. For the HTL and ETL, other filters should be used. Scatter Plot: Visualizes the solar cell efficiency (y-axis), open circuit voltage (x-axis), and the short circuit current density (color map). This important plot provides insights on the relationship between efficiency and these electrical properties. Scatter Plot: Visualizes the solar cell efficiency (y-axis), open circuit voltage (x-axis), and the solar cell architecture, either p-i-n (pin) or n-i-p (nip) or Schottky. If it is not defined, it is marked as unknown. Histogram: Displays the bandgap of the solar cells data uploaded to NOMAD. It is interactive, meaning that you can use it to show the range that you are looking for. Histogram: Shows the illumination intensity, providing insights into the testing conditions for the solar cells. Interactive TERMS Plot: Categorizes the method for the fabrication of the absorber layer. Interactive TERMS Plot: Categorizes the device stack, providing information on the configuration of the solar cell layers. Interactive TERMS Plot: Categorizes the materials used for the ETL. Interactive TERMS Plot: Categorizes the materials used for the HTL. Apply Filters: Use the filters provided to refine your search. In our example, we are looking for Sn-based solar cells with C60 as the ETL. So let's choose Sn in the periodic table and fix the ETL on C60 in the interactive TERMS plot. These filters narrow down the results to about 400 entries. Add more filters if your research question can be better answered. For instance, if you are looking for absorber materials with band gaps greater than 1.3 eV, you can use the slider on the interactive bandgap histogram to narrow down your search further. Or, if you are only looking for pin architecture, click on pin on the respective scatter plot to filter down the results only to the ones with pin. Modify or Add Customized Widgets: All the widgets on each dashboard are customizable. By clicking on the pen icon on the top right of each widget, you can access the customizable quantities of that widget. For example, you can change the entity plotted on a specific axis, the color map, or the units in which that specific quantity is presented. Explore Results: Review the search results to find data entries that match your criteria. Here you can also look for specific entries in different visualizations. For example, you can zoom in on the scatter plots and hover your mouse pointer over each data point to read the value of the plotted entities. Clicking on each data point will bring you to the entry page. Each entry provides detailed metadata and links to the associated datasets, making it accessible for researchers to find and use the most relevant data for their work. In summary, the USE CASES (or apps) option in NOMAD EXPLORE menu offer a powerful way to explore domain-specific data, making it easier for researchers to find and use the most relevant data for their work. By leveraging these predefined categories and advanced search tools, you can streamline your data discovery process and focus more on advancing your research.","title":"Dashboard Overview"},{"location":"Module2/M2_x_inspecting_entry_pages.html","text":"Exploring Uploaded Data in NOMAD \u00b6 Inspecting the Entries in the Explore Page \u00b6 In the previous section , we learned how to use the GUI filter menu, to filter the entries which match our search criteria. Now let's look at the the search results in more details. In the center of the page, you will find a list of all entries that fulfill our chosen search criteria. By default, the GUI lists the 20 newest entries first, but we can load more entries by clicking on the LOAD MORE at the bottom of the list. You can also customize the columns displayed by using the button with three vertical thick lines on the top right. Clicking on an entry (anywhere except on the right arrow!) brings up a brief overview of key metadata. On the left, we have the metadata extracted from the uploaded file by the NOMAD parsers; in the middle, we have the user-provided metadata; and on the right, we have the persistent identifiers for the entry and its upload, as well as, the path for the file in NOMAD. Opening the Entry Page \u00b6 Clicking on right arrow next to an entry navigates you to the respective entry page. On the entry page, you will see a general overview of the entry. On the left, core metadata are displayed again. On the right, various cards present the available information. The cards you see depend on the properties or data available for that entry. For instance, here we see cards for the electronic properties that we searched for in the previous section . The electronic properties card shows us the band structure and density of states. These are interactive; you can zoom in and move them around. You might also see a picture of the Brillouin Zone and information about possible bandgaps etc. Note that the availability of certain information may differ from entry to entry. Raw and Processed Data in the FILES and DATA Tabs \u00b6 After we could see an overview of the entry in the entry page, we might be interested to inspect more details: FILES Tab: This tab shows the uploaded files. NOMAD lists all files in the same directory, as they usually belong together. The mainfile for this entry is displayed at the top. For example, if it is a VASP run, the mainfile would be the XML file. All the information in this entry are created from those files. You can preview text files and download all or individual files. What is a Mainfile? Each entry has one raw file that defines it, referred to as the mainfile . Typically, most, if not all, processed data of an entry is derived from this mainfile. DATA Tab: Also known as the \"processed data\" tab, this shows the results of the parsing and normalization process done by NOMAD. NOMAD puts all the data in a unified, hierarchical, and machine-processable format, following the NOMAD metainfo.","title":"Entry Pages"},{"location":"Module2/M2_x_inspecting_entry_pages.html#exploring-uploaded-data-in-nomad","text":"","title":"Exploring Uploaded Data in NOMAD"},{"location":"Module2/M2_x_inspecting_entry_pages.html#inspecting-the-entries-in-the-explore-page","text":"In the previous section , we learned how to use the GUI filter menu, to filter the entries which match our search criteria. Now let's look at the the search results in more details. In the center of the page, you will find a list of all entries that fulfill our chosen search criteria. By default, the GUI lists the 20 newest entries first, but we can load more entries by clicking on the LOAD MORE at the bottom of the list. You can also customize the columns displayed by using the button with three vertical thick lines on the top right. Clicking on an entry (anywhere except on the right arrow!) brings up a brief overview of key metadata. On the left, we have the metadata extracted from the uploaded file by the NOMAD parsers; in the middle, we have the user-provided metadata; and on the right, we have the persistent identifiers for the entry and its upload, as well as, the path for the file in NOMAD.","title":"Inspecting the Entries in the Explore Page"},{"location":"Module2/M2_x_inspecting_entry_pages.html#opening-the-entry-page","text":"Clicking on right arrow next to an entry navigates you to the respective entry page. On the entry page, you will see a general overview of the entry. On the left, core metadata are displayed again. On the right, various cards present the available information. The cards you see depend on the properties or data available for that entry. For instance, here we see cards for the electronic properties that we searched for in the previous section . The electronic properties card shows us the band structure and density of states. These are interactive; you can zoom in and move them around. You might also see a picture of the Brillouin Zone and information about possible bandgaps etc. Note that the availability of certain information may differ from entry to entry.","title":"Opening the Entry Page"},{"location":"Module2/M2_x_inspecting_entry_pages.html#raw-and-processed-data-in-the-files-and-data-tabs","text":"After we could see an overview of the entry in the entry page, we might be interested to inspect more details: FILES Tab: This tab shows the uploaded files. NOMAD lists all files in the same directory, as they usually belong together. The mainfile for this entry is displayed at the top. For example, if it is a VASP run, the mainfile would be the XML file. All the information in this entry are created from those files. You can preview text files and download all or individual files. What is a Mainfile? Each entry has one raw file that defines it, referred to as the mainfile . Typically, most, if not all, processed data of an entry is derived from this mainfile. DATA Tab: Also known as the \"processed data\" tab, this shows the results of the parsing and normalization process done by NOMAD. NOMAD puts all the data in a unified, hierarchical, and machine-processable format, following the NOMAD metainfo.","title":"Raw and Processed Data in the FILES and DATA Tabs"},{"location":"Module3/1_Overview.html","text":"========= NOMAD can be considered as a database for materials-science data. A database is a structured collection of data designed to capture the relationships between sets of similar data. These specific pieces of data are referred to as data records. In NOMAD, data records are called Entries . Entries in NOMAD can be searched for, and each entry has its own individual page on the NOMAD GUI. Every entry in NOMAD (data record) includes a set of attributes defining information about the respective record. In databases, at least one attribute must serve as the unique identifier for the record, and this designated attribute becomes the primary key. In NOMAD, the primary key for an Entry is the Entry ID , and is assigned automatically by NOMAD. Entries related to the same subject (application, experimental method, computation method, etc.) are organized into groups, each possessing a defined organizational structure. The arrangement of a data group is known as a schema , serving as a map that outlines the relationships between each attribute in a data record. Importantly, all records within a data group adhere to the same schema. The precise definition of the schema is crucial for optimizing database searches and constructing FAIR (Findable, Accessible, Interoperable, and Reusable) data. In this module, we will learn how a user can create entries of structured data in NOMAD. Upon completion of the module, participants will be able to: 1. Illustrate various methods for inserting information into NOMAD to create entries. 2. Identify and utilize built-in schemas within NOMAD. 3. Develop and implement custom schemas to meet specific data requirements. 4. Proficiently create and employ a personalized Electronic Laboratory Notebook (ELN) within the NOMAD platform.","title":"1 Overview"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_0_ELN_overview_combined_concise.html","text":"Overview \u00b6 An Electronic Lab Notebook (ELN) is a digital platform for recording, organizing, and sharing experimental activities, data and metadata. It provides scientists with searchable records, accessible from any device, allowing for more efficient data integration, enhanced collaboration, and robust research documentation. Benefits of Using an ELN Searchability: Instantly locate and retrieve samples data and experimental details. Organization: Store all experimental data, from raw measurements to metadata, in one place. Collaboration: Easily share and manage projects with your team. Security: Keep your data backed up and protected from loss or damage. For more reasons to implement an ELN, check out this external resource on how ELNs enhance research reproducibility. In this module, you will be guided through NOMAD ELN functionality to efficiently manage your research data. You will learn to design experiments, document procedures, organize data, and share results with colleagues. In a typical experiment, you will work through these main processes: Planning: Outline your resources and steps. Documenting: Record materials, procedures, and results. Managing: Organize data and notes for easy access. Sharing: Share your progress and results with your team members or collaborators. Next, we will explore each of these processes in more detail. Planning Your Experiments \u00b6 Planning is the starting point to effectively conduct your experiments and obtain meaningful results and insights. This is the foundation for taking full advantage of NOMAD's ELN functionality. Here are a few foundational steps to consider: Identify Materials and Instruments : List the materials and instruments needed for your experiments. Create a Sample List : Prepare a list of samples that will be used or prepared, obviously, as far as you can plan. Define Processes and Conditions : Outline the processes, instruments, and conditions required for sample preparation. Listing of Measurements : Develop a detailed list of measurements for examining samples. Define Experimental Procedures : Establish procedures that cover each experimental step from sample preparation to measurements and analysis. Documenting Your Experiments \u00b6 After planning, the next step is to document those plans by creating a record for each entity and activity involved in your experiment. Using NOMAD's ELN for documentation creates a thorough and organized record of your experimental work. Here's how to document your plans with NOMAD: Record Materials and Instruments : Create a record for each substance, tool, and instrument, including detailed descriptions and parameters. List Samples and Preparations : Document the list of samples to be used or prepared, along with their specific characteristics and preparation methods. Outline Measurement Parameters : Clearly define the measurements for studying the samples, specifying techniques and parameters to be used. Document Experimental Procedures : Describe the steps from sample preparation to measurement and analysis. Make sure that each step is meticulously recorded. NOMAD offers built-in schemas designed to document key aspects of experiments. While these cover essential and general needs, advanced users can create custom schemas for more specific requirements. Here is a list of built-in schemas to start with: Entity or Activity NOMAD's built-in schema name Materials Substance ELN Tools or instruments Instrument ELN Samples Generic sample ELN Sample preparation Material processing ELN Characterization Measurement ELN Experiment Experiment ELN Managing Your Experiments \u00b6 After planning and documenting all entities and activities involved in your experiment, the next important step is to effectively manage them. In NOMAD, managing experiments means integrating various (meta)data records, which are referred to as Entities or Activities in NOMAD\u2019s data model, into a single experiment record. This process sets up a clearly defined workflow and connects all entries using schemas. This is accomplished by creating references between entries, such as Entities or Activities . For example, you can link all the measurements performed on a single sample, or reference each prepared sample to its elemental constituents, etc. Sharing Your Progress and Outcomes \u00b6 Finally, it is essential to share your scientific progress and findings with others. Sharing your research progress and findings with team members or collaborators not only keeps everyone aligned but also allows for real-time input and feedback. NOMAD allows for sharing ELNs with selected individuals\u2014whether the project's PIs, team members, collaborators, or the entire research community\u2014with just a few clicks. Learning Objectives \u00b6 In this module, we will learn how to create entries of structured data in NOMAD. Upon completion of the module, participants will be able to: Illustrate various methods for inserting information into NOMAD to create entries. Identify and utilize built-in schemas within NOMAD. Develop and implement custom schemas to meet specific data requirements. Proficiently create and employ a personalized Electronic LaboratoryNotebook (ELN) within the NOMAD platform.","title":"Overview"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_0_ELN_overview_combined_concise.html#overview","text":"An Electronic Lab Notebook (ELN) is a digital platform for recording, organizing, and sharing experimental activities, data and metadata. It provides scientists with searchable records, accessible from any device, allowing for more efficient data integration, enhanced collaboration, and robust research documentation. Benefits of Using an ELN Searchability: Instantly locate and retrieve samples data and experimental details. Organization: Store all experimental data, from raw measurements to metadata, in one place. Collaboration: Easily share and manage projects with your team. Security: Keep your data backed up and protected from loss or damage. For more reasons to implement an ELN, check out this external resource on how ELNs enhance research reproducibility. In this module, you will be guided through NOMAD ELN functionality to efficiently manage your research data. You will learn to design experiments, document procedures, organize data, and share results with colleagues. In a typical experiment, you will work through these main processes: Planning: Outline your resources and steps. Documenting: Record materials, procedures, and results. Managing: Organize data and notes for easy access. Sharing: Share your progress and results with your team members or collaborators. Next, we will explore each of these processes in more detail.","title":"Overview"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_0_ELN_overview_combined_concise.html#planning-your-experiments","text":"Planning is the starting point to effectively conduct your experiments and obtain meaningful results and insights. This is the foundation for taking full advantage of NOMAD's ELN functionality. Here are a few foundational steps to consider: Identify Materials and Instruments : List the materials and instruments needed for your experiments. Create a Sample List : Prepare a list of samples that will be used or prepared, obviously, as far as you can plan. Define Processes and Conditions : Outline the processes, instruments, and conditions required for sample preparation. Listing of Measurements : Develop a detailed list of measurements for examining samples. Define Experimental Procedures : Establish procedures that cover each experimental step from sample preparation to measurements and analysis.","title":"Planning Your Experiments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_0_ELN_overview_combined_concise.html#documenting-your-experiments","text":"After planning, the next step is to document those plans by creating a record for each entity and activity involved in your experiment. Using NOMAD's ELN for documentation creates a thorough and organized record of your experimental work. Here's how to document your plans with NOMAD: Record Materials and Instruments : Create a record for each substance, tool, and instrument, including detailed descriptions and parameters. List Samples and Preparations : Document the list of samples to be used or prepared, along with their specific characteristics and preparation methods. Outline Measurement Parameters : Clearly define the measurements for studying the samples, specifying techniques and parameters to be used. Document Experimental Procedures : Describe the steps from sample preparation to measurement and analysis. Make sure that each step is meticulously recorded. NOMAD offers built-in schemas designed to document key aspects of experiments. While these cover essential and general needs, advanced users can create custom schemas for more specific requirements. Here is a list of built-in schemas to start with: Entity or Activity NOMAD's built-in schema name Materials Substance ELN Tools or instruments Instrument ELN Samples Generic sample ELN Sample preparation Material processing ELN Characterization Measurement ELN Experiment Experiment ELN","title":"Documenting Your Experiments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_0_ELN_overview_combined_concise.html#managing-your-experiments","text":"After planning and documenting all entities and activities involved in your experiment, the next important step is to effectively manage them. In NOMAD, managing experiments means integrating various (meta)data records, which are referred to as Entities or Activities in NOMAD\u2019s data model, into a single experiment record. This process sets up a clearly defined workflow and connects all entries using schemas. This is accomplished by creating references between entries, such as Entities or Activities . For example, you can link all the measurements performed on a single sample, or reference each prepared sample to its elemental constituents, etc.","title":"Managing Your Experiments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_0_ELN_overview_combined_concise.html#sharing-your-progress-and-outcomes","text":"Finally, it is essential to share your scientific progress and findings with others. Sharing your research progress and findings with team members or collaborators not only keeps everyone aligned but also allows for real-time input and feedback. NOMAD allows for sharing ELNs with selected individuals\u2014whether the project's PIs, team members, collaborators, or the entire research community\u2014with just a few clicks.","title":"Sharing Your Progress and Outcomes"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_0_ELN_overview_combined_concise.html#learning-objectives","text":"In this module, we will learn how to create entries of structured data in NOMAD. Upon completion of the module, participants will be able to: Illustrate various methods for inserting information into NOMAD to create entries. Identify and utilize built-in schemas within NOMAD. Develop and implement custom schemas to meet specific data requirements. Proficiently create and employ a personalized Electronic LaboratoryNotebook (ELN) within the NOMAD platform.","title":"Learning Objectives"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_1_planning_your_experiments.html","text":"Planning Your Expriments \u00b6 Planning is the critical first step in effectively conducting your experiments to obtain meaningful results and conclusions. This is the foundation for taking full advantage of NOMAD's ELN functionality and the ELN's built-in schemas. This involves several key actions: Identify Materials and Instruments : Identify the materials and instruments that will be essential throughout your experiments. Create a Sample List : Compile a comprehensive list of samples that will either be used or prepared for your experiments. Define Processes and Conditions : Clearly outline the processes, instruments, and conditions required to accurately prepare the samples. Listing of Measurements : Develop a detailed list of measurements that will be used to examine the prepared samples. Define Experimental Workflows : Establish clear experimental workflows that cover each step from sample preparation to measurements and analysis. By carefully completing these planning tasks, you will lay a solid foundation for the smooth execution and meaningful results of your scientific endeavors.","title":"Planning Your Expriments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_1_planning_your_experiments.html#planning-your-expriments","text":"Planning is the critical first step in effectively conducting your experiments to obtain meaningful results and conclusions. This is the foundation for taking full advantage of NOMAD's ELN functionality and the ELN's built-in schemas. This involves several key actions: Identify Materials and Instruments : Identify the materials and instruments that will be essential throughout your experiments. Create a Sample List : Compile a comprehensive list of samples that will either be used or prepared for your experiments. Define Processes and Conditions : Clearly outline the processes, instruments, and conditions required to accurately prepare the samples. Listing of Measurements : Develop a detailed list of measurements that will be used to examine the prepared samples. Define Experimental Workflows : Establish clear experimental workflows that cover each step from sample preparation to measurements and analysis. By carefully completing these planning tasks, you will lay a solid foundation for the smooth execution and meaningful results of your scientific endeavors.","title":"Planning Your Expriments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_2_documenting_your_experiments.html","text":"Documenting Your Experiments \u00b6 After carefully planning your experiments, the next critical step is to document those plans and their excution by creating a record for each entity and activity involved in your experiment. Using NOMAD as an ELN for documentation ensures a thorough and efficient record of your experimental strategies. Here's how to document your plans with NOMAD: Record Materials and Instruments : Create a record for each substance, tool, and instrument that will be used in your experiment, including detailed descriptions and relevant parameters. List Samples and Preparations : Document the list of samples to be used or prepared, along with their specific characteristics and preparation methods. Outline Measurement Parameters : Clearly define the measurements performed to study the samples, specifying the parameters and techniques to be used. Document Experimental Procedures : Describe the experimental workflow from sample preparation through measurements and analysis, ensuring that each step is meticulously recorded. By documenting your experimental plans with NOMAD, you create a comprehensive record that not only helps maintain clarity and organization, but also facilitates reproducibility and collaboration with other researchers. NOMAD offers specifically tailored built-in schemas that can be used to create the records mentioned above: Entity or Activity NOMAD ELN schema Materials Substance ELN Tools or instruments Instrument ELN Samples Generic sample ELN Sample preparation Material processing ELN Characterization Measurement ELN Experiment Experiment ELN","title":"Documenting Your Experiments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_2_documenting_your_experiments.html#documenting-your-experiments","text":"After carefully planning your experiments, the next critical step is to document those plans and their excution by creating a record for each entity and activity involved in your experiment. Using NOMAD as an ELN for documentation ensures a thorough and efficient record of your experimental strategies. Here's how to document your plans with NOMAD: Record Materials and Instruments : Create a record for each substance, tool, and instrument that will be used in your experiment, including detailed descriptions and relevant parameters. List Samples and Preparations : Document the list of samples to be used or prepared, along with their specific characteristics and preparation methods. Outline Measurement Parameters : Clearly define the measurements performed to study the samples, specifying the parameters and techniques to be used. Document Experimental Procedures : Describe the experimental workflow from sample preparation through measurements and analysis, ensuring that each step is meticulously recorded. By documenting your experimental plans with NOMAD, you create a comprehensive record that not only helps maintain clarity and organization, but also facilitates reproducibility and collaboration with other researchers. NOMAD offers specifically tailored built-in schemas that can be used to create the records mentioned above: Entity or Activity NOMAD ELN schema Materials Substance ELN Tools or instruments Instrument ELN Samples Generic sample ELN Sample preparation Material processing ELN Characterization Measurement ELN Experiment Experiment ELN","title":"Documenting Your Experiments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_3_managing_your_experiments.html","text":"Managing Your Experiments \u00b6 After carefully planning your experiments and thoroughly documenting all entities and activities involved, the next crucial step is to effectively manage them. Efficient experiment management is essential for maintaining organization and ensuring the quality of research outcomes. In NOMAD, the experiment management process involves integrating multiple (meta)data records into a single experiment, establishing a clear workflow, and creating comprehensive connections to all entries using the built-in schemas. NOMAD also facilitates recording and analyzing data. By using NOMAD's features, you can streamline the experimental process, ensuring accurate documentation, efficient workflow management, and thorough analysis. This leads to robust and reliable research outcomes. This is accomplished by creating references between entries, such as entities or activities. For example, you can link all the measurements performed on a single sample, or reference each prepared sample to its constituents.","title":"Managing Your Experiments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_3_managing_your_experiments.html#managing-your-experiments","text":"After carefully planning your experiments and thoroughly documenting all entities and activities involved, the next crucial step is to effectively manage them. Efficient experiment management is essential for maintaining organization and ensuring the quality of research outcomes. In NOMAD, the experiment management process involves integrating multiple (meta)data records into a single experiment, establishing a clear workflow, and creating comprehensive connections to all entries using the built-in schemas. NOMAD also facilitates recording and analyzing data. By using NOMAD's features, you can streamline the experimental process, ensuring accurate documentation, efficient workflow management, and thorough analysis. This leads to robust and reliable research outcomes. This is accomplished by creating references between entries, such as entities or activities. For example, you can link all the measurements performed on a single sample, or reference each prepared sample to its constituents.","title":"Managing Your Experiments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_4_sharing_your_experiments.html","text":"Sharing Your Progress and Outcomes \u00b6 Finally, it is essential to share your progress and findings with others. This involves keeping your collaborators updated on your latest experiments and enabling them to record their own experiments. By fostering open communication and collaboration, you can benefit from shared insights and collective expertise, accelerating the pace of scientific discovery and innovation. NOMAD allows you to share your ELNs with your PI, collaborators, and the community.","title":"Sharing Your Progress and Outcomes"},{"location":"Module3/5_ELN_using_built-in_schema/M3_1_4_sharing_your_experiments.html#sharing-your-progress-and-outcomes","text":"Finally, it is essential to share your progress and findings with others. This involves keeping your collaborators updated on your latest experiments and enabling them to record their own experiments. By fostering open communication and collaboration, you can benefit from shared insights and collective expertise, accelerating the pace of scientific discovery and innovation. NOMAD allows you to share your ELNs with your PI, collaborators, and the community.","title":"Sharing Your Progress and Outcomes"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_0_getting_started.html","text":"Getting Started \u00b6 NOMAD is a free, and open-source data management platform for materials science, whose goal is to make scientific research data FAIR (findable, accessible, interoperable and reusable). NOMAD provides tools for data management, sharing, and publishing. The platform lets you structure, explore, and analyze your data and the data of others. Similar to general use databases, NOMAD manages a structured collection of data and is designed to capture the relationships between sets of similar data. These specific pieces of data are referred to as data records. Introduction to NOMAD Entries \u00b6 In NOMAD, data records are called Entries , which have individual pages that can be explored using the NOMAD graphical user interface (GUI) or programmatically via the API. Every entry in NOMAD (data record) includes a set of attributes defining information about the respective record. In databases generally, at least one attribute serves as the unique identifier for the record, known as the primary key. In NOMAD, the primary key for an entry is the entry_id , and is assigned automatically by NOMAD. Entries related to the same subject (application, experimental method, computation method, etc.) are organized into groups, each with a defined organizational structure. The arrangement of a data group is known as a schema , serving as a map that outlines the relationships between each attribute in a data record. Importantly, all records within a data group adhere to the same schema. The precise definition of the schema is crucial for optimizing database searches and constructing FAIR(Findable, Accessible, Interoperable, and Reusable) data. Entries in NOMAD ELN \u00b6 Using the NOMAD ELN functionality, you will want to capture all the information related to the substances, samples, and instruments that will be used in your experiment. Furthermore, documenting the measurements performed and the analysis of the acquired data will also be needed. For each record, an entry in NOMAD is created based on a data schema that defines the structure. A schema serves as a template with input fields for various parameters of each entry. NOMAD offers a wide range of built-in schemas for general use, as well as, the ability to create your own schema and uploading it to NOMAD. Learning Objectives \u00b6 In this module, we will learn how a user can create entries of structured data in NOMAD. Upon completion of the module, participants will be able to: Illustrate various methods for inserting information into NOMAD to create entries. Identify and utilize built-in schemas within NOMAD. Develop and implement custom schemas to meet specific data requirements. Proficiently create and employ a personalized Electronic LaboratoryNotebook (ELN) within the NOMAD platform.","title":"Getting Started"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_0_getting_started.html#getting-started","text":"NOMAD is a free, and open-source data management platform for materials science, whose goal is to make scientific research data FAIR (findable, accessible, interoperable and reusable). NOMAD provides tools for data management, sharing, and publishing. The platform lets you structure, explore, and analyze your data and the data of others. Similar to general use databases, NOMAD manages a structured collection of data and is designed to capture the relationships between sets of similar data. These specific pieces of data are referred to as data records.","title":"Getting Started"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_0_getting_started.html#introduction-to-nomad-entries","text":"In NOMAD, data records are called Entries , which have individual pages that can be explored using the NOMAD graphical user interface (GUI) or programmatically via the API. Every entry in NOMAD (data record) includes a set of attributes defining information about the respective record. In databases generally, at least one attribute serves as the unique identifier for the record, known as the primary key. In NOMAD, the primary key for an entry is the entry_id , and is assigned automatically by NOMAD. Entries related to the same subject (application, experimental method, computation method, etc.) are organized into groups, each with a defined organizational structure. The arrangement of a data group is known as a schema , serving as a map that outlines the relationships between each attribute in a data record. Importantly, all records within a data group adhere to the same schema. The precise definition of the schema is crucial for optimizing database searches and constructing FAIR(Findable, Accessible, Interoperable, and Reusable) data.","title":"Introduction to NOMAD Entries"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_0_getting_started.html#entries-in-nomad-eln","text":"Using the NOMAD ELN functionality, you will want to capture all the information related to the substances, samples, and instruments that will be used in your experiment. Furthermore, documenting the measurements performed and the analysis of the acquired data will also be needed. For each record, an entry in NOMAD is created based on a data schema that defines the structure. A schema serves as a template with input fields for various parameters of each entry. NOMAD offers a wide range of built-in schemas for general use, as well as, the ability to create your own schema and uploading it to NOMAD.","title":"Entries in NOMAD ELN"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_0_getting_started.html#learning-objectives","text":"In this module, we will learn how a user can create entries of structured data in NOMAD. Upon completion of the module, participants will be able to: Illustrate various methods for inserting information into NOMAD to create entries. Identify and utilize built-in schemas within NOMAD. Develop and implement custom schemas to meet specific data requirements. Proficiently create and employ a personalized Electronic LaboratoryNotebook (ELN) within the NOMAD platform.","title":"Learning Objectives"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_0_getting_started_combined_concise.html","text":"Entries in NOMAD \u00b6 NOMAD is a free, open-source data management platform for materials science, aimed at making scientific research data FAIR. Similar to general-use databases, NOMAD manages a structured collection of data and is designed to capture relationships between them. In general database terminology, these pieces of data are referred to as data records. In NOMAD, they are called Entries . Entries in NOMAD can be visualized in the NOMAD graphical user interface (GUI) as well as accessed programmatically via its API. In general databases, every data record includes a set of attributes describing it. One attribute, always present, serves as a unique identifier for that data record, which is called the primary key of that data record. In NOMAD, each Entry (data record) has an entry_id as its primary key, which is automatically assigned by NOMAD. Entries related to the same topic, such as application, experimental method, computation method, are organized into groups with a defined structure . This structured organization in NOMAD is referred to as schema . A schema outlines relationships between attributes within each entry, which can be used to make a template with input fields for various entry parameters. This schema-based organization is essential for optimizing database searches needed for FAIR data. NOMAD offers a wide range of built-in schemas for general use, while also allowing users to create and upload custom schemas for their specific needs. This flexibility allow users to define new data structures for their particular types of research data, while all entries within the same data group follow the same schema, ensuring consistency, interoperability and reusability across records. NOMAD ELN Entries \u00b6 Using the NOMAD ELN functionality, you can capture comprehensive information about substances, samples, instruments, and experimental workflows. Each ELN record in NOMAD is created based on a schema that defines the structure and necessary parameters. For instance, the Measurement ELN schema is designed for documenting characterization measurements. This schema allows you to record specific measurement details, such as instrument settings, calibration data, and the actual measured physical properties (like absorption or diffraction patterns). Learning Objectives \u00b6 In this module, we will learn how to create entries of structured data in NOMAD. Upon completion of the module, participants will be able to: Illustrate various methods for inserting information into NOMAD to create entries. Identify and utilize built-in schemas within NOMAD. Develop and implement custom schemas to meet specific data requirements. Proficiently create and employ a personalized Electronic LaboratoryNotebook (ELN) within the NOMAD platform.","title":"ELN entries in NOMAD"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_0_getting_started_combined_concise.html#entries-in-nomad","text":"NOMAD is a free, open-source data management platform for materials science, aimed at making scientific research data FAIR. Similar to general-use databases, NOMAD manages a structured collection of data and is designed to capture relationships between them. In general database terminology, these pieces of data are referred to as data records. In NOMAD, they are called Entries . Entries in NOMAD can be visualized in the NOMAD graphical user interface (GUI) as well as accessed programmatically via its API. In general databases, every data record includes a set of attributes describing it. One attribute, always present, serves as a unique identifier for that data record, which is called the primary key of that data record. In NOMAD, each Entry (data record) has an entry_id as its primary key, which is automatically assigned by NOMAD. Entries related to the same topic, such as application, experimental method, computation method, are organized into groups with a defined structure . This structured organization in NOMAD is referred to as schema . A schema outlines relationships between attributes within each entry, which can be used to make a template with input fields for various entry parameters. This schema-based organization is essential for optimizing database searches needed for FAIR data. NOMAD offers a wide range of built-in schemas for general use, while also allowing users to create and upload custom schemas for their specific needs. This flexibility allow users to define new data structures for their particular types of research data, while all entries within the same data group follow the same schema, ensuring consistency, interoperability and reusability across records.","title":"Entries in NOMAD"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_0_getting_started_combined_concise.html#nomad-eln-entries","text":"Using the NOMAD ELN functionality, you can capture comprehensive information about substances, samples, instruments, and experimental workflows. Each ELN record in NOMAD is created based on a schema that defines the structure and necessary parameters. For instance, the Measurement ELN schema is designed for documenting characterization measurements. This schema allows you to record specific measurement details, such as instrument settings, calibration data, and the actual measured physical properties (like absorption or diffraction patterns).","title":"NOMAD ELN Entries"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_0_getting_started_combined_concise.html#learning-objectives","text":"In this module, we will learn how to create entries of structured data in NOMAD. Upon completion of the module, participants will be able to: Illustrate various methods for inserting information into NOMAD to create entries. Identify and utilize built-in schemas within NOMAD. Develop and implement custom schemas to meet specific data requirements. Proficiently create and employ a personalized Electronic LaboratoryNotebook (ELN) within the NOMAD platform.","title":"Learning Objectives"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_1_creating_entries_built-in_schema.html","text":"Creating NOMAD Entries Using the ELN Built-in ELN Schemas \u00b6 In this section, you will learn how to use NOMAD's built-in ELN schemas to create entries for entities and activities in your experiment. After planning for your experiments, you would start by documenting your experimental work. This involves creating a record of all the materials and instruments that will be used during your experiment, i.e., entities . In addition, you should outline the conditions under which the materials will be processed and the methods of measurement, i.e., activities . In NOMAD, this is done by creating entries for each entity and activity that will be used in your experiments. Creating entries using built-in ELN Schemas \u00b6 To create an entry in NOMAD using a built-in ELN schema, you may follow these general steps. For this example, we select the Basic ELN built-in schema in Step 8, which provides a free-text field. Alternatively, you can choose other built-in schemas from a list, depending on your needs. Step 1: Step 2: Step 3: Step 4: Step 5: Step 6: Create an entry Using NOMAD's Built-in Schema. \u00b6 Step 7: Step 8: Step 9: Step 10: What is Next? \u00b6 In the following sections, we will apply these general procedures to create entries for a sample ELN focused on the solution processing of organic materials. As we work through this example, we'll explore the various built-in schemas and the specific quantities they provide for \"input\".","title":"Getting started"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_1_creating_entries_built-in_schema.html#creating-nomad-entries-using-the-eln-built-in-eln-schemas","text":"In this section, you will learn how to use NOMAD's built-in ELN schemas to create entries for entities and activities in your experiment. After planning for your experiments, you would start by documenting your experimental work. This involves creating a record of all the materials and instruments that will be used during your experiment, i.e., entities . In addition, you should outline the conditions under which the materials will be processed and the methods of measurement, i.e., activities . In NOMAD, this is done by creating entries for each entity and activity that will be used in your experiments.","title":"Creating NOMAD Entries Using the ELN Built-in ELN Schemas"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_1_creating_entries_built-in_schema.html#creating-entries-using-built-in-eln-schemas","text":"To create an entry in NOMAD using a built-in ELN schema, you may follow these general steps. For this example, we select the Basic ELN built-in schema in Step 8, which provides a free-text field. Alternatively, you can choose other built-in schemas from a list, depending on your needs. Step 1: Step 2: Step 3: Step 4: Step 5: Step 6:","title":"Creating entries using built-in ELN Schemas"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_1_creating_entries_built-in_schema.html#create-an-entry-using-nomads-built-in-schema","text":"Step 7: Step 8: Step 9: Step 10:","title":"Create an entry Using NOMAD's Built-in Schema."},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_1_creating_entries_built-in_schema.html#what-is-next","text":"In the following sections, we will apply these general procedures to create entries for a sample ELN focused on the solution processing of organic materials. As we work through this example, we'll explore the various built-in schemas and the specific quantities they provide for \"input\".","title":"What is Next?"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_2_creating_entities.html","text":"Create Entries for the Entities of Your Experiment \u00b6 Recording entities in NOMAD is done by creating entries for each entity. Entities are objects that are used throughout your experiments, and can include the following: Substances or raw materials used to create samples. Samples to be examined. Instruments used to create and measure the samples. For example, consider an experiment where you need to prepare polymer solutions and then deposit them on substrates to create thin film samples. Then, you want to measure the optical absorption spectra and/or the electrical conductivity of your samples. To do this, you need your solute (polymer powder) and solvent to make the solution at a predefined concentration. Then you will deposit your thin film on a pre-cleaned substrate to create your samples. The polymer powder, solvent, and substrate are considered to be the substances used to create your samples, i.e., the solution and the thin film, respectively. For example, here are substances that will be used throughout the experiment: and here are the samples you planned for this experiment: In the process of preparing your samples , you will use several instruments , such as a balance, UV-Ozone cleaner, sonicator, and spin coater. Once the samples are ready, you will use another set of instruments to measure the optical absorption spectrum and electrical conductivity, i.e., a UV-Vis-NIR spectrometer and an electrical characterization setup, respectively. NOMAD provides built-in schemas for the three entities mentioned above, with input fields to define them. Creating entries using these built-in schemas allows you to use these entries for other ELN entries and workflows within NOMAD. Let's get hands-on and create entries based on built-in schemas for the different entities that will be used in this sample experiment. Substances Samples Instruments","title":"Entities of the Experiment"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_2_creating_entities.html#create-entries-for-the-entities-of-your-experiment","text":"Recording entities in NOMAD is done by creating entries for each entity. Entities are objects that are used throughout your experiments, and can include the following: Substances or raw materials used to create samples. Samples to be examined. Instruments used to create and measure the samples. For example, consider an experiment where you need to prepare polymer solutions and then deposit them on substrates to create thin film samples. Then, you want to measure the optical absorption spectra and/or the electrical conductivity of your samples. To do this, you need your solute (polymer powder) and solvent to make the solution at a predefined concentration. Then you will deposit your thin film on a pre-cleaned substrate to create your samples. The polymer powder, solvent, and substrate are considered to be the substances used to create your samples, i.e., the solution and the thin film, respectively. For example, here are substances that will be used throughout the experiment: and here are the samples you planned for this experiment: In the process of preparing your samples , you will use several instruments , such as a balance, UV-Ozone cleaner, sonicator, and spin coater. Once the samples are ready, you will use another set of instruments to measure the optical absorption spectrum and electrical conductivity, i.e., a UV-Vis-NIR spectrometer and an electrical characterization setup, respectively. NOMAD provides built-in schemas for the three entities mentioned above, with input fields to define them. Creating entries using these built-in schemas allows you to use these entries for other ELN entries and workflows within NOMAD. Let's get hands-on and create entries based on built-in schemas for the different entities that will be used in this sample experiment. Substances Samples Instruments","title":"Create Entries for the Entities of Your Experiment"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_3_substance_entities.html","text":"Create Entries for your Substances Using the Substance ELN Schema \u00b6 In this section you will learn how to create NOMAD entries for entities that will be used as substances in your experiments. You will use NOMAD's built-in schema, called the Substance ELN , and explore the various fields you can populate and the information you can add to NOMAD. Based on the example described earlier, we will need to create entries for the following substances: P3HT powder Chloroform Glass substrate Pre-patterned ITO substrates For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Substance ELN from the drop down menu, enter a name for your record, and click CREATE . After clicking the CREATE button, NOMAD will automatically perform the following tasks: NOMAD creates a file for the entry, using the format .archive.json . The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows user to fill in the information about the substance. The Substance ELN built-in schema provides several fields that allow input of different quantities: Substance name: This is the name of the input file created. Datetime: Allows entry for a date/time stamp. Substance ID: A human-readable ID that is unique for the substance within the lab. Can be manually entered. Detailed Substance Description: A free text field that can be used to enter any additional information about the entry. Remember, for this ELN you are using a built-in schema that was created to be as generic as possible to accommodate a wide range of users. You can use the different fields in ways that best suit your needs. For example, the Datetime field can be used as a timestamp for opening the polymer container, purchase date, or entry creation date. The Substance ELN allows you to include additional information for your entity by using subsections. These can be found at the bottom of the Entry/Data/data page and include: Elemental composition Pure substance Substance identifier elemental_composition Subsection: \u00b6 Here you can create quantities to define the chemical composition of your substance. You can add the chemical element, its atomic fraction, and its mass fraction. Add as many elements as needed to represent your substance. This section is useful for chemical substances. pure_substance Subsection: \u00b6 Here you can define your substance as a pure material purchased from an external vendor. This subsection allows data entry based on three different definition types. PubChem CAS General Regardless of the selected definition type, all pure substance subsections contain the following fields: Substance name IUPAC name Molecular formula Molecular unit Inchi Inchi Key Smile button Canonical Smile CAS Num","title":"Substances"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_3_substance_entities.html#create-entries-for-your-substances-using-the-substance-eln-schema","text":"In this section you will learn how to create NOMAD entries for entities that will be used as substances in your experiments. You will use NOMAD's built-in schema, called the Substance ELN , and explore the various fields you can populate and the information you can add to NOMAD. Based on the example described earlier, we will need to create entries for the following substances: P3HT powder Chloroform Glass substrate Pre-patterned ITO substrates For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Substance ELN from the drop down menu, enter a name for your record, and click CREATE . After clicking the CREATE button, NOMAD will automatically perform the following tasks: NOMAD creates a file for the entry, using the format .archive.json . The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows user to fill in the information about the substance. The Substance ELN built-in schema provides several fields that allow input of different quantities: Substance name: This is the name of the input file created. Datetime: Allows entry for a date/time stamp. Substance ID: A human-readable ID that is unique for the substance within the lab. Can be manually entered. Detailed Substance Description: A free text field that can be used to enter any additional information about the entry. Remember, for this ELN you are using a built-in schema that was created to be as generic as possible to accommodate a wide range of users. You can use the different fields in ways that best suit your needs. For example, the Datetime field can be used as a timestamp for opening the polymer container, purchase date, or entry creation date. The Substance ELN allows you to include additional information for your entity by using subsections. These can be found at the bottom of the Entry/Data/data page and include: Elemental composition Pure substance Substance identifier","title":"Create Entries for your Substances Using the Substance ELN Schema"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_3_substance_entities.html#elemental_composition-subsection","text":"Here you can create quantities to define the chemical composition of your substance. You can add the chemical element, its atomic fraction, and its mass fraction. Add as many elements as needed to represent your substance. This section is useful for chemical substances.","title":"elemental_composition Subsection:"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_3_substance_entities.html#pure_substance-subsection","text":"Here you can define your substance as a pure material purchased from an external vendor. This subsection allows data entry based on three different definition types. PubChem CAS General Regardless of the selected definition type, all pure substance subsections contain the following fields: Substance name IUPAC name Molecular formula Molecular unit Inchi Inchi Key Smile button Canonical Smile CAS Num","title":"pure_substance Subsection:"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_4_sample_entites.html","text":"Creating Entries for Your Samples Using the Generic sample ELN Schema \u00b6 In this section, you will learn how to create NOMAD entries for entities that will be the samples used or produced in your experiments. You will use the NOMAD built-in schema called Generic Sample ELN and explore the various fields you can populate and the information you can add to NOMAD. Based on the example described earlier, we need to create entries for the following entities: P3HT Thin Film P3HT Solution For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Generic Sample ELN from the drop-down menu, enter a name for your entry, and click CREATE . After clicking the CREATE button, NOMAD will automatically perform the following tasks: NOMAD creates a file for the entry, using the format .archive.json . The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows users to input information about the substance. The built-in generic ELN schema provides general fields that allow the entry of various quantities: Short name: This is the name of the record file created. Datetime: Allows input for a date/time stamp. ID: A human readable ID that is unique to the substance within the lab. can be entered manually. Description: A free text field that can be used to enter any additional information about the entry. Remember, for this ELN you are using a built-in schema that was created to be as generic as possible to accommodate a wide range of users. You can use the different fields in ways that best suit your needs. For example, the Description field can be used to add any batch information, lab conditions, storage location, etc. The Generic Sample ELN allows you to include additional information for your samples by using subsections. These can be found at the bottom of the Entry/data/Data page and include: Elemental Composition Components Sample identifier Elemental Composition Subsection: \u00b6 Here you can create quantities to define the chemical composition of your sample. You can add the chemical element, its atomic fraction, and its mass fraction. Add as many elements as needed to represent your samples. Components Subsection: \u00b6 Here you can provide more information about your sample based on the components it is made up of. The Components subsection provides three different ways to add this information: Component System Component Pure Substance Component The System Component section allows the user to select from the substances created in the ELN as components for the sample. This allows you to link your intended product to the raw materials and allows you to track and identify potential impurities or batch variations, if the results suggest it. Note that only items such as Samples or Substances can be selected in the System Component section. For the P3HT thin film sample, we can select the components as the solution used to prepare the sample and the glass substrate.","title":"Samples"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_4_sample_entites.html#creating-entries-for-your-samples-using-the-generic-sample-eln-schema","text":"In this section, you will learn how to create NOMAD entries for entities that will be the samples used or produced in your experiments. You will use the NOMAD built-in schema called Generic Sample ELN and explore the various fields you can populate and the information you can add to NOMAD. Based on the example described earlier, we need to create entries for the following entities: P3HT Thin Film P3HT Solution For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Generic Sample ELN from the drop-down menu, enter a name for your entry, and click CREATE . After clicking the CREATE button, NOMAD will automatically perform the following tasks: NOMAD creates a file for the entry, using the format .archive.json . The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows users to input information about the substance. The built-in generic ELN schema provides general fields that allow the entry of various quantities: Short name: This is the name of the record file created. Datetime: Allows input for a date/time stamp. ID: A human readable ID that is unique to the substance within the lab. can be entered manually. Description: A free text field that can be used to enter any additional information about the entry. Remember, for this ELN you are using a built-in schema that was created to be as generic as possible to accommodate a wide range of users. You can use the different fields in ways that best suit your needs. For example, the Description field can be used to add any batch information, lab conditions, storage location, etc. The Generic Sample ELN allows you to include additional information for your samples by using subsections. These can be found at the bottom of the Entry/data/Data page and include: Elemental Composition Components Sample identifier","title":"Creating Entries for Your Samples Using the Generic sample ELN Schema"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_4_sample_entites.html#elemental-composition-subsection","text":"Here you can create quantities to define the chemical composition of your sample. You can add the chemical element, its atomic fraction, and its mass fraction. Add as many elements as needed to represent your samples.","title":"Elemental Composition Subsection:"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_4_sample_entites.html#components-subsection","text":"Here you can provide more information about your sample based on the components it is made up of. The Components subsection provides three different ways to add this information: Component System Component Pure Substance Component The System Component section allows the user to select from the substances created in the ELN as components for the sample. This allows you to link your intended product to the raw materials and allows you to track and identify potential impurities or batch variations, if the results suggest it. Note that only items such as Samples or Substances can be selected in the System Component section. For the P3HT thin film sample, we can select the components as the solution used to prepare the sample and the glass substrate.","title":"Components Subsection:"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_5_instrument_entities.html","text":"Creating Entries for the Instrument Using the Instrument ELN Schema \u00b6 In this section, you will learn how to create NOMAD entries for entities that are instruments used to prepare and characterize your samples. You will use NOMAD's built-in schema, called Instrument ELN , and explore the various fields you can populate and the information you can add to NOMAD. Based on the example described earlier, we will need to create entries for the following instruments: UV Ozone cleaner Ultrasonic bath Scale Optical spectrometer Conductivity board Spin coater For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Instrument ELN from the drop-down menu, enter a name for your entry, and click CREATE . Let's start by creating an entry for the UV Ozone Purifier. After clicking the CREATE button, the following tasks are automatically performed within NOMAD: NOMAD will create a file for the entry using the .archive.json format. The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows users to input information about the instrument. In the Instrument ELN built-in schema, several fields are available to accept entry of different quantities: Short name: This is the name of the input file created. Datetime: Allows the entry of a date/time stamp. ID: A human readable ID that is unique to the instrument within the laboratory. Description: A free text field that can be used to enter any additional information about the entry. Remember that this is your ELN and you are using a built-in schema that was created to be as general as possible to accommodate a wide range of users. You are free to use the various fields as you see fit.","title":"Instruments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_5_instrument_entities.html#creating-entries-for-the-instrument-using-the-instrument-eln-schema","text":"In this section, you will learn how to create NOMAD entries for entities that are instruments used to prepare and characterize your samples. You will use NOMAD's built-in schema, called Instrument ELN , and explore the various fields you can populate and the information you can add to NOMAD. Based on the example described earlier, we will need to create entries for the following instruments: UV Ozone cleaner Ultrasonic bath Scale Optical spectrometer Conductivity board Spin coater For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Instrument ELN from the drop-down menu, enter a name for your entry, and click CREATE . Let's start by creating an entry for the UV Ozone Purifier. After clicking the CREATE button, the following tasks are automatically performed within NOMAD: NOMAD will create a file for the entry using the .archive.json format. The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows users to input information about the instrument. In the Instrument ELN built-in schema, several fields are available to accept entry of different quantities: Short name: This is the name of the input file created. Datetime: Allows the entry of a date/time stamp. ID: A human readable ID that is unique to the instrument within the laboratory. Description: A free text field that can be used to enter any additional information about the entry. Remember that this is your ELN and you are using a built-in schema that was created to be as general as possible to accommodate a wide range of users. You are free to use the various fields as you see fit.","title":"Creating Entries for the Instrument Using the Instrument ELN Schema"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_6_creating_activities.html","text":"Create Entries for the Activities of Your Experiments: \u00b6 Now that you have created the record for the entities of your experiments, the next step is to create the activities involved in your experiments. Activities include: Sample preparation Characterizing the samples We will first document the preparation procedure for our samples, i.e., the P3HT in chloroform solvent and the P3HT thin film on glass. For this, we will use the built-in ELN schema Material Processing ELN . This schema allows the processes to be linked to the equipment used, the substances used as input materials, and the samples resulting from the process. In addition, workflows can be automatically created and modified to generate a visual graph representing the process. Go to the next section for a step-by-step illustration of how to create an activity record using the Material Processing ELN schema, and an explanation of the different features that can be used.","title":"Activities of the Experiment"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_6_creating_activities.html#create-entries-for-the-activities-of-your-experiments","text":"Now that you have created the record for the entities of your experiments, the next step is to create the activities involved in your experiments. Activities include: Sample preparation Characterizing the samples We will first document the preparation procedure for our samples, i.e., the P3HT in chloroform solvent and the P3HT thin film on glass. For this, we will use the built-in ELN schema Material Processing ELN . This schema allows the processes to be linked to the equipment used, the substances used as input materials, and the samples resulting from the process. In addition, workflows can be automatically created and modified to generate a visual graph representing the process. Go to the next section for a step-by-step illustration of how to create an activity record using the Material Processing ELN schema, and an explanation of the different features that can be used.","title":"Create Entries for the Activities of Your Experiments:"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_7_materials_processing_activitiy.html","text":"Create Materials Preparation Activities Using the Material Processing ELN Schema \u00b6 In this section you will learn how to create NOMAD entries for activities that will be used to describe the sample preparation process in your experiments. You will use the NOMAD built-in schema called Material Processing ELN and explore the various fields you can fill in and the information you can add to NOMAD. Based on the example described earlier, we will need to create entries for the following processes: Preparation of P3HT solution in chloroform. Preparation of P3HT thin film on glass. Preparation of P3HT thin film on prepatterned ITO substrate. For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Material Processing ELN from the drop down menu, enter a name for your record, and click CREATE . After clicking the CREATE button, the following tasks are automatically performed in NOMAD: NOMAD will create a file for the entry using the .archive.json format. The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows user input to fill in the information about the substance. The built-in Material Processing ELN schema provides several fields that allow you to enter different quantities: Short name: This is the name of the input file created. Starting Time and Ending Time: Allows you to enter a date/time stamp for the start and end time of your process. ID: A human readable ID that is unique to the substance within the lab. Can be manually entered. Location: A text field that can be used to specify the storage location of your sample or the location where it was prepared. Description: A free text field that can be used to enter any additional information about the entry. Remember, for this ELN you are using a built-in schema that was created to be as generic as possible to accommodate a wide range of users. You can use the different fields in ways that best suit your needs. For example, the Description field can be used to provide information about the synthesis process, environmental conditions, or any relevant observations. The Material Processing ELN allows you to include additional information for your activities by using subsections. These can be found at the bottom of the Entry/DATA/data page and include steps instruments samples First, we add the relevant information about our process as quantities in the main fields, which will define the main quantities for our activity record. The next step is to record the steps performed during the solution preparation process. The steps subsection can be used to create multiple steps as needed. Let's create the following steps: powder_weighing: Weigh 6 mg of P3HT. solvent_filling: Fill 2 ml of chloroform into a vial. mix: add the polymer powder to the sovent vial. Continue by adding the connection of your process to the instrument used and the resulting sample entities. Note that the added information in the subsections will be used to automatically fill in the Workflow section as tasks, as well as the References section, which can be viewed in the Overview page. You can modify the workflow section by adding additional information such as inputs, additional tasks, and outputs. To do this, click on the Workflow2 section in the side pane. Let us add the substance entities we created for P3HT powder and chloroform solvent as inputs to our process. The changes will be reflected in the Workflow section and the Reference section of the Overview page.","title":"Processes"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_7_materials_processing_activitiy.html#create-materials-preparation-activities-using-the-material-processing-eln-schema","text":"In this section you will learn how to create NOMAD entries for activities that will be used to describe the sample preparation process in your experiments. You will use the NOMAD built-in schema called Material Processing ELN and explore the various fields you can fill in and the information you can add to NOMAD. Based on the example described earlier, we will need to create entries for the following processes: Preparation of P3HT solution in chloroform. Preparation of P3HT thin film on glass. Preparation of P3HT thin film on prepatterned ITO substrate. For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Material Processing ELN from the drop down menu, enter a name for your record, and click CREATE . After clicking the CREATE button, the following tasks are automatically performed in NOMAD: NOMAD will create a file for the entry using the .archive.json format. The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows user input to fill in the information about the substance. The built-in Material Processing ELN schema provides several fields that allow you to enter different quantities: Short name: This is the name of the input file created. Starting Time and Ending Time: Allows you to enter a date/time stamp for the start and end time of your process. ID: A human readable ID that is unique to the substance within the lab. Can be manually entered. Location: A text field that can be used to specify the storage location of your sample or the location where it was prepared. Description: A free text field that can be used to enter any additional information about the entry. Remember, for this ELN you are using a built-in schema that was created to be as generic as possible to accommodate a wide range of users. You can use the different fields in ways that best suit your needs. For example, the Description field can be used to provide information about the synthesis process, environmental conditions, or any relevant observations. The Material Processing ELN allows you to include additional information for your activities by using subsections. These can be found at the bottom of the Entry/DATA/data page and include steps instruments samples First, we add the relevant information about our process as quantities in the main fields, which will define the main quantities for our activity record. The next step is to record the steps performed during the solution preparation process. The steps subsection can be used to create multiple steps as needed. Let's create the following steps: powder_weighing: Weigh 6 mg of P3HT. solvent_filling: Fill 2 ml of chloroform into a vial. mix: add the polymer powder to the sovent vial. Continue by adding the connection of your process to the instrument used and the resulting sample entities. Note that the added information in the subsections will be used to automatically fill in the Workflow section as tasks, as well as the References section, which can be viewed in the Overview page. You can modify the workflow section by adding additional information such as inputs, additional tasks, and outputs. To do this, click on the Workflow2 section in the side pane. Let us add the substance entities we created for P3HT powder and chloroform solvent as inputs to our process. The changes will be reflected in the Workflow section and the Reference section of the Overview page.","title":"Create Materials Preparation Activities Using the Material Processing ELN Schema"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_8_measurement_activity.html","text":"Create Measurement Activities Using the Measurement ELN Schema \u00b6 In this section, you will learn how to create NOMAD entries for activities that describe the measurements in your experiments. You will use the NOMAD built-in schema called Measurement ELN and explore the various fields you can fill in, as well as the information you can add to NOMAD. Based on the example described earlier, we will need to create entries for the following processes: Measuring the optical absorption spectra. Measuring the electrical conductivity. For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Measurement ELN from the drop-down menu, enter a name for your entry, and click CREATE .","title":"Measurements"},{"location":"Module3/5_ELN_using_built-in_schema/M3_2_8_measurement_activity.html#create-measurement-activities-using-the-measurement-eln-schema","text":"In this section, you will learn how to create NOMAD entries for activities that describe the measurements in your experiments. You will use the NOMAD built-in schema called Measurement ELN and explore the various fields you can fill in, as well as the information you can add to NOMAD. Based on the example described earlier, we will need to create entries for the following processes: Measuring the optical absorption spectra. Measuring the electrical conductivity. For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Measurement ELN from the drop-down menu, enter a name for your entry, and click CREATE .","title":"Create Measurement Activities Using the Measurement ELN Schema"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_0_managing_experiments.html","text":"Managing Your Experiments \u00b6 Once you have defined all the substances, samples, processes, and measurements involved in your experimental work, you can start creating workflows that connect all these elements into a cohesive overview of the entire experiment. NOMAD's ELN offers a specially designed schema to help you do this: the Experiment ELN . Experiment ELN Schema: Integrating Your Experiment \u00b6 The Experiment ELN schema allows you to reference all the entries of processes and measurements as steps within a single NOMAD entry that describes the complete experiment. By using this built-in schema, you can easily visualize how all the different parts of your experiment connect and flow together. This creates a structured and traceable record of your work, ensuring that every detail is documented in context. Using the built-in Experiment ELN schema has some benefits: Integrated Experiment View See all related processes and measurements in one place. Simplified Workflow Management Create clear, step-by-step documentation of your experiment. Enhanced Traceability Easily trace the origins and relationships between various experimental components. NOMAD as a File-Based System \u00b6 NOMAD operates as a file-based system, meaning that each entry you create is stored as a file within your upload folder. This structure has several advantages: Organizing and Managing Files : Folder Structure: You can organize your entries into folders, similar to how you would on your computer. This allows for a clean and logical arrangement of all your experimental data. File Management: Besides entries, you can also upload other types of files, such as images, PDFs, or supplementary documents. These files can be stored and organized within the same structure, making NOMAD a powerful tool for managing all your experiment-related files. Shared Drive Capabilities NOMAD's file-based nature also means that it can function as an online shared drive. This enables collaboration with your team, as everyone with access can view, edit, and contribute to the same set of files. The shared drive feature is particularly useful for distributed teams or when working across different locations. Backup and Download For peace of mind, NOMAD allows you to download the entire upload folder to your local machine. This ensures that you have a backup of all your work, safeguarding against data loss. Learning Objectives \u00b6 Here is what you will learn in this section: Creating an experiment entry using the Experiment ELN schema. Step-by-step instructions on how to create a new entry that describes your entire experimental workflow. How to link processes, samples, and measurements to provide a comprehensive overview of your experiment. Viewing and managing files within uploads. How to navigate the file structure within NOMAD. Techniques for organizing your entries into folders for easier management. Tips on uploading additional files, managing folder structures, and downloading your data for backup.","title":"Overview"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_0_managing_experiments.html#managing-your-experiments","text":"Once you have defined all the substances, samples, processes, and measurements involved in your experimental work, you can start creating workflows that connect all these elements into a cohesive overview of the entire experiment. NOMAD's ELN offers a specially designed schema to help you do this: the Experiment ELN .","title":"Managing Your Experiments"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_0_managing_experiments.html#experiment-eln-schema-integrating-your-experiment","text":"The Experiment ELN schema allows you to reference all the entries of processes and measurements as steps within a single NOMAD entry that describes the complete experiment. By using this built-in schema, you can easily visualize how all the different parts of your experiment connect and flow together. This creates a structured and traceable record of your work, ensuring that every detail is documented in context. Using the built-in Experiment ELN schema has some benefits: Integrated Experiment View See all related processes and measurements in one place. Simplified Workflow Management Create clear, step-by-step documentation of your experiment. Enhanced Traceability Easily trace the origins and relationships between various experimental components.","title":"Experiment ELN Schema: Integrating Your Experiment"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_0_managing_experiments.html#nomad-as-a-file-based-system","text":"NOMAD operates as a file-based system, meaning that each entry you create is stored as a file within your upload folder. This structure has several advantages: Organizing and Managing Files : Folder Structure: You can organize your entries into folders, similar to how you would on your computer. This allows for a clean and logical arrangement of all your experimental data. File Management: Besides entries, you can also upload other types of files, such as images, PDFs, or supplementary documents. These files can be stored and organized within the same structure, making NOMAD a powerful tool for managing all your experiment-related files. Shared Drive Capabilities NOMAD's file-based nature also means that it can function as an online shared drive. This enables collaboration with your team, as everyone with access can view, edit, and contribute to the same set of files. The shared drive feature is particularly useful for distributed teams or when working across different locations. Backup and Download For peace of mind, NOMAD allows you to download the entire upload folder to your local machine. This ensures that you have a backup of all your work, safeguarding against data loss.","title":"NOMAD as a File-Based System"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_0_managing_experiments.html#learning-objectives","text":"Here is what you will learn in this section: Creating an experiment entry using the Experiment ELN schema. Step-by-step instructions on how to create a new entry that describes your entire experimental workflow. How to link processes, samples, and measurements to provide a comprehensive overview of your experiment. Viewing and managing files within uploads. How to navigate the file structure within NOMAD. Techniques for organizing your entries into folders for easier management. Tips on uploading additional files, managing folder structures, and downloading your data for backup.","title":"Learning Objectives"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_1_integrating_your_experiment.html","text":"Integrating Your Experiment Using the Experiment ELN Schema \u00b6 In this section, you will learn how to create NOMAD entries for the entire experimental workflow that links processes and measurements to provide a comprehensive overview of your experiment. You will use the NOMAD built-in schema called Experiment ELN and explore the various fields you can fill in and the information you can add to NOMAD. Based on the example described earlier, we will need to create an entry that includes (references) the following entries as steps: Preparing P3HT solutions. Depositing P3HT thin-films on glass substrate. Measuring the optical absorption spectroscopy. For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Experiment ELN from the drop-down menu, enter a name for your entry, and click CREATE . After clicking the CREATE button, NOMAD will automatically perform the following tasks: NOMAD creates a file for the entry, using the format .archive.json . The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows users to input information about the experiment. The built-in Experiment ELN schema provides general fields that allow the entry of various quantities: name: This is the name of the record file created. starting Time: Allows input for a date/time stamp. ID: A human readable ID that is unique to the substance within the lab. This can be entered manually. description: A free text field that can be used to enter any additional information about the entry. Remember that this is your ELN and you are using a built-in schema that was created to be as generic as possible to accommodate a wide range of users. You can use the different fields however you like. For example, the description field can be used to add any batch information, lab conditions, storage location, etc. The Experiment ELN allows you to include the steps of your experiments by using the steps subsection. The steps subsection can be found at the bottom of the Entry/DATA/data page. To begin adding steps to your experiment, click on the + icon next to the steps subsection. This action will open a new data entry panel where you can input the following details: step name: Provide a descriptive name for your step. activity: Reference the activity entry related to this step in your experiment. You can either select an existing entry or create a new one on the fly using the + icon. starting time: Specify when the step begins. activity ID: Provide the unique identifier for the activity. comment: A rich text editor to include any extra information. Let's proceed by adding the activity steps mentioned above and thier relevant information to our experiment. Activity 1: Solution preparation Activity 2: P3HT on glass Activity 3: Optical absorption of P3HT films After adding each activity, make sure you click on the save icon, then proceed to add the next activity by clicking on the + icon next to the steps subsection. Once you are done, you can see the steps are listed under the steps subsection. Important note: In the Experiment ELN , you can only add steps of activitiy schemas, that is, entries created with the Process ELN and Measurement ELN . Substances and samples can be already nested within those enties. These steps will be used to automatically fill in the Workflow section as tasks, as well as the References section, which can be viewed in the OVERVIEW page. You can now interact with the workflow graph to view the details of each of the steps in your experiment.","title":"Integrating Activities"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_1_integrating_your_experiment.html#integrating-your-experiment-using-the-experiment-eln-schema","text":"In this section, you will learn how to create NOMAD entries for the entire experimental workflow that links processes and measurements to provide a comprehensive overview of your experiment. You will use the NOMAD built-in schema called Experiment ELN and explore the various fields you can fill in and the information you can add to NOMAD. Based on the example described earlier, we will need to create an entry that includes (references) the following entries as steps: Preparing P3HT solutions. Depositing P3HT thin-films on glass substrate. Measuring the optical absorption spectroscopy. For general steps on how to create an entry in NOMAD using the built-in ELN schema, see the section Entries in NOMAD . When you reach the step 8, select Experiment ELN from the drop-down menu, enter a name for your entry, and click CREATE . After clicking the CREATE button, NOMAD will automatically perform the following tasks: NOMAD creates a file for the entry, using the format .archive.json . The entry file is stored in the main upload directory. NOMAD will open the entry, switch to the DATA tab, and open the data subsections page. The data subsections page allows users to input information about the experiment. The built-in Experiment ELN schema provides general fields that allow the entry of various quantities: name: This is the name of the record file created. starting Time: Allows input for a date/time stamp. ID: A human readable ID that is unique to the substance within the lab. This can be entered manually. description: A free text field that can be used to enter any additional information about the entry. Remember that this is your ELN and you are using a built-in schema that was created to be as generic as possible to accommodate a wide range of users. You can use the different fields however you like. For example, the description field can be used to add any batch information, lab conditions, storage location, etc. The Experiment ELN allows you to include the steps of your experiments by using the steps subsection. The steps subsection can be found at the bottom of the Entry/DATA/data page. To begin adding steps to your experiment, click on the + icon next to the steps subsection. This action will open a new data entry panel where you can input the following details: step name: Provide a descriptive name for your step. activity: Reference the activity entry related to this step in your experiment. You can either select an existing entry or create a new one on the fly using the + icon. starting time: Specify when the step begins. activity ID: Provide the unique identifier for the activity. comment: A rich text editor to include any extra information. Let's proceed by adding the activity steps mentioned above and thier relevant information to our experiment. Activity 1: Solution preparation Activity 2: P3HT on glass Activity 3: Optical absorption of P3HT films After adding each activity, make sure you click on the save icon, then proceed to add the next activity by clicking on the + icon next to the steps subsection. Once you are done, you can see the steps are listed under the steps subsection. Important note: In the Experiment ELN , you can only add steps of activitiy schemas, that is, entries created with the Process ELN and Measurement ELN . Substances and samples can be already nested within those enties. These steps will be used to automatically fill in the Workflow section as tasks, as well as the References section, which can be viewed in the OVERVIEW page. You can now interact with the workflow graph to view the details of each of the steps in your experiment.","title":"Integrating Your Experiment Using the Experiment ELN Schema"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_2_organizing_your_upload.html","text":"Organizing Your Upload \u00b6 NOMAD is a file-based system. This means that the data and information within NOMAD are primarily stored, managed, and accessed in the form of files. This approach contrasts with systems that might rely heavily on databases for data storage. In the context of NOMAD as an ELN, being a file-based system means that all the experimental data, schemas, and configurations are stored as files, which you can manage directly. This can make it easier to integrate with other tools, create local backups, and share data across different platforms. In this section, you will learn how to view and manage files within NOMAD. We\u2019ll cover navigating the file structure, organizing entries into folders, and uploading or downloading files for better data management and backup. Accessing Your Files in NOMAD \u00b6 To access your files in an upload, simply navigate to the FILES tab at the top of the upload page. This will open a page that functions similarly to the file explorer on your computer, displaying all files associated with your upload, whether they have been processed by NOMAD or not. In essence, this page acts as your online storage drive for the uploaded content in NOMAD. You can organize your files by creating folders, upload additional items like images or PDFs, and even download the entire directory to your local computer for offline access. Note that changing the folders structre will not affect the view or the function of the entries on OVERVIEW tab. Organizing Your Entries into Folders \u00b6 Let's organize the entries into folders of materials, samples, instruments, processes, results, and measurements. You can create new folders by clicking on the folder icon shown on the top. Then choose a name for your folder and click OK . In order to move your entries to a relevant folder, click on the folder to which you want to transfer entries to. A second panel will open to show the contents of the folder, which should be empty by now. You can drag-and-drop the entry files into the relevant folder. A prompt will appear asking if you want to copy or move the folder. Select move . Adding Files to Your Upload \u00b6 Being a file-based system, NOMAD allows you to upload documents and images to your upload. To add files to your upload, click on the upload cloud icon shown at the top. For example, you can add relevant publications and images to your upload and share them with your collaborators, in a similar manner to online shared drives. Backing Up Your Upload \u00b6 NOMAD allows you to download the entire upload folder to your local machine. To do so, click on the download cloud icon on the top. All the contents of your upload will be downloaded as a zip file to your computer.","title":"Organizing the Upload"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_2_organizing_your_upload.html#organizing-your-upload","text":"NOMAD is a file-based system. This means that the data and information within NOMAD are primarily stored, managed, and accessed in the form of files. This approach contrasts with systems that might rely heavily on databases for data storage. In the context of NOMAD as an ELN, being a file-based system means that all the experimental data, schemas, and configurations are stored as files, which you can manage directly. This can make it easier to integrate with other tools, create local backups, and share data across different platforms. In this section, you will learn how to view and manage files within NOMAD. We\u2019ll cover navigating the file structure, organizing entries into folders, and uploading or downloading files for better data management and backup.","title":"Organizing Your Upload"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_2_organizing_your_upload.html#accessing-your-files-in-nomad","text":"To access your files in an upload, simply navigate to the FILES tab at the top of the upload page. This will open a page that functions similarly to the file explorer on your computer, displaying all files associated with your upload, whether they have been processed by NOMAD or not. In essence, this page acts as your online storage drive for the uploaded content in NOMAD. You can organize your files by creating folders, upload additional items like images or PDFs, and even download the entire directory to your local computer for offline access. Note that changing the folders structre will not affect the view or the function of the entries on OVERVIEW tab.","title":"Accessing Your Files in NOMAD"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_2_organizing_your_upload.html#organizing-your-entries-into-folders","text":"Let's organize the entries into folders of materials, samples, instruments, processes, results, and measurements. You can create new folders by clicking on the folder icon shown on the top. Then choose a name for your folder and click OK . In order to move your entries to a relevant folder, click on the folder to which you want to transfer entries to. A second panel will open to show the contents of the folder, which should be empty by now. You can drag-and-drop the entry files into the relevant folder. A prompt will appear asking if you want to copy or move the folder. Select move .","title":"Organizing Your Entries into Folders"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_2_organizing_your_upload.html#adding-files-to-your-upload","text":"Being a file-based system, NOMAD allows you to upload documents and images to your upload. To add files to your upload, click on the upload cloud icon shown at the top. For example, you can add relevant publications and images to your upload and share them with your collaborators, in a similar manner to online shared drives.","title":"Adding Files to Your Upload"},{"location":"Module3/5_ELN_using_built-in_schema/M3_3_2_organizing_your_upload.html#backing-up-your-upload","text":"NOMAD allows you to download the entire upload folder to your local machine. To do so, click on the download cloud icon on the top. All the contents of your upload will be downloaded as a zip file to your computer.","title":"Backing Up Your Upload"},{"location":"Module3/5_ELN_using_built-in_schema/M3_4_0_sharing_your_eln.html","text":"Collaborating on Your ELN \u00b6 One key advantage of using an ELN over traditional paper lab books is the ability for multiple collaborators to simultaneously record and update experimental work on the same platform. It helps with keeping everyone up-to-date and promotes seamless communication and coordination. This approach keeps everyone up-to-date and promotes seamless communication and coordination. By sharing progress in real-time, team members and leaders can provide timely feedback and support. In this section, you will learn how to share your NOMAD ELN with your colleagues to maximize collaboration and efficiency. Sharing Your ELN \u00b6 NOMAD enables you to share your ELN by a dedicated access management function of your upload. Access Management in NOMAD Access management in NOMAD ELN operates on the upload level. This means that once you share your upload with someone, they can access all the entries within that upload. To share your ELN, click on the Manage upload members icon. Next, the user management prompt will appear, allowing you to add your colleagues. To grant access to your ELN, start typing the name of the user you wish to add, then select them from the drop-down list. Note that the user must have a NOMAD account to be listed. After selecting a user, you'll need to assign their role. Choose between Co-author or Reviewer , based on the level of access you want to provide: Co-author : Can create, edit, and delete entries within the upload. Reviewer : Has read-only access to the entries. Click SUBMIT ! Inviting Colleagues to Create a NOMAD Account to Access Your ELN. \u00b6 If you want to add a co-author or share your data with someone that is not already a NOMAD user, you can invite them by providing a few details. The invited person will then receive an Email to set a password and complete their profile, allowing you to add them as a co-author or share data immediately. Make Your ELN Publicly Visible \u00b6 In NOMAD's ELN, the Publicly visible option allows you to share your ELN with everyone without officially publishing it or making it immutable . In other words, this option enables public access for viewing purposes only , i.e., allowing your team or collaboraters to access your data without commiting to a permanent, uneditable publication status. This feature is especially useful during collaboration phases, where you want your team members to access and view your data freely, even without a NOMAD account, but you still want to retain control over further edits and changes.","title":"Sharing and Collaboration"},{"location":"Module3/5_ELN_using_built-in_schema/M3_4_0_sharing_your_eln.html#collaborating-on-your-eln","text":"One key advantage of using an ELN over traditional paper lab books is the ability for multiple collaborators to simultaneously record and update experimental work on the same platform. It helps with keeping everyone up-to-date and promotes seamless communication and coordination. This approach keeps everyone up-to-date and promotes seamless communication and coordination. By sharing progress in real-time, team members and leaders can provide timely feedback and support. In this section, you will learn how to share your NOMAD ELN with your colleagues to maximize collaboration and efficiency.","title":"Collaborating on Your ELN"},{"location":"Module3/5_ELN_using_built-in_schema/M3_4_0_sharing_your_eln.html#sharing-your-eln","text":"NOMAD enables you to share your ELN by a dedicated access management function of your upload. Access Management in NOMAD Access management in NOMAD ELN operates on the upload level. This means that once you share your upload with someone, they can access all the entries within that upload. To share your ELN, click on the Manage upload members icon. Next, the user management prompt will appear, allowing you to add your colleagues. To grant access to your ELN, start typing the name of the user you wish to add, then select them from the drop-down list. Note that the user must have a NOMAD account to be listed. After selecting a user, you'll need to assign their role. Choose between Co-author or Reviewer , based on the level of access you want to provide: Co-author : Can create, edit, and delete entries within the upload. Reviewer : Has read-only access to the entries. Click SUBMIT !","title":"Sharing Your ELN"},{"location":"Module3/5_ELN_using_built-in_schema/M3_4_0_sharing_your_eln.html#inviting-colleagues-to-create-a-nomad-account-to-access-your-eln","text":"If you want to add a co-author or share your data with someone that is not already a NOMAD user, you can invite them by providing a few details. The invited person will then receive an Email to set a password and complete their profile, allowing you to add them as a co-author or share data immediately.","title":"Inviting Colleagues to Create a NOMAD Account to Access Your ELN."},{"location":"Module3/5_ELN_using_built-in_schema/M3_4_0_sharing_your_eln.html#make-your-eln-publicly-visible","text":"In NOMAD's ELN, the Publicly visible option allows you to share your ELN with everyone without officially publishing it or making it immutable . In other words, this option enables public access for viewing purposes only , i.e., allowing your team or collaboraters to access your data without commiting to a permanent, uneditable publication status. This feature is especially useful during collaboration phases, where you want your team members to access and view your data freely, even without a NOMAD account, but you still want to retain control over further edits and changes.","title":"Make Your ELN Publicly Visible"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_0_overview.html","text":"Building Your Customized ELN Template \u00b6 In the previous section, you documented your experimen using NOMAD's built-in schemas. While these built-in NOMAD ELN schemas are versatile, they may sometimes be too general, limiting their applicability if specific input fields are required for your research. These input fields might not always cover all the quantities that you want to include in the structured data model within NOMAD. However, a key advantage of NOMAD is its flexibility: you can create your own customized ELN tailored to your specific needs. Using NOMAD's schema language, known as NOMAD metainfo, you can build a more precise and relevant data structure for your research. In this section, you will learn: How to create an ELN template using YAML files. The data structure of NOMAD. How to control the interactivity of different fields in your ELN.","title":"M3 4 0 overview"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_0_overview.html#building-your-customized-eln-template","text":"In the previous section, you documented your experimen using NOMAD's built-in schemas. While these built-in NOMAD ELN schemas are versatile, they may sometimes be too general, limiting their applicability if specific input fields are required for your research. These input fields might not always cover all the quantities that you want to include in the structured data model within NOMAD. However, a key advantage of NOMAD is its flexibility: you can create your own customized ELN tailored to your specific needs. Using NOMAD's schema language, known as NOMAD metainfo, you can build a more precise and relevant data structure for your research. In this section, you will learn: How to create an ELN template using YAML files. The data structure of NOMAD. How to control the interactivity of different fields in your ELN.","title":"Building Your Customized ELN Template"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_1_ground_guidelines.html","text":"Guidlines for Building a Custom ELN Schema \u00b6 To build and customize your NOMAD ELN, you'll need to write code using the NOMAD metainfo schema language. However, don't be intimidated by the term \"code\"\u2014this process requires only a basic understanding of programming and computer algorithms. In this section, we\u2019ll first outline the essential rules for writing a NOMAD ELN schema. Then, we'll guide you step-by-step as you put these principles into practice to create your first custom ELN in NOMAD. 1. Schemas can be written in YAML language NOMAD schemas can be created with your favorite text editor using the YAML language and saved in the NOMAD archive.yaml format. YAML, short for \"YAML Ain't Markup Language,\" is a lightweight and human-readable data serialization language. It is a way to represent data in a structured format that's both possible for humans to read and write, and for machines to parse and process. It uses indentation and whitespace to organize data. What is a serialization language? A serialization language is a way to convert complex data structures, like objects or lists, into another format that can be readily stored or transmitted. This format can then be used to recreate the original data structure later. Common examples include JSON, XML, and YAML, which turn data into a text format that can be shared between different systems or saved for later use. NOMAD's archive.yaml files start with the definitions keyword and can have a name and description . definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. 2. Sections must be defined with a name, quantities, and instructions The building blocks of a schema are sections. They are a representation of data and they are the objects that can be instantiated in NOMAD to build your ELN. Sections must be defined with a name, a set of quantities, and instructions on how they should be handled. What is meant by instantiation? \"Instantiation\" means creating a specific instance of something based on a general definition or template. For example, in programming or data management, if you have a general template (like a class or section), instantiating it means creating a real, usable object or entity from that template with specific values or attributes. Sections can be defined by using the keyword sections and the name of the section with a single indentation. definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : My_second_section : My_third_section : 3. Sections can inherit from other sections Sections can also borrow the structure of other sections. This is called inheritance. Inheritance allows a section to reuse definitions from a parent section. This is useful because NOMAD offers several built-in objects called base sections, which can help you write your schema efficiently. The built-in base sections can be inherited to a user defined section using the base_sections keyword, and then listing the addresses of the desired base sections with a single indentation. definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : base_sections : - nomad.datamodel.data.EntryData 4. Sections contain quantities and other sections (subsections) Each section contains a set of quantites that need to be captured by the ELN. The quantities represent the parameters of your measurement or processing conditions. In addition, sections can also include subsections, which can be instantiated within the NOMAD dashboard of the relevant section. definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : base_sections : - nomad.datamodel.data.EntryData quantities : sub_sections : 5. Quantities are defined with type, shape and unit properties Quantities define possible primitive values. The first line in the quantity definition includes a user-defined name for the quantitiy. The basic properties that go into a quantity definition are type , shape , and unit . definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : quantities : first_quantity : - type : #For example, str or np.float64 - shape : #For example scalar or list (['*']) - unit : #For example, meters, amperes, or seconds 6. Section and quantities can have annotations \u00b6 Annotations provide additional information that NOMAD can use to alter its behavior around these definitions and how users can interact with them. Annotations are named blocks of key-value pairs definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : m_annotations : annotation_name : key1 : value key2 : value quantities : first_quantity : type : value shape : value unit : value m_annotations : annotation_name : key1 : value","title":"Getting started"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_1_ground_guidelines.html#guidlines-for-building-a-custom-eln-schema","text":"To build and customize your NOMAD ELN, you'll need to write code using the NOMAD metainfo schema language. However, don't be intimidated by the term \"code\"\u2014this process requires only a basic understanding of programming and computer algorithms. In this section, we\u2019ll first outline the essential rules for writing a NOMAD ELN schema. Then, we'll guide you step-by-step as you put these principles into practice to create your first custom ELN in NOMAD. 1. Schemas can be written in YAML language NOMAD schemas can be created with your favorite text editor using the YAML language and saved in the NOMAD archive.yaml format. YAML, short for \"YAML Ain't Markup Language,\" is a lightweight and human-readable data serialization language. It is a way to represent data in a structured format that's both possible for humans to read and write, and for machines to parse and process. It uses indentation and whitespace to organize data. What is a serialization language? A serialization language is a way to convert complex data structures, like objects or lists, into another format that can be readily stored or transmitted. This format can then be used to recreate the original data structure later. Common examples include JSON, XML, and YAML, which turn data into a text format that can be shared between different systems or saved for later use. NOMAD's archive.yaml files start with the definitions keyword and can have a name and description . definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. 2. Sections must be defined with a name, quantities, and instructions The building blocks of a schema are sections. They are a representation of data and they are the objects that can be instantiated in NOMAD to build your ELN. Sections must be defined with a name, a set of quantities, and instructions on how they should be handled. What is meant by instantiation? \"Instantiation\" means creating a specific instance of something based on a general definition or template. For example, in programming or data management, if you have a general template (like a class or section), instantiating it means creating a real, usable object or entity from that template with specific values or attributes. Sections can be defined by using the keyword sections and the name of the section with a single indentation. definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : My_second_section : My_third_section : 3. Sections can inherit from other sections Sections can also borrow the structure of other sections. This is called inheritance. Inheritance allows a section to reuse definitions from a parent section. This is useful because NOMAD offers several built-in objects called base sections, which can help you write your schema efficiently. The built-in base sections can be inherited to a user defined section using the base_sections keyword, and then listing the addresses of the desired base sections with a single indentation. definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : base_sections : - nomad.datamodel.data.EntryData 4. Sections contain quantities and other sections (subsections) Each section contains a set of quantites that need to be captured by the ELN. The quantities represent the parameters of your measurement or processing conditions. In addition, sections can also include subsections, which can be instantiated within the NOMAD dashboard of the relevant section. definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : base_sections : - nomad.datamodel.data.EntryData quantities : sub_sections : 5. Quantities are defined with type, shape and unit properties Quantities define possible primitive values. The first line in the quantity definition includes a user-defined name for the quantitiy. The basic properties that go into a quantity definition are type , shape , and unit . definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : quantities : first_quantity : - type : #For example, str or np.float64 - shape : #For example scalar or list (['*']) - unit : #For example, meters, amperes, or seconds","title":"Guidlines for Building a Custom ELN Schema"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_1_ground_guidelines.html#6-section-and-quantities-can-have-annotations","text":"Annotations provide additional information that NOMAD can use to alter its behavior around these definitions and how users can interact with them. Annotations are named blocks of key-value pairs definitions : name : My NOMAD ELN description : This is an electronic lab notebook schema that includes several sections. sections : My_first_section : m_annotations : annotation_name : key1 : value key2 : value quantities : first_quantity : type : value shape : value unit : value m_annotations : annotation_name : key1 : value","title":"6. Section and quantities can have annotations"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_2_basic_eln.html","text":"Your First NOMAD Custom ELN \u00b6 In this section, we will take you through writing your first custom ELN schema, which we will use to create a basic ELN to capture an experiment of solution processing of polymer films. We will create an ELN for documenting polymer thin films samples prepared by spincoating. We would like our ELN to accept input for the following quantities: - Sample ID - Creation time - Substrate - Substrate cleaning procedure - Solute - Solvent - Solution concentration (mg/ml) - Spin coating speed (in rpm) - Spin coating acceleration (in rpm/s) - Film thickness (in nm) - Comments Let's Build Our Custom Schema \u00b6 The first step of writing a schema is to create a new file using any text editor and save it in the .archive.yaml format. Let's name our file My_spincoating_ELN.archive.yaml . The next step is to start filling in the schema file with the name, description, and sections. We will use the keyword definitions , and will only define one section in this schema. The sections should inherit the base section nomad.datamodel.data.EntryData . definitions : name : My spincoating ELN description : This is an electronic lab notebook for documenting polymer thin films samples prepared by spincoating sections : polymer_thin_films : base_sections : - nomad.datamodel.data.EntryData Now we define the quantities which will capture the information of our experiment. definitions : name : My spincoating ELN description : This is an electronic lab notebook for documenting polymer thin films samples prepared by spincoating sections : polymer_thin_films : base_sections : - nomad.datamodel.data.EntryData quantities : Sample ID : Creation time : Substrate : Substrate cleaning procedure : Solute : Solvent : Solution concentration : Spin coating speed : Spin coating acceleration : Film thickness : Comments : At this stage, we have created the skeleton for our custom schema. Let's add a type for each of the quantities in the schema using the keyword type , which will define the kind of values that can be used for each quantity. A comprehensive list of all the values that type accepts can be found here . We will also add units to the numerical quantities to define their physical meaning. In our example, we will use the following types for our data: text (str) numbers (np.float64) date and time (Datetime) list of choices (Enum) definitions : name : My spincoating ELN description : This is an electronic lab notebook for documenting polymer thin films samples prepared by spincoating sections : polymer_thin_films : base_sections : - nomad.datamodel.data.EntryData quantities : Sample ID : type : str Creation time : type : Datetime Substrate : type : type_kind : Enum type_data : - 'glass' - 'ITO' Substrate cleaning procedure : type : str Solute : type : type_kind : Enum type_data : - 'Regioregular P3HT' - 'Regiorandom P3HT' Solvent : type : type_kind : Enum type_data : - 'Chloroform' - 'Dichlorobenzene' Solution concentration : type : np.float64 unit : mg/ml Spin coating speed : type : np.float64 unit : rpm Spin coating acceleration : type : np.float64 unit : rpm / second Film thickness : type : np.float64 unit : nm Comments : type : str Let's save the schema and instantiate an entry to see how it will look like in the NOMAD GUI. The entry created contains all the quantities that we have created; however, it offers no means for user interactivity, i.e., input for the ELN. In order to allow user input in your ELN, we need to instruct NOMAD how to deal with the different quantities using annotations. Annotations in NOMAD are created using the m_annotations . Annotations are named blocks of key-value pairs. m_annotations : annotation_name : key1 : value key2 : value A complete list of annotations available in NOMAD can be found here . We will primarily use the ELN annotations in the schema to allow data entry to quantities we have created. definitions : name : My spincoating ELN description : This is an electronic lab notebook for documenting polymer thin films samples prepared by spincoating sections : polymer_thin_films : base_sections : - nomad.datamodel.data.EntryData quantities : Sample ID : type : str m_annotations : eln : component : StringEditQuantity Creation time : type : Datetime m_annotations : eln : component : DateTimeEditQuantity Substrate : type : type_kind : Enum type_data : - 'glass' - 'ITO' m_annotations : eln : component : RadioEnumEditQuantity Substrate cleaning procedure : type : str m_annotations : eln : component : RichTextEditQuantity Solute : type : type_kind : Enum type_data : - 'Regioregular P3HT' - 'Regiorandom P3HT' m_annotations : eln : component : RadioEnumEditQuantity Solvent : type : type_kind : Enum type_data : - 'Chloroform' - 'Dichlorobenzene' m_annotations : eln : component : RadioEnumEditQuantity Solution concentration : type : np.float64 unit : mg/ml m_annotations : eln : component : NumberEditQuantity defaultDisplayUnit : \"mg/ml\" Spin coating speed : type : np.float64 unit : rpm m_annotations : eln : component : NumberEditQuantity defaultDisplayUnit : \"rpm\" Spin coating acceleration : type : np.float64 unit : rpm / second m_annotations : eln : component : NumberEditQuantity defaultDisplayUnit : \"rpm/s\" Film thickness : type : np.float64 unit : nm m_annotations : eln : component : NumberEditQuantity defaultDisplayUnit : \"nm\" Comments : type : str m_annotations : eln : component : RichTextEditQuantity Let's save the schema and re-upload the file to NOMAD. Entries associated with this schema will be automatically updated. You can see now that the ELN allows for interactivity, and accepts input of user for the various quantities. This is now a functional ELN in which you can document the samples and the fabrication conditions. Let's create a few entries with your first ELN.","title":"Building a Custom ELN"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_2_basic_eln.html#your-first-nomad-custom-eln","text":"In this section, we will take you through writing your first custom ELN schema, which we will use to create a basic ELN to capture an experiment of solution processing of polymer films. We will create an ELN for documenting polymer thin films samples prepared by spincoating. We would like our ELN to accept input for the following quantities: - Sample ID - Creation time - Substrate - Substrate cleaning procedure - Solute - Solvent - Solution concentration (mg/ml) - Spin coating speed (in rpm) - Spin coating acceleration (in rpm/s) - Film thickness (in nm) - Comments","title":"Your First NOMAD Custom ELN"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_2_basic_eln.html#lets-build-our-custom-schema","text":"The first step of writing a schema is to create a new file using any text editor and save it in the .archive.yaml format. Let's name our file My_spincoating_ELN.archive.yaml . The next step is to start filling in the schema file with the name, description, and sections. We will use the keyword definitions , and will only define one section in this schema. The sections should inherit the base section nomad.datamodel.data.EntryData . definitions : name : My spincoating ELN description : This is an electronic lab notebook for documenting polymer thin films samples prepared by spincoating sections : polymer_thin_films : base_sections : - nomad.datamodel.data.EntryData Now we define the quantities which will capture the information of our experiment. definitions : name : My spincoating ELN description : This is an electronic lab notebook for documenting polymer thin films samples prepared by spincoating sections : polymer_thin_films : base_sections : - nomad.datamodel.data.EntryData quantities : Sample ID : Creation time : Substrate : Substrate cleaning procedure : Solute : Solvent : Solution concentration : Spin coating speed : Spin coating acceleration : Film thickness : Comments : At this stage, we have created the skeleton for our custom schema. Let's add a type for each of the quantities in the schema using the keyword type , which will define the kind of values that can be used for each quantity. A comprehensive list of all the values that type accepts can be found here . We will also add units to the numerical quantities to define their physical meaning. In our example, we will use the following types for our data: text (str) numbers (np.float64) date and time (Datetime) list of choices (Enum) definitions : name : My spincoating ELN description : This is an electronic lab notebook for documenting polymer thin films samples prepared by spincoating sections : polymer_thin_films : base_sections : - nomad.datamodel.data.EntryData quantities : Sample ID : type : str Creation time : type : Datetime Substrate : type : type_kind : Enum type_data : - 'glass' - 'ITO' Substrate cleaning procedure : type : str Solute : type : type_kind : Enum type_data : - 'Regioregular P3HT' - 'Regiorandom P3HT' Solvent : type : type_kind : Enum type_data : - 'Chloroform' - 'Dichlorobenzene' Solution concentration : type : np.float64 unit : mg/ml Spin coating speed : type : np.float64 unit : rpm Spin coating acceleration : type : np.float64 unit : rpm / second Film thickness : type : np.float64 unit : nm Comments : type : str Let's save the schema and instantiate an entry to see how it will look like in the NOMAD GUI. The entry created contains all the quantities that we have created; however, it offers no means for user interactivity, i.e., input for the ELN. In order to allow user input in your ELN, we need to instruct NOMAD how to deal with the different quantities using annotations. Annotations in NOMAD are created using the m_annotations . Annotations are named blocks of key-value pairs. m_annotations : annotation_name : key1 : value key2 : value A complete list of annotations available in NOMAD can be found here . We will primarily use the ELN annotations in the schema to allow data entry to quantities we have created. definitions : name : My spincoating ELN description : This is an electronic lab notebook for documenting polymer thin films samples prepared by spincoating sections : polymer_thin_films : base_sections : - nomad.datamodel.data.EntryData quantities : Sample ID : type : str m_annotations : eln : component : StringEditQuantity Creation time : type : Datetime m_annotations : eln : component : DateTimeEditQuantity Substrate : type : type_kind : Enum type_data : - 'glass' - 'ITO' m_annotations : eln : component : RadioEnumEditQuantity Substrate cleaning procedure : type : str m_annotations : eln : component : RichTextEditQuantity Solute : type : type_kind : Enum type_data : - 'Regioregular P3HT' - 'Regiorandom P3HT' m_annotations : eln : component : RadioEnumEditQuantity Solvent : type : type_kind : Enum type_data : - 'Chloroform' - 'Dichlorobenzene' m_annotations : eln : component : RadioEnumEditQuantity Solution concentration : type : np.float64 unit : mg/ml m_annotations : eln : component : NumberEditQuantity defaultDisplayUnit : \"mg/ml\" Spin coating speed : type : np.float64 unit : rpm m_annotations : eln : component : NumberEditQuantity defaultDisplayUnit : \"rpm\" Spin coating acceleration : type : np.float64 unit : rpm / second m_annotations : eln : component : NumberEditQuantity defaultDisplayUnit : \"rpm/s\" Film thickness : type : np.float64 unit : nm m_annotations : eln : component : NumberEditQuantity defaultDisplayUnit : \"nm\" Comments : type : str m_annotations : eln : component : RichTextEditQuantity Let's save the schema and re-upload the file to NOMAD. Entries associated with this schema will be automatically updated. You can see now that the ELN allows for interactivity, and accepts input of user for the various quantities. This is now a functional ELN in which you can document the samples and the fabrication conditions. Let's create a few entries with your first ELN.","title":"Let's Build Our Custom Schema"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html","text":"Adding Characterization Data Files to Your NOMAD ELN \u00b6 In this section, we will explore the process of documenting experiments within the NOMAD ELN, focusing on the essential step of uploading characterization results along with sample and processing information. Within NOMAD, any file type can be added seamlessly through the ELN upload feature. However, to take full advantage of NOMAD's capabilities, it is critical that data files become Entries. These Entries are processed by NOMAD, transforming them into structured data that allows for metadata extraction, visualization, and analysis. Objective \u00b6 Our objective is to upload these files into our ELN, make NOMAD parse the data within these files, and then visualize the data in plots that can be viewed in NOMAD. File Handling in NOMAD \u00b6 In NOMAD, uploaded files are categorized into two main types: raw files and processed data files . This distinction is critical to understanding how NOMAD handles and interprets your data. To facilitate this process, NOMAD is continually expanding its support for various data formats. This support ensures that NOMAD users have access to parsers designed to read specific data formats seamlessly. What is a parser in NOMAD? Think of it as a specialized tool in the NOMAD toolkit. Parsers are small programs designed to take a file as input and produce processed data. They play a crucial role in transforming information from various source formats into NOMAD's structured, schema-based format. Each parser is tailored to a specific file format and is designed to handle data files of that format. In addition, the parsers are able to navigate through multiple files, such as those referenced in the main file, to ensure comprehensive data processing. Supported Data Categories \u00b6 NOMAD supports two main categories of data: Theory and Computations data and Measurement (spectroscopy) data. Theory and Computations Data: For a comprehensive list of codes and related file formats supported in NOMAD, please visit this page . Measurement (Spectroscopy) Data: For generic tabular data in '.csv' and '.xlsx' formats, NOMAD offers a flexible tabular parser that adapts to the structure of the data within the file. For Optical Absorption Spectroscopy, NOMAD supports '.asc' files from PerkinElmer Lambda instruments. For X-ray Diffraction, NOMAD supports the following file formats: '.xrdml' files from Panalytical instruments. '.rasx' files from Rigaku instruments. '.brml' files from Bruker instruments. Using NOMAD's Tabular Parser \u00b6 In the following sections, we'll explore how to utilize NOMAD's tabular parser effectively to enhance your data documentation and visualization. It is very common to export measurement data into a tabular format such as .csv or .xlsx. NOMAD offers a versatile tabular parser that can be configured to process tabular data with different representations, such as data arranged in columns or rows. Columns: each column contains an array of cells that we want to parse into one quantity. Example: current and voltage arrays to be plotted as x and y. Rows: each row contains a set of cells that we want to parse into a section, i.e., a set of quantities. Example: an inventory tabular data file (for substrates, precursors, or more) where each column represents a property and each row corresponds to one unit stored in the inventory. More details on the different representations of tabular data can be found here . Steps to Upload and Visualize Data \u00b6 In this workshop, we will work with measurement data from optical absorption spectroscopy and conductivity measurement stored in a the tabular format '.csv', with the columns representations shown in the figure below. Our objective is to upload these files into our ELN, make NOMAD parse the data within these files, and then visualize the data in plots that can be viewed in NOMAD. To achieve this, we will write a schema using the YAML language, and then illustrate how it can be used as ELN in NOMAD. Step 1: Defining and Saving the Schema File \u00b6 Let's start by creating a new schema file with the .archive.yaml format, and create a section called Optical_absorption. definitions : name : This is a parser for optical absorption data in the .csv format sections : Optical_absorption : Step 2: Adding the Needed Base Sections \u00b6 The next step is to inherit the base sections to meet our ELN needs. To create entries from this schema we will use nomad.datamodel.data.EntryData To use the tabular parser we will use nomad.parsing.tabular.TableData To enable the plot function we will use nomad.datamodel.metainfo.plot.PlotSection Step 3: Defining the Quantities of Our Schema \u00b6 We will define the quantities in our ELN schema. Three quantities are needed: A quantity to allow the upload of the data file, and apply the tabular parser to transform the data into NOMAD structure. Let's call this data_file . A quantity to store the values of our x-axis coming from the tabular parser. Let's call this wavelength . A quantity to store the values of our y-axis coming from the tabular parser. Let's call this absorption . At this stage our schema file will look like this: definitions : name : This is a parser for optical absorption data in the .csv format sections : Optical_absorption : base_sections : - nomad.datamodel.data.EntryData - nomad.parsing.tabular.TableData - nomad.datamodel.metainfo.plot.PlotSection quantities : data_file : type : str descritpion : Upload your .csv data file wavelength : type : np.float64 shape : [ '*' ] absorbance : type : np.float64 shape : [ '*' ] Step 4: Instructing NOMAD on How to Treat Different Quantities \u00b6 For this, we use the m_annotations function within each of the quantities to perform the intended task. The data_file quantity: We will need three annotations: - The first is to instruct NOMAD to allow for droping and selecting files in this quantity. Here we will use the following: eln : component : FileEditQuantity - The second is to instruct NOMAD to open the operating system's data browser to select files: browser : adaptor : RawFileAdaptor - The third one instructs NOMAD to apply the tabular parser to extract the data from the uploaded file: tabular_parser : parsing_options : comment : '#' skiprows : [ 1 ] mapping_options : - mapping_mode : column file_mode : current_entry sections : - '#root' Those interested in learning more about parsing_options and mapping_options can find additional details in the official NOMAD documentation . The wavelength quantity: This quantitiy will accept the values from the first column of our tabular data file, that will be extracted by the tabular parser. For this purpose we will use the following annotation: tabular : name : Wavelength Note that the value for the name key should be exactly written as the header of the first column in the data file . The absorbance quantity: This quantitiy will accept the values from the second column of our tabular data file, that will be extracted by the tabular parser. For this purpose we will use the following annotation: tabular : name : Absorbance Note that the value for the name key should be exactly written as the header of the second column of the data file . Our schema file at this stage will look as follows: definitions : name : This is a parser for optical absorption data in the .csv format sections : Optical_absorption : base_sections : - nomad.datamodel.data.EntryData - nomad.parsing.tabular.TableData - nomad.datamodel.metainfo.plot.PlotSection quantities : data_file : type : str descritpion : Upload your .csv data file m_annotations : eln : component : FileEditQuantity browser : adaptor : RawFileAdaptor tabular_parser : parsing_options : comment : '#' skiprows : [ 1 ] mapping_options : - mapping_mode : column file_mode : current_entry sections : - '#root' wavelength : type : np.float64 shape : [ '*' ] m_annotations : tabular : name : Wavelength absorbance : type : np.float64 shape : [ '*' ] m_annotations : tabular : name : Absorbance Step 5: Creating a Plot for Your Data \u00b6 To visualize the data from the uploaded and parsed file within your ELN, we will use an annotation for the main section of our schema Optical_absorption By using the plotly_graph_object annotation we instruct NOMAD which quantities to use for the x-axis and the y-axis, as well as provide the title of the plot. Within the plotly_graph_object annotation, the data key defines the quantites for each axis. Here, these varaiable names match those which are defined in the schema. Finally, plot's title is set using the layout key. plotly_graph_object : - data : x : \"#wavelength\" y : \"#absorbance\" layout : title : Optical Spectrum At this stage the schema file will be written as follows: definitions : name : This is a parser for optical absorption data in the .csv format sections : Optical_absorption : base_sections : - nomad.datamodel.data.EntryData - nomad.parsing.tabular.TableData - nomad.datamodel.metainfo.plot.PlotSection quantities : data_file : type : str descritpion : Upload your .csv data file m_annotations : eln : component : FileEditQuantity browser : adaptor : RawFileAdaptor tabular_parser : parsing_options : comment : '#' skiprows : [ 1 ] mapping_options : - mapping_mode : column file_mode : current_entry sections : - '#root' wavelength : type : np.float64 shape : [ '*' ] m_annotations : tabular : name : Wavelength absorbance : type : np.float64 shape : [ '*' ] m_annotations : tabular : name : Absorbance m_annotations : plotly_graph_object : data : x : \"#wavelength\" y : \"#absorbance\" layout : title : Optical Spectrum Step 6: Uploading the Schema File to NOMAD and Creating an Entry \u00b6 Now that we have created the ELN schema file for parsing the optical absorption data file, let's put it into the test in the NOMAD GUI. Follow this sequence to complete the process: Navigate to NOMAD and start NOMAD by clicking on the OPEN NOMAD button. Click \"Uploads\" under the \"PUBLISH\" menu. Click \"CREATE A NEW UPLOAD\". Give a name to your upload by clicking on the pen button. Type the desired name then click SAVE . Then click DROP FILES HERE OR CLICK TO OPEN DIALOG and select the schema file (the \".archive.yaml\" file you just prepared) or simply drag and dropthe file here. The file will be uploaded and processed by NOMAD, the process status will appear as SUCSESS. Then, to create an entry and upload the data file, click CREATE FROM SCHEMA . Select the \"Custom schema\" radiobox, and then click the mangifying glass button to browse the available custom schema. Click \"Only this upload\" radiobox. You see now the custom schema you have just uploaded. Click on it and then select its name, \"Optical_absorption\" to be instantiated. Add a name to your entry, e.g., my_first_data, and click CREATE . The entry is now created based on our schema. In the \"DATA\" tab click on the cloud button to open the system browser and select your data file. Click save to start the tabular parser. The columns from the data file are now transformed to array values in the quantities that were defined in the schema. You can click on the quantities, i.e., wavelength and absorbance, to view the data The plot will be also visible if you scroll down.","title":"Using the Tabular Parser"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#adding-characterization-data-files-to-your-nomad-eln","text":"In this section, we will explore the process of documenting experiments within the NOMAD ELN, focusing on the essential step of uploading characterization results along with sample and processing information. Within NOMAD, any file type can be added seamlessly through the ELN upload feature. However, to take full advantage of NOMAD's capabilities, it is critical that data files become Entries. These Entries are processed by NOMAD, transforming them into structured data that allows for metadata extraction, visualization, and analysis.","title":"Adding Characterization Data Files to Your NOMAD ELN"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#objective","text":"Our objective is to upload these files into our ELN, make NOMAD parse the data within these files, and then visualize the data in plots that can be viewed in NOMAD.","title":"Objective"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#file-handling-in-nomad","text":"In NOMAD, uploaded files are categorized into two main types: raw files and processed data files . This distinction is critical to understanding how NOMAD handles and interprets your data. To facilitate this process, NOMAD is continually expanding its support for various data formats. This support ensures that NOMAD users have access to parsers designed to read specific data formats seamlessly. What is a parser in NOMAD? Think of it as a specialized tool in the NOMAD toolkit. Parsers are small programs designed to take a file as input and produce processed data. They play a crucial role in transforming information from various source formats into NOMAD's structured, schema-based format. Each parser is tailored to a specific file format and is designed to handle data files of that format. In addition, the parsers are able to navigate through multiple files, such as those referenced in the main file, to ensure comprehensive data processing.","title":"File Handling in NOMAD"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#supported-data-categories","text":"NOMAD supports two main categories of data: Theory and Computations data and Measurement (spectroscopy) data. Theory and Computations Data: For a comprehensive list of codes and related file formats supported in NOMAD, please visit this page . Measurement (Spectroscopy) Data: For generic tabular data in '.csv' and '.xlsx' formats, NOMAD offers a flexible tabular parser that adapts to the structure of the data within the file. For Optical Absorption Spectroscopy, NOMAD supports '.asc' files from PerkinElmer Lambda instruments. For X-ray Diffraction, NOMAD supports the following file formats: '.xrdml' files from Panalytical instruments. '.rasx' files from Rigaku instruments. '.brml' files from Bruker instruments.","title":"Supported Data Categories"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#using-nomads-tabular-parser","text":"In the following sections, we'll explore how to utilize NOMAD's tabular parser effectively to enhance your data documentation and visualization. It is very common to export measurement data into a tabular format such as .csv or .xlsx. NOMAD offers a versatile tabular parser that can be configured to process tabular data with different representations, such as data arranged in columns or rows. Columns: each column contains an array of cells that we want to parse into one quantity. Example: current and voltage arrays to be plotted as x and y. Rows: each row contains a set of cells that we want to parse into a section, i.e., a set of quantities. Example: an inventory tabular data file (for substrates, precursors, or more) where each column represents a property and each row corresponds to one unit stored in the inventory. More details on the different representations of tabular data can be found here .","title":"Using NOMAD's Tabular Parser"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#steps-to-upload-and-visualize-data","text":"In this workshop, we will work with measurement data from optical absorption spectroscopy and conductivity measurement stored in a the tabular format '.csv', with the columns representations shown in the figure below. Our objective is to upload these files into our ELN, make NOMAD parse the data within these files, and then visualize the data in plots that can be viewed in NOMAD. To achieve this, we will write a schema using the YAML language, and then illustrate how it can be used as ELN in NOMAD.","title":"Steps to Upload and Visualize Data"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#step-1-defining-and-saving-the-schema-file","text":"Let's start by creating a new schema file with the .archive.yaml format, and create a section called Optical_absorption. definitions : name : This is a parser for optical absorption data in the .csv format sections : Optical_absorption :","title":"Step 1: Defining and Saving the Schema File"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#step-2-adding-the-needed-base-sections","text":"The next step is to inherit the base sections to meet our ELN needs. To create entries from this schema we will use nomad.datamodel.data.EntryData To use the tabular parser we will use nomad.parsing.tabular.TableData To enable the plot function we will use nomad.datamodel.metainfo.plot.PlotSection","title":"Step 2: Adding the Needed Base Sections"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#step-3-defining-the-quantities-of-our-schema","text":"We will define the quantities in our ELN schema. Three quantities are needed: A quantity to allow the upload of the data file, and apply the tabular parser to transform the data into NOMAD structure. Let's call this data_file . A quantity to store the values of our x-axis coming from the tabular parser. Let's call this wavelength . A quantity to store the values of our y-axis coming from the tabular parser. Let's call this absorption . At this stage our schema file will look like this: definitions : name : This is a parser for optical absorption data in the .csv format sections : Optical_absorption : base_sections : - nomad.datamodel.data.EntryData - nomad.parsing.tabular.TableData - nomad.datamodel.metainfo.plot.PlotSection quantities : data_file : type : str descritpion : Upload your .csv data file wavelength : type : np.float64 shape : [ '*' ] absorbance : type : np.float64 shape : [ '*' ]","title":"Step 3: Defining the Quantities of Our Schema"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#step-4-instructing-nomad-on-how-to-treat-different-quantities","text":"For this, we use the m_annotations function within each of the quantities to perform the intended task. The data_file quantity: We will need three annotations: - The first is to instruct NOMAD to allow for droping and selecting files in this quantity. Here we will use the following: eln : component : FileEditQuantity - The second is to instruct NOMAD to open the operating system's data browser to select files: browser : adaptor : RawFileAdaptor - The third one instructs NOMAD to apply the tabular parser to extract the data from the uploaded file: tabular_parser : parsing_options : comment : '#' skiprows : [ 1 ] mapping_options : - mapping_mode : column file_mode : current_entry sections : - '#root' Those interested in learning more about parsing_options and mapping_options can find additional details in the official NOMAD documentation . The wavelength quantity: This quantitiy will accept the values from the first column of our tabular data file, that will be extracted by the tabular parser. For this purpose we will use the following annotation: tabular : name : Wavelength Note that the value for the name key should be exactly written as the header of the first column in the data file . The absorbance quantity: This quantitiy will accept the values from the second column of our tabular data file, that will be extracted by the tabular parser. For this purpose we will use the following annotation: tabular : name : Absorbance Note that the value for the name key should be exactly written as the header of the second column of the data file . Our schema file at this stage will look as follows: definitions : name : This is a parser for optical absorption data in the .csv format sections : Optical_absorption : base_sections : - nomad.datamodel.data.EntryData - nomad.parsing.tabular.TableData - nomad.datamodel.metainfo.plot.PlotSection quantities : data_file : type : str descritpion : Upload your .csv data file m_annotations : eln : component : FileEditQuantity browser : adaptor : RawFileAdaptor tabular_parser : parsing_options : comment : '#' skiprows : [ 1 ] mapping_options : - mapping_mode : column file_mode : current_entry sections : - '#root' wavelength : type : np.float64 shape : [ '*' ] m_annotations : tabular : name : Wavelength absorbance : type : np.float64 shape : [ '*' ] m_annotations : tabular : name : Absorbance","title":"Step 4: Instructing NOMAD on How to Treat Different Quantities"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#step-5-creating-a-plot-for-your-data","text":"To visualize the data from the uploaded and parsed file within your ELN, we will use an annotation for the main section of our schema Optical_absorption By using the plotly_graph_object annotation we instruct NOMAD which quantities to use for the x-axis and the y-axis, as well as provide the title of the plot. Within the plotly_graph_object annotation, the data key defines the quantites for each axis. Here, these varaiable names match those which are defined in the schema. Finally, plot's title is set using the layout key. plotly_graph_object : - data : x : \"#wavelength\" y : \"#absorbance\" layout : title : Optical Spectrum At this stage the schema file will be written as follows: definitions : name : This is a parser for optical absorption data in the .csv format sections : Optical_absorption : base_sections : - nomad.datamodel.data.EntryData - nomad.parsing.tabular.TableData - nomad.datamodel.metainfo.plot.PlotSection quantities : data_file : type : str descritpion : Upload your .csv data file m_annotations : eln : component : FileEditQuantity browser : adaptor : RawFileAdaptor tabular_parser : parsing_options : comment : '#' skiprows : [ 1 ] mapping_options : - mapping_mode : column file_mode : current_entry sections : - '#root' wavelength : type : np.float64 shape : [ '*' ] m_annotations : tabular : name : Wavelength absorbance : type : np.float64 shape : [ '*' ] m_annotations : tabular : name : Absorbance m_annotations : plotly_graph_object : data : x : \"#wavelength\" y : \"#absorbance\" layout : title : Optical Spectrum","title":"Step 5: Creating a Plot for Your Data"},{"location":"Module3/6_ELN_using_custom_schema/M3_4_3_measurement_data.html#step-6-uploading-the-schema-file-to-nomad-and-creating-an-entry","text":"Now that we have created the ELN schema file for parsing the optical absorption data file, let's put it into the test in the NOMAD GUI. Follow this sequence to complete the process: Navigate to NOMAD and start NOMAD by clicking on the OPEN NOMAD button. Click \"Uploads\" under the \"PUBLISH\" menu. Click \"CREATE A NEW UPLOAD\". Give a name to your upload by clicking on the pen button. Type the desired name then click SAVE . Then click DROP FILES HERE OR CLICK TO OPEN DIALOG and select the schema file (the \".archive.yaml\" file you just prepared) or simply drag and dropthe file here. The file will be uploaded and processed by NOMAD, the process status will appear as SUCSESS. Then, to create an entry and upload the data file, click CREATE FROM SCHEMA . Select the \"Custom schema\" radiobox, and then click the mangifying glass button to browse the available custom schema. Click \"Only this upload\" radiobox. You see now the custom schema you have just uploaded. Click on it and then select its name, \"Optical_absorption\" to be instantiated. Add a name to your entry, e.g., my_first_data, and click CREATE . The entry is now created based on our schema. In the \"DATA\" tab click on the cloud button to open the system browser and select your data file. Click save to start the tabular parser. The columns from the data file are now transformed to array values in the quantities that were defined in the schema. You can click on the quantities, i.e., wavelength and absorbance, to view the data The plot will be also visible if you scroll down.","title":"Step 6: Uploading the Schema File to NOMAD and Creating an Entry"},{"location":"Module4/M4_0_overview.html","text":"Overview \u00b6 In the first three modules, we covered the NOMAD features through its Graphical User Interface (GUI). However, NOMAD has much more to offer, especially when it comes to aligning with the FAIR principles for Research Data Management (RDM). One key feature is the ability to access data programmatically through its Application Programming Interface (API), which strongly supports the accessibility and reusability of the data. Understanding and utilizing the NOMAD API opens up new possibilities for researchers. It serves as a gateway to a more dynamic, efficient, and customizable research and analysis experience. With the NOMAD API you will be able to: Filter Data Efficiently : Build customized datasets for specific needs, such as machine learning applications. Automate Tasks : Streamline data collection, retrieval, and analysis through automation. In this module, you will learn how to: Set up your programming environment : Prepare both local and cloud-based environments for using the NOMAD API. Understand API basics : Gain an understanding of essential API concepts and HTTP methods. Retrieve data from NOMAD using API : Explore practical examples of making API calls, using pagination for large datasets, and leveraging API helpers available in the NOMAD GUI. Upload data using API : Discover how to authenticate with the API, create datasets and upload your data programmatically.","title":"Overview"},{"location":"Module4/M4_0_overview.html#overview","text":"In the first three modules, we covered the NOMAD features through its Graphical User Interface (GUI). However, NOMAD has much more to offer, especially when it comes to aligning with the FAIR principles for Research Data Management (RDM). One key feature is the ability to access data programmatically through its Application Programming Interface (API), which strongly supports the accessibility and reusability of the data. Understanding and utilizing the NOMAD API opens up new possibilities for researchers. It serves as a gateway to a more dynamic, efficient, and customizable research and analysis experience. With the NOMAD API you will be able to: Filter Data Efficiently : Build customized datasets for specific needs, such as machine learning applications. Automate Tasks : Streamline data collection, retrieval, and analysis through automation. In this module, you will learn how to: Set up your programming environment : Prepare both local and cloud-based environments for using the NOMAD API. Understand API basics : Gain an understanding of essential API concepts and HTTP methods. Retrieve data from NOMAD using API : Explore practical examples of making API calls, using pagination for large datasets, and leveraging API helpers available in the NOMAD GUI. Upload data using API : Discover how to authenticate with the API, create datasets and upload your data programmatically.","title":"Overview"},{"location":"Module4/M4_1_getting_started/M4_1_0_getting_started.html","text":"Setting up your programming environment \u00b6 To use NOMAD programmatically, you need to set up an appropriate programming environment. What is a programming environment? A programming environment refers to the setup and tools required for developing and running software. It includes both the hardware and software components that provide a workspace for coding, testing, and executing programs. There are two suggested approaches: Local installation : Download and install Python 3 along with the required libraries on your machine. You may also want to try API commands using a terminal. Cloud-based solutions : Alternatively, cloud platforms offer convenient, ready-to-use environments. Among several options, we suggest the NOMAD Remote Tools Hub (NORTH) as it offers an optimized experience tailored to NOMAD users. What is a terminal? A terminal, also known as a Command Line Interface (CLI), is a text-based tool that allows you to interact directly with your operating system using text commands. It allows execution of programs and direct management of files and processes. Different operating systems provide distinct types of terminals: Linux : The default terminal is usually named \"Terminal\" and typically runs the Bash shell, widely used in Linux environments. macOS : The terminal is also called \"Terminal\" or in inewer versions zsh , and generally uses Bash as well. Windows : Includes Command Prompt and PowerShell , which differ from Bash. While CLI commands vary across these operating systems, many fundamental tasks remain similar. In this tutorial, you see a few instances of bash commands. For some further information about \"terminal,\" \"console,\" \"shell,\" and \"command line,\" you can read here . In this workshop, we outline steps that are applicable to both local and cloud-based setups, so you can choose either option. But if you are setting up your local programming environment for the first time, we suggest to reserve some time (up to 2 hours) for that.","title":"Programming Environments"},{"location":"Module4/M4_1_getting_started/M4_1_0_getting_started.html#setting-up-your-programming-environment","text":"To use NOMAD programmatically, you need to set up an appropriate programming environment. What is a programming environment? A programming environment refers to the setup and tools required for developing and running software. It includes both the hardware and software components that provide a workspace for coding, testing, and executing programs. There are two suggested approaches: Local installation : Download and install Python 3 along with the required libraries on your machine. You may also want to try API commands using a terminal. Cloud-based solutions : Alternatively, cloud platforms offer convenient, ready-to-use environments. Among several options, we suggest the NOMAD Remote Tools Hub (NORTH) as it offers an optimized experience tailored to NOMAD users. What is a terminal? A terminal, also known as a Command Line Interface (CLI), is a text-based tool that allows you to interact directly with your operating system using text commands. It allows execution of programs and direct management of files and processes. Different operating systems provide distinct types of terminals: Linux : The default terminal is usually named \"Terminal\" and typically runs the Bash shell, widely used in Linux environments. macOS : The terminal is also called \"Terminal\" or in inewer versions zsh , and generally uses Bash as well. Windows : Includes Command Prompt and PowerShell , which differ from Bash. While CLI commands vary across these operating systems, many fundamental tasks remain similar. In this tutorial, you see a few instances of bash commands. For some further information about \"terminal,\" \"console,\" \"shell,\" and \"command line,\" you can read here . In this workshop, we outline steps that are applicable to both local and cloud-based setups, so you can choose either option. But if you are setting up your local programming environment for the first time, we suggest to reserve some time (up to 2 hours) for that.","title":"Setting up your programming environment"},{"location":"Module4/M4_1_getting_started/M4_1_1_preparing_local.html","text":"Setting Up a Local Programming Environment \u00b6 A local programming environment can include various tools depending on its purpose. To interact with NOMAD programmatically and analyze data, you would need a Linux terminal and Python 3 . This setup is sufficient for the topics covered in this tutorial. What are the main tools in a typical programming setup? The purpose of the programming environment defines the specific tools to be used in the respective setup. These generally include: IDE (Integrated Development Environment) : Tools like Visual Studio Code (vscode) or PyCharm to edit and manage code projects efficiently. You don't necessarily need one for this tutorial, we highly recommended to have one, like vscode. Git : Git or another version control system for managing and tracking code changes. Installing Git on Windows also installs Git Bash by default, providing a Unix-like command-line interface suitable for this tutorial. Terminal : A command-line interface to execute text commands. For our purposes, the default terminal on Linux and macOS works well. Windows users may prefer WSL (Windows Subsystem for Linux) or Git Bash to run bash commands. Setting Up a Linux Terminal \u00b6 For Linux and macOS users, the default terminal is sufficient for this tutorial. Windows users have several options: Install Git for Windows : Installing Git which also provides Git Bash , a Linux-like terminal. It is the fastest way and is sufficient for this tutorial. Windows Subsystem for Linux (WSL) : WSL runs a full Linux environment including its terminal on Windows. Learn how to install WSL . Separate Linux Installation : For those interested in a dedicated Linux OS, you can install a full distribution like Ubuntu, parallel to your Windows. You can learn more about installing Ubuntu in its official website . Installing Python 3 using Anaconda \u00b6 Python provides a strong toolkit for our purpose. You will mostly use the requests library for connecting to NOMAD. For data analysis and visualization, you will need libraries such as NumPy (a base N-dimensional array package), SciPy (for scientific computing), and Matplotlib (for plotting). Anaconda is a recommended Python distribution for scientific computing that brings these libraries together and simplifies setup. Go to the Anaconda download page and download the Anaconda installer for Python 3. You can skip the registration. Follow the steps provided by the installation wizard, and select the default or recommended options if you are unsure. After installation, a system reboot may be helpful. With this step completed, Python 3 and the essential libraries for data analysis and visualization are now ready for use on your system. Running Jupyter Notebook for Python \u00b6 Open Anaconda Navigator from your applications list and launch Jupyter Notebook. This opens a browser window or tab with Jupyter's file explorer. In the Jupyter Notebook interface, navigate to your desired directory and click the New button in the top right corner and select 'Python 3' from the drop-down menu. This will create a new Jupyter Notebook with a Python 3 kernel. You can rename the notebook by clicking on the default title ( Untitled ). To run code, type it in a cell and press Shift + Enter (e.g., try 1 + 1 ). Installing the requests Library Using Jupyter Notebook \u00b6 The requests library, needed for making HTTP requests, is not included in the default Anaconda package. Therefore, it should be installed separately, e.g., using the pip package manager. It's a good practice to update pip frequently, e.g., before installing the requests library. To do this, run the following commands in a Jupyter Notebook cell: ! pip install -- upgrade pip ! pip install requests What is pip? pip is a package manager specifically for Python, making it easy to install and manage Python libraries like requests . Other package managers include: conda : An alternative to pip, suitable for Python and other data science packages. Homebrew : Primarily for macOS and Linux, simplifies installing software not included in the OS. apt : The default package manager for Debian-based Linux systems, mainly used for system-level packages.","title":"Local Installation"},{"location":"Module4/M4_1_getting_started/M4_1_1_preparing_local.html#setting-up-a-local-programming-environment","text":"A local programming environment can include various tools depending on its purpose. To interact with NOMAD programmatically and analyze data, you would need a Linux terminal and Python 3 . This setup is sufficient for the topics covered in this tutorial. What are the main tools in a typical programming setup? The purpose of the programming environment defines the specific tools to be used in the respective setup. These generally include: IDE (Integrated Development Environment) : Tools like Visual Studio Code (vscode) or PyCharm to edit and manage code projects efficiently. You don't necessarily need one for this tutorial, we highly recommended to have one, like vscode. Git : Git or another version control system for managing and tracking code changes. Installing Git on Windows also installs Git Bash by default, providing a Unix-like command-line interface suitable for this tutorial. Terminal : A command-line interface to execute text commands. For our purposes, the default terminal on Linux and macOS works well. Windows users may prefer WSL (Windows Subsystem for Linux) or Git Bash to run bash commands.","title":"Setting Up a Local Programming Environment"},{"location":"Module4/M4_1_getting_started/M4_1_1_preparing_local.html#setting-up-a-linux-terminal","text":"For Linux and macOS users, the default terminal is sufficient for this tutorial. Windows users have several options: Install Git for Windows : Installing Git which also provides Git Bash , a Linux-like terminal. It is the fastest way and is sufficient for this tutorial. Windows Subsystem for Linux (WSL) : WSL runs a full Linux environment including its terminal on Windows. Learn how to install WSL . Separate Linux Installation : For those interested in a dedicated Linux OS, you can install a full distribution like Ubuntu, parallel to your Windows. You can learn more about installing Ubuntu in its official website .","title":"Setting Up a Linux Terminal"},{"location":"Module4/M4_1_getting_started/M4_1_1_preparing_local.html#installing-python-3-using-anaconda","text":"Python provides a strong toolkit for our purpose. You will mostly use the requests library for connecting to NOMAD. For data analysis and visualization, you will need libraries such as NumPy (a base N-dimensional array package), SciPy (for scientific computing), and Matplotlib (for plotting). Anaconda is a recommended Python distribution for scientific computing that brings these libraries together and simplifies setup. Go to the Anaconda download page and download the Anaconda installer for Python 3. You can skip the registration. Follow the steps provided by the installation wizard, and select the default or recommended options if you are unsure. After installation, a system reboot may be helpful. With this step completed, Python 3 and the essential libraries for data analysis and visualization are now ready for use on your system.","title":"Installing Python 3 using Anaconda"},{"location":"Module4/M4_1_getting_started/M4_1_1_preparing_local.html#running-jupyter-notebook-for-python","text":"Open Anaconda Navigator from your applications list and launch Jupyter Notebook. This opens a browser window or tab with Jupyter's file explorer. In the Jupyter Notebook interface, navigate to your desired directory and click the New button in the top right corner and select 'Python 3' from the drop-down menu. This will create a new Jupyter Notebook with a Python 3 kernel. You can rename the notebook by clicking on the default title ( Untitled ). To run code, type it in a cell and press Shift + Enter (e.g., try 1 + 1 ).","title":"Running Jupyter Notebook for Python"},{"location":"Module4/M4_1_getting_started/M4_1_1_preparing_local.html#installing-the-requests-library-using-jupyter-notebook","text":"The requests library, needed for making HTTP requests, is not included in the default Anaconda package. Therefore, it should be installed separately, e.g., using the pip package manager. It's a good practice to update pip frequently, e.g., before installing the requests library. To do this, run the following commands in a Jupyter Notebook cell: ! pip install -- upgrade pip ! pip install requests What is pip? pip is a package manager specifically for Python, making it easy to install and manage Python libraries like requests . Other package managers include: conda : An alternative to pip, suitable for Python and other data science packages. Homebrew : Primarily for macOS and Linux, simplifies installing software not included in the OS. apt : The default package manager for Debian-based Linux systems, mainly used for system-level packages.","title":"Installing the requests Library Using Jupyter Notebook"},{"location":"Module4/M4_1_getting_started/M4_1_2_preparing_north.html","text":"The NOMAD Remote Tools Hub (NORTH) \u00b6 NORTH allows you to run pre-configured applications directly on the NOMAD servers. With NORTH, you can access, analyze, and create data directly in your uploads using your favorite tools with no setup required. With NORTH you will have everything you need for this tutoria, i.e., a Linux terminal and a Python 3 installation with the required libraries. Follow these steps to get started with NORTH: Begin by visiting the NOMAD . Once there, open NOMAD to reach the main interface. Navigate to the 'ANALYZE' section, which you'll find at the top of the page. Here, click on NOMAD Remote Tools Hub to access the remote tools available to you. To use NORTH, you need to be logged in. Click on 'LOGIN / REGISTER' to proceed. If you are not yet registered, now is the right the time to join the NOMAD community. In the NORTH page you see several options. Let's start a new session by selecting 'Jupyter Notebook: The Classic Notebook Interface'. Click on LAUNCH , wait a few seconds and click OPEN . Note: If a new page does not automatically appear, check to see if pop-ups are allowed in your browser, as this might prevent the notebook from opening. If you still see issues, try it with the Chrome browser. In the new page you see several tools, including a Python 3 (ipykernel) under Notebook and a Terminal under Other. So you have everything you need. for now, let's choose the Python 3 (ipykernel) to begin your session, which is the environment where you can run Python code. A Jupyter Notebook page opens, equipped with the libraries you need. This is where you can analyze data, visualize results, or interact with the NOMAD API.","title":"Cloud-based (NORTH)"},{"location":"Module4/M4_1_getting_started/M4_1_2_preparing_north.html#the-nomad-remote-tools-hub-north","text":"NORTH allows you to run pre-configured applications directly on the NOMAD servers. With NORTH, you can access, analyze, and create data directly in your uploads using your favorite tools with no setup required. With NORTH you will have everything you need for this tutoria, i.e., a Linux terminal and a Python 3 installation with the required libraries. Follow these steps to get started with NORTH: Begin by visiting the NOMAD . Once there, open NOMAD to reach the main interface. Navigate to the 'ANALYZE' section, which you'll find at the top of the page. Here, click on NOMAD Remote Tools Hub to access the remote tools available to you. To use NORTH, you need to be logged in. Click on 'LOGIN / REGISTER' to proceed. If you are not yet registered, now is the right the time to join the NOMAD community. In the NORTH page you see several options. Let's start a new session by selecting 'Jupyter Notebook: The Classic Notebook Interface'. Click on LAUNCH , wait a few seconds and click OPEN . Note: If a new page does not automatically appear, check to see if pop-ups are allowed in your browser, as this might prevent the notebook from opening. If you still see issues, try it with the Chrome browser. In the new page you see several tools, including a Python 3 (ipykernel) under Notebook and a Terminal under Other. So you have everything you need. for now, let's choose the Python 3 (ipykernel) to begin your session, which is the environment where you can run Python code. A Jupyter Notebook page opens, equipped with the libraries you need. This is where you can analyze data, visualize results, or interact with the NOMAD API.","title":"The NOMAD Remote Tools Hub (NORTH)"},{"location":"Module4/M4_1_getting_started/M4_1_3_api_basics.html","text":"Before we start with the practical examples of using NOMAD\u2019s API, let's briefly provide a short explanation of essential concepts: What is API? \u00b6 An API, Application Programming Interface, is a set of rules and protocols that allows different software programs or hardware to communicate with each other, facilitating communication and data exchange between them. APIs are widely used across various systems, including web-based services, database systems, operating systems, and even hardware interactions. In the context of this tutorial, we focus on APIs used for web-based interactions, such as those employed by NOMAD to facilitate data access and management. An API works by sending requests and receiving responses , using specific endpoints defined by the API to access particular resources or services. Data are usually exchanged in standard formats like JSON or XML. The interaction with an API involves three main parts: The User : Initiates requests, typically through a client application. The Client : A software that formats and sends the user\u2019s requests according to the API's rules. The Server : A system that processes requests and provides the appropriate responses, often hosting the API. What is an API Client? API clients are tools, libraries, or software that enable interaction with APIs. Each type serves specific purposes, from quick testing to developing full-scale applications. Here are some common types: Command-line tools like cURL are lightweight and flexible, ideal for testing and debugging APIs. Programming libraries such as Python's requests allow to integrate API calls into analysis applications and automation programmatically. Software like web browsers (e.g., Chrome, Firefox) or Postman provide user-friendly interfaces for exploring and interacting with APIs. The user does not directly interact with the server. Instead, they rely on the client to communicate with the API. The client formats the user\u2019s request, sends it to the server, and processes the response from the API to present it to the user. The API specifies how these requests and responses are structured, ensuring that communication is standardized and secure. You can think of this relationship as similar to a restaurant experience.: 1 you (the user) place your order (the request), and the waiter (the client) brings it to the kitchen (the server). After your dish (the data) is prepared, the waiter delivers it back to you (the response). This waiter-service interaction is similar to how an API functions as a messenger between you and an application. The API ensures that requests are formatted correctly and responses are delivered accurately, much like the waiter ensures the order is communicated to the kitchen and delivered to the diner. Web-hosted APIs often use the Hypertext Transfer Protocol (HTTP), which provides standard methods for sending requests and receiving responses, as well as reporting the status of these interactions. What are different types of APIs? APIs connect systems, and their types differ to meet diverse functionality and performance demands. Here are the 3 widely used ones. REST (Representational State Transfer) : The most commonly used API for web-hosted services, relying on HTTP methods and data formats like JSON (e.g., NOMAD). GraphQL : A flexible API type that allows clients to request only the data they need, reducing inefficiencies (e.g., GitHub). Webhooks : Event-driven APIs that send real-time updates to clients when specific events occur (e.g., Discord, PayPal). Common data formats used by APIs: JSON and XML APIs often use standardized formats to exchange data. The two most common ones are: JSON (JavaScript Object Notation) : A lightweight format that represents data as key-value pairs, making it easy for both humans and machines to read and process. { \"name\" : \"Max Mustermann\" , \"role\" : \"Lab Technician\" , \"email\" : \"max.mustermann@uni-something.de\" , \"phone\" : \"+49 123 456789\" } XML (eXtensible Markup Language) : A markup language designed for data transport and storage, structured in a readable, hierarchical format. <contact> <name> Max Mustermann </name> <role> Lab Technician </role> <email> max.mustermann@uni-something.de </email> <phone> +49 123 456789 </phone> </contact> What is HTTP? HTTP, or Hypertext Transfer Protocol, is a standardized set of rules for exchanging data over the web. It enables communication between clients (like browsers or applications) and servers, allowing requests for and delivery of resources such as webpages, images, or data. When you enter a website address (URL) into your browser, the browser sends an HTTP request to the server hosting that site. The server processes the request and responds with the required data, which the browser then renders for you to interact with. HTTP is not just for web browsing\u2014it also forms the foundation for APIs, enabling communication and data exchange between systems. The most frequently used HTTP methods for interacting with resources are: GET : Retrieves data from a server. Commonly used to fetch webpages or resources. POST : Sends data to the server to create a new resource. DELETE : Removes a resource from the server at a specified URL. PUT : Updates an existing resource or creates a new one if it doesn\u2019t exist. Before getting into the details, lets go throught the general workflow of APIs and the essential elements: What is the general API workflow? \u00b6 1. It all starts with creating a request: API requests are prepared by the user and sent to the server through the client. A request typically includes the following elements: HTTP Method : Specifies the action to be performed (e.g., GET, POST, DELETE). Endpoint : The URL of the resource or service on the srver. Headers : Provide metadata, such as authentication tokens or content type. Body : Contains the data required for actions like creating or updating resources. What is an API endpoint? An API endpoint is a specific URL that acts as an entry point to interact with a particular resource or service provided by an API. Endpoints define where requests should be sent and what actions can be performed. For example, in NOMAD's API, each resource is associated with specific endpoints that allow you to retrieve, create, update, or delete data. The NOMAD API Dashboard highlights these endpoints and the types of requests they support: 2. Processing the request by the server: The server receives the request, and validates it. If valid, the server interacts with its resources (e.g., a database or application) and prepares a response. Otherwise, it prepares an error response (e.g., status code 400 for a bad request). 3. Sending the back the response: The server sends the response back through the client, which includes: Status Code : A numeric code telling that the request was successful (200), or why it failed. (e.g., 404). Headers : Provide metadata about the response. Body : Contains the data or error details, and typically comes in formats like JSON or XML. What are HTTP status codes? These codes can help you diagnose and address issues with your requests by indicating whether the request was successful or if an error occurred. Here are the main status codes you might receive from NOMAD API and how to address errors in case of a failure: Response Code Description Common Causes Suggested Action 200 Successful Response Correct request execution None needed 400 Bad Request Syntax errors, invalid arguments Fix syntax and parameters 401 Unauthorized Invalid credentials Verify credentials 404 Not Found Incorrect URL, missing resource Check URL and resource 422 Validation Error Semantic errors in data Adjust data to API documentation For a visual explanation of how APIs work, you may find this YouTube video helpful. \u21a9","title":"API Basics"},{"location":"Module4/M4_1_getting_started/M4_1_3_api_basics.html#what-is-api","text":"An API, Application Programming Interface, is a set of rules and protocols that allows different software programs or hardware to communicate with each other, facilitating communication and data exchange between them. APIs are widely used across various systems, including web-based services, database systems, operating systems, and even hardware interactions. In the context of this tutorial, we focus on APIs used for web-based interactions, such as those employed by NOMAD to facilitate data access and management. An API works by sending requests and receiving responses , using specific endpoints defined by the API to access particular resources or services. Data are usually exchanged in standard formats like JSON or XML. The interaction with an API involves three main parts: The User : Initiates requests, typically through a client application. The Client : A software that formats and sends the user\u2019s requests according to the API's rules. The Server : A system that processes requests and provides the appropriate responses, often hosting the API. What is an API Client? API clients are tools, libraries, or software that enable interaction with APIs. Each type serves specific purposes, from quick testing to developing full-scale applications. Here are some common types: Command-line tools like cURL are lightweight and flexible, ideal for testing and debugging APIs. Programming libraries such as Python's requests allow to integrate API calls into analysis applications and automation programmatically. Software like web browsers (e.g., Chrome, Firefox) or Postman provide user-friendly interfaces for exploring and interacting with APIs. The user does not directly interact with the server. Instead, they rely on the client to communicate with the API. The client formats the user\u2019s request, sends it to the server, and processes the response from the API to present it to the user. The API specifies how these requests and responses are structured, ensuring that communication is standardized and secure. You can think of this relationship as similar to a restaurant experience.: 1 you (the user) place your order (the request), and the waiter (the client) brings it to the kitchen (the server). After your dish (the data) is prepared, the waiter delivers it back to you (the response). This waiter-service interaction is similar to how an API functions as a messenger between you and an application. The API ensures that requests are formatted correctly and responses are delivered accurately, much like the waiter ensures the order is communicated to the kitchen and delivered to the diner. Web-hosted APIs often use the Hypertext Transfer Protocol (HTTP), which provides standard methods for sending requests and receiving responses, as well as reporting the status of these interactions. What are different types of APIs? APIs connect systems, and their types differ to meet diverse functionality and performance demands. Here are the 3 widely used ones. REST (Representational State Transfer) : The most commonly used API for web-hosted services, relying on HTTP methods and data formats like JSON (e.g., NOMAD). GraphQL : A flexible API type that allows clients to request only the data they need, reducing inefficiencies (e.g., GitHub). Webhooks : Event-driven APIs that send real-time updates to clients when specific events occur (e.g., Discord, PayPal). Common data formats used by APIs: JSON and XML APIs often use standardized formats to exchange data. The two most common ones are: JSON (JavaScript Object Notation) : A lightweight format that represents data as key-value pairs, making it easy for both humans and machines to read and process. { \"name\" : \"Max Mustermann\" , \"role\" : \"Lab Technician\" , \"email\" : \"max.mustermann@uni-something.de\" , \"phone\" : \"+49 123 456789\" } XML (eXtensible Markup Language) : A markup language designed for data transport and storage, structured in a readable, hierarchical format. <contact> <name> Max Mustermann </name> <role> Lab Technician </role> <email> max.mustermann@uni-something.de </email> <phone> +49 123 456789 </phone> </contact> What is HTTP? HTTP, or Hypertext Transfer Protocol, is a standardized set of rules for exchanging data over the web. It enables communication between clients (like browsers or applications) and servers, allowing requests for and delivery of resources such as webpages, images, or data. When you enter a website address (URL) into your browser, the browser sends an HTTP request to the server hosting that site. The server processes the request and responds with the required data, which the browser then renders for you to interact with. HTTP is not just for web browsing\u2014it also forms the foundation for APIs, enabling communication and data exchange between systems. The most frequently used HTTP methods for interacting with resources are: GET : Retrieves data from a server. Commonly used to fetch webpages or resources. POST : Sends data to the server to create a new resource. DELETE : Removes a resource from the server at a specified URL. PUT : Updates an existing resource or creates a new one if it doesn\u2019t exist. Before getting into the details, lets go throught the general workflow of APIs and the essential elements:","title":"What is API?"},{"location":"Module4/M4_1_getting_started/M4_1_3_api_basics.html#what-is-the-general-api-workflow","text":"1. It all starts with creating a request: API requests are prepared by the user and sent to the server through the client. A request typically includes the following elements: HTTP Method : Specifies the action to be performed (e.g., GET, POST, DELETE). Endpoint : The URL of the resource or service on the srver. Headers : Provide metadata, such as authentication tokens or content type. Body : Contains the data required for actions like creating or updating resources. What is an API endpoint? An API endpoint is a specific URL that acts as an entry point to interact with a particular resource or service provided by an API. Endpoints define where requests should be sent and what actions can be performed. For example, in NOMAD's API, each resource is associated with specific endpoints that allow you to retrieve, create, update, or delete data. The NOMAD API Dashboard highlights these endpoints and the types of requests they support: 2. Processing the request by the server: The server receives the request, and validates it. If valid, the server interacts with its resources (e.g., a database or application) and prepares a response. Otherwise, it prepares an error response (e.g., status code 400 for a bad request). 3. Sending the back the response: The server sends the response back through the client, which includes: Status Code : A numeric code telling that the request was successful (200), or why it failed. (e.g., 404). Headers : Provide metadata about the response. Body : Contains the data or error details, and typically comes in formats like JSON or XML. What are HTTP status codes? These codes can help you diagnose and address issues with your requests by indicating whether the request was successful or if an error occurred. Here are the main status codes you might receive from NOMAD API and how to address errors in case of a failure: Response Code Description Common Causes Suggested Action 200 Successful Response Correct request execution None needed 400 Bad Request Syntax errors, invalid arguments Fix syntax and parameters 401 Unauthorized Invalid credentials Verify credentials 404 Not Found Incorrect URL, missing resource Check URL and resource 422 Validation Error Semantic errors in data Adjust data to API documentation For a visual explanation of how APIs work, you may find this YouTube video helpful. \u21a9","title":"What is the general API workflow?"},{"location":"Module4/M4_2_retrieve_data/M4_2_1_example_api_explained.html","text":"Your first NOMAD API call \u00b6 Let's consider the following example. Imagine we would like search NOMAD for materials that contain both Ti and O in their chemical formula. Since NOMAD ,ost likely contains a large number of matching entries, we narrow the search to just one result and request only its entry_id . The Request \u00b6 For this you will need to prepare a proper search query, send it to the NOMAD's API using a client. NOMAD servers process your request and prepare a response and sends back to you via the client. You can run the following command in your terminal to execute this query: curl -X POST \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json\" \\ -d '{ \"query\": { \"results.material.elements\": { \"all\": [\"Ti\", \"O\"] } }, \"pagination\": { \"page_size\": 1 }, \"required\": { \"include\": [\"entry_id\"] } }' In this example: You use cURL as your API client in the terminal to send your HTTP request. The HTTP method used is POST , implying that you are sending (i.e., posting) data to create or update resources (in this case, you are creating a search query in NOMAD servers, which did not exist previously, therefore you used POST ). The -X flag in curl specifies the request method ( POST ) to use when communicating with the HTTP server. You send the request to the API endpoint which is \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" . Since the initiall part of the URL stays the same in different endpoints, sometimes only the changing part ( \"/entries/query\" ) is referred to as endpoint. What are NOMAD's API endpoints? In general, rach API has documentation explaining its endpoints. A list of API endpoints and their documentation can be found in NOMAD's API dashboard . The endpoints in NOMAD often imply their functionality. For instance, a POST request to the endpoint /datasets/{dataset_id}/action/doi implies assigning a DOI to a dataset by providing its dataset_id . Headers are flagged with -H . The header Content-Type: application/json tells that the the data that you are sending (the request body) is in JSON format. The Accept: application/json tells you would like to receive the response as JSON format as well. If an authentication were required, it would be included here as well. The Body (data we are sending) is flagged with -d . The JSON object after the flag -d within the single quotes is the actual JSON data being sent with the request, which defines the search parameters and what information to include in the response: { \"query\" : { \"results.material.elements\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 }, \"required\" : { \"include\" : [ \"entry_id\" ] } } Let's have a closer look to the data (request body) you included after the -d flag. It defines the search criteria (materials containing the elements \"Ti\" and \"O\"), how many results to return (in our case, 1), and what details to include in the response (entry_id) in a structured format. The Response \u00b6 The response you receive from the server will look similar to the following: { \"owner\" : \"public\" , \"query\" : { \"name\" : \"results.material.elements\" , \"value\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"entry_id\" , \"order\" : \"asc\" , \"total\" : 41544 , \"next_page_after_value\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" }, \"required\" : { \"include\" : [ \"entry_id\" ] }, \"data\" : [ { \"entry_id\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" } ] } If you look at the response closely, you will notice that the response contains information that is partly repeating your request (not necessarily word-for-word, but in a normalized way), and partly default values for optional parameters that you even did not include in our request , and finally the actual data that you requested . Let\u2019s consider the following simplified layout of the server response that you received from the /entries/query endpoint of the NOMAD API. It includes placeholders (\"...\") to represent details that we discuss later: { \"owner\" : \"public\" , \"query\" : { ... }, \"pagination\" : { ... }, \"required\" : { ... }, \"data\" : [{ \"entry_id\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" }] } In this simplified response: owner : Indicates which data are being queried. public means publicly available data. You will see more details soon. query : This section reiterates the search parameters you sent, but in a normalized structure. pagination : Manages how the data is returned or displayed. required : Elements or parameters that you specifically requested to be included or excluded in the response data. data : The actual data you received as response. Now, let's break it down and explore more: owner : The owner parameter in the NOMAD API allows you to filter entries based on their visibility and ownership. You can specify your search to include various levels of access such as admin , all , public , shared , staging , user , and visible . Each option tailors the search to different visibility levels, from entries accessible by all users to those restricted to private use. For detailed explanations of each option, refer to the NOMAD Documentation . query : The reformatted version of our initial query (see the \"name\" and \"value\" lables): \"query\" : { \"name\" : \"results.material.elements\" , \"value\" : { \"all\" : [ \"Ti\" , \"O\" ] } } Advanced query options in NOMAD The example we are inspecting here is actually a basic query example. NOMAD supports more complex queries that can be written using logical operators such as and , or , and not . Also, you can use shortcuts to modify the logical combination of values in a list, such as any: (default) or none: . Furthermore, comparison operators like lt: (less than) or gt: (greater than) can also be used to refine your query further, either as shortcuts or extra filter parameters. For detailed explanations and examples of advanced query options, refer to the NOMAD Documentation . pagination : The pagination section in the API response organizes how results are delivered: page_size : Number of results per page, that you set to 1 in your query. order_by : Indicates the criteria used to order the entries. Here the matching 41544 entries were sorted by entry_id (the default in this case) and the first one ( \"page_size\": 1 ) is returned in the response. order : The sorting direction to sort the 41544 matching entries. can be ascending asc ) or descending desc . The default is asc . total : Total number of entries matching the query, here 41544. next_page_after_value : Provides a cursor for pagination. This value is used to fetch the next set of results in subsequent queries. It helps in managing large datasets by allowing the user to continue retrieving data right where the previous query left off. The NOMAD Documentation provides a an example on how to use the next_page_after_value in order to fetch larger datasets. \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"entry_id\" , \"order\" : \"asc\" , \"total\" : 41544 , \"next_page_after_value\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" } Please note that the pagination section in the response also includes default values for parameters not explicitly set in our POST request, such as order_by , order , and next_page_after_value . In our next example , we will explore how these parameters can be used to enhance control over API calls. How do they look in Python? \u00b6 The above example was just to showcase different components of a basic API call. We used the curl client, because it was easier to explain different components of an API request. However, our focus in this tutorial will be on using the Python's requests library as the API client. This approach combines simplicity with powerful capabilities, making it ideal for both beginners and experienced users. In particular its built-in JSON decoding capabilities, simplify working with JSON data. The same POST request can be written using the Python requests library. For this, open a programming environment of your choice (e.g., VSCode, Jupyter Notebook, NORTH, etc.) and paste the following piece of code and run: import requests import json url = \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" headers = { \"Content-Type\" : \"application/json\" , \"Accept\" : \"application/json\" } request_data = { \"query\" : { \"results.material.elements\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 }, \"required\" : { \"include\" : [ \"entry_id\" ] } } response = requests . post ( url , headers = headers , data = json . dumps ( request_data )) formatted_response = json . dumps ( response . json (), indent = 2 ) print ( formatted_response ) The response you receive should look similar to what you have received when you sent the API call using curl from your terminal: { \"owner\" : \"public\" , \"query\" : { \"name\" : \"results.material.elements\" , \"value\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"entry_id\" , \"order\" : \"asc\" , \"total\" : 41544 , \"next_page_after_value\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" }, \"required\" : { \"include\" : [ \"entry_id\" ] }, \"data\" : [ { \"entry_id\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" } ] }","title":"Your First API Call"},{"location":"Module4/M4_2_retrieve_data/M4_2_1_example_api_explained.html#your-first-nomad-api-call","text":"Let's consider the following example. Imagine we would like search NOMAD for materials that contain both Ti and O in their chemical formula. Since NOMAD ,ost likely contains a large number of matching entries, we narrow the search to just one result and request only its entry_id .","title":"Your first NOMAD API call"},{"location":"Module4/M4_2_retrieve_data/M4_2_1_example_api_explained.html#the-request","text":"For this you will need to prepare a proper search query, send it to the NOMAD's API using a client. NOMAD servers process your request and prepare a response and sends back to you via the client. You can run the following command in your terminal to execute this query: curl -X POST \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/json\" \\ -d '{ \"query\": { \"results.material.elements\": { \"all\": [\"Ti\", \"O\"] } }, \"pagination\": { \"page_size\": 1 }, \"required\": { \"include\": [\"entry_id\"] } }' In this example: You use cURL as your API client in the terminal to send your HTTP request. The HTTP method used is POST , implying that you are sending (i.e., posting) data to create or update resources (in this case, you are creating a search query in NOMAD servers, which did not exist previously, therefore you used POST ). The -X flag in curl specifies the request method ( POST ) to use when communicating with the HTTP server. You send the request to the API endpoint which is \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" . Since the initiall part of the URL stays the same in different endpoints, sometimes only the changing part ( \"/entries/query\" ) is referred to as endpoint. What are NOMAD's API endpoints? In general, rach API has documentation explaining its endpoints. A list of API endpoints and their documentation can be found in NOMAD's API dashboard . The endpoints in NOMAD often imply their functionality. For instance, a POST request to the endpoint /datasets/{dataset_id}/action/doi implies assigning a DOI to a dataset by providing its dataset_id . Headers are flagged with -H . The header Content-Type: application/json tells that the the data that you are sending (the request body) is in JSON format. The Accept: application/json tells you would like to receive the response as JSON format as well. If an authentication were required, it would be included here as well. The Body (data we are sending) is flagged with -d . The JSON object after the flag -d within the single quotes is the actual JSON data being sent with the request, which defines the search parameters and what information to include in the response: { \"query\" : { \"results.material.elements\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 }, \"required\" : { \"include\" : [ \"entry_id\" ] } } Let's have a closer look to the data (request body) you included after the -d flag. It defines the search criteria (materials containing the elements \"Ti\" and \"O\"), how many results to return (in our case, 1), and what details to include in the response (entry_id) in a structured format.","title":"The Request"},{"location":"Module4/M4_2_retrieve_data/M4_2_1_example_api_explained.html#the-response","text":"The response you receive from the server will look similar to the following: { \"owner\" : \"public\" , \"query\" : { \"name\" : \"results.material.elements\" , \"value\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"entry_id\" , \"order\" : \"asc\" , \"total\" : 41544 , \"next_page_after_value\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" }, \"required\" : { \"include\" : [ \"entry_id\" ] }, \"data\" : [ { \"entry_id\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" } ] } If you look at the response closely, you will notice that the response contains information that is partly repeating your request (not necessarily word-for-word, but in a normalized way), and partly default values for optional parameters that you even did not include in our request , and finally the actual data that you requested . Let\u2019s consider the following simplified layout of the server response that you received from the /entries/query endpoint of the NOMAD API. It includes placeholders (\"...\") to represent details that we discuss later: { \"owner\" : \"public\" , \"query\" : { ... }, \"pagination\" : { ... }, \"required\" : { ... }, \"data\" : [{ \"entry_id\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" }] } In this simplified response: owner : Indicates which data are being queried. public means publicly available data. You will see more details soon. query : This section reiterates the search parameters you sent, but in a normalized structure. pagination : Manages how the data is returned or displayed. required : Elements or parameters that you specifically requested to be included or excluded in the response data. data : The actual data you received as response. Now, let's break it down and explore more: owner : The owner parameter in the NOMAD API allows you to filter entries based on their visibility and ownership. You can specify your search to include various levels of access such as admin , all , public , shared , staging , user , and visible . Each option tailors the search to different visibility levels, from entries accessible by all users to those restricted to private use. For detailed explanations of each option, refer to the NOMAD Documentation . query : The reformatted version of our initial query (see the \"name\" and \"value\" lables): \"query\" : { \"name\" : \"results.material.elements\" , \"value\" : { \"all\" : [ \"Ti\" , \"O\" ] } } Advanced query options in NOMAD The example we are inspecting here is actually a basic query example. NOMAD supports more complex queries that can be written using logical operators such as and , or , and not . Also, you can use shortcuts to modify the logical combination of values in a list, such as any: (default) or none: . Furthermore, comparison operators like lt: (less than) or gt: (greater than) can also be used to refine your query further, either as shortcuts or extra filter parameters. For detailed explanations and examples of advanced query options, refer to the NOMAD Documentation . pagination : The pagination section in the API response organizes how results are delivered: page_size : Number of results per page, that you set to 1 in your query. order_by : Indicates the criteria used to order the entries. Here the matching 41544 entries were sorted by entry_id (the default in this case) and the first one ( \"page_size\": 1 ) is returned in the response. order : The sorting direction to sort the 41544 matching entries. can be ascending asc ) or descending desc . The default is asc . total : Total number of entries matching the query, here 41544. next_page_after_value : Provides a cursor for pagination. This value is used to fetch the next set of results in subsequent queries. It helps in managing large datasets by allowing the user to continue retrieving data right where the previous query left off. The NOMAD Documentation provides a an example on how to use the next_page_after_value in order to fetch larger datasets. \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"entry_id\" , \"order\" : \"asc\" , \"total\" : 41544 , \"next_page_after_value\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" } Please note that the pagination section in the response also includes default values for parameters not explicitly set in our POST request, such as order_by , order , and next_page_after_value . In our next example , we will explore how these parameters can be used to enhance control over API calls.","title":"The Response"},{"location":"Module4/M4_2_retrieve_data/M4_2_1_example_api_explained.html#how-do-they-look-in-python","text":"The above example was just to showcase different components of a basic API call. We used the curl client, because it was easier to explain different components of an API request. However, our focus in this tutorial will be on using the Python's requests library as the API client. This approach combines simplicity with powerful capabilities, making it ideal for both beginners and experienced users. In particular its built-in JSON decoding capabilities, simplify working with JSON data. The same POST request can be written using the Python requests library. For this, open a programming environment of your choice (e.g., VSCode, Jupyter Notebook, NORTH, etc.) and paste the following piece of code and run: import requests import json url = \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" headers = { \"Content-Type\" : \"application/json\" , \"Accept\" : \"application/json\" } request_data = { \"query\" : { \"results.material.elements\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 }, \"required\" : { \"include\" : [ \"entry_id\" ] } } response = requests . post ( url , headers = headers , data = json . dumps ( request_data )) formatted_response = json . dumps ( response . json (), indent = 2 ) print ( formatted_response ) The response you receive should look similar to what you have received when you sent the API call using curl from your terminal: { \"owner\" : \"public\" , \"query\" : { \"name\" : \"results.material.elements\" , \"value\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"entry_id\" , \"order\" : \"asc\" , \"total\" : 41544 , \"next_page_after_value\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" }, \"required\" : { \"include\" : [ \"entry_id\" ] }, \"data\" : [ { \"entry_id\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" } ] }","title":"How do they look in Python?"},{"location":"Module4/M4_2_retrieve_data/M4_2_2_leveraging_pagination.html","text":"Handling Large Datasets \u00b6 When working with large datasets in NOMAD's API, efficient retrieval of data is important. pagination allows you to manage large responses by splitting them into manageable pages. In this section, you will see how to use pagination to retrieve a specific number of entries in two different ways: directly, using page_size parameter, in batches, using a cursor. To start, let's recall [the first basic example] where you sent a POST request to NOMAD's API to obtain the entry_id of entries containing both Ti and O in their chemical formula. The response looked like this: { \"owner\" : \"public\" , \"query\" : { \"name\" : \"results.material.elements\" , \"value\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"entry_id\" , \"order\" : \"asc\" , \"total\" : 41531 , \"next_page_after_value\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" }, \"required\" : { \"include\" : [ \"entry_id\" ] }, \"data\" : [ { \"entry_id\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" } ] } Directly, Using page_size \u00b6 Suppose you want to obtain the entry_id of the 120 most recently uploaded entries while maintaining the same query criteria. In the previous example, the pagination had \"page_size\": 1 , which retrieved only one entry. NOMAD's API allows setting the page_size up to 10000. We can also specify parameters such as order_by and order . To achieve this, you can modify the pagination in your POST request: set the page_size to 120, and change order_by and order to \"upload_create_time\" and \"desc\", respectively. import requests import json url = \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" headers = { \"Content-Type\" : \"application/json\" , \"Accept\" : \"application/json\" } request_data = { \"query\" : { \"results.material.elements\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 120 , \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" }, \"required\" : { \"include\" : [ \"entry_id\" ] } } response = requests . post ( url , headers = headers , data = json . dumps ( request_data )) response_json = response . json () for entry in response_json [ 'data' ]: print ( entry [ 'entry_id' ]) In this specific example, you are only querying the entry_id , which is a string of a few dozen bytes. Setting \"page_size\": 10000 will give a response with a size of a few hundred megabytes. But what if we wanted to query larger data (e.g., raw files) or inspect a larger number of entries (e.g., 120000 entries)? In Batches, Using a Cursor \u00b6 If we need to query larger datasets or inspect a larger number of entries (e.g., 120000 entries), it is practical to retrieve data iteratively to avoid crashes. The next_page_after_value in the pagination of response can be used as a cursor to iteratively paginate through the results. How? The page_after_value is an attribute that defines the position after which the page begins, and is used to navigate through the total list of results. When requesting the first page, no value should be provided for page_after_value parameter, therefore, we will set it to None . Each response will contain a value next_page_after_value (see the response on top of this page), which can be used to obtain the next page (by setting the page_after_value in our next request to this value). By treating next_page_after_value as a cursor \u2014indicating where the next page of results starts\u2014 and applying some Python programming techniques, you can retrieve large datasets piece by piece, avoiding system overload. The following example shows retrieving 120 entries in batches of 5 each. import requests import json url = \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" page_after_value = None entries_retrieved = 0 desired_number_of_entries = 120 headers = { \"Content-Type\" : \"application/json\" , \"Accept\" : \"application/json\" } request_data = { \"query\" : { \"results.material.elements\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 5 , \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" , \"page_after_value\" : page_after_value }, \"required\" : { \"include\" : [ \"entry_id\" ] } } while entries_retrieved < desired_number_of_entries : response = requests . post ( url , headers = headers , data = json . dumps ( request_data )) response_json = response . json () request_data [ 'pagination' ][ 'page_after_value' ] = response_json [ 'pagination' ][ 'next_page_after_value' ] for entry in response_json [ 'data' ]: print ( entry [ 'entry_id' ]) entries_retrieved += 1 if entries_retrieved >= desired_number_of_entries : break","title":"Retrieving Multiple Entries"},{"location":"Module4/M4_2_retrieve_data/M4_2_2_leveraging_pagination.html#handling-large-datasets","text":"When working with large datasets in NOMAD's API, efficient retrieval of data is important. pagination allows you to manage large responses by splitting them into manageable pages. In this section, you will see how to use pagination to retrieve a specific number of entries in two different ways: directly, using page_size parameter, in batches, using a cursor. To start, let's recall [the first basic example] where you sent a POST request to NOMAD's API to obtain the entry_id of entries containing both Ti and O in their chemical formula. The response looked like this: { \"owner\" : \"public\" , \"query\" : { \"name\" : \"results.material.elements\" , \"value\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"entry_id\" , \"order\" : \"asc\" , \"total\" : 41531 , \"next_page_after_value\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" }, \"required\" : { \"include\" : [ \"entry_id\" ] }, \"data\" : [ { \"entry_id\" : \"---5IlI80yY5VWEpBMT_H-TBHOiH\" } ] }","title":"Handling Large Datasets"},{"location":"Module4/M4_2_retrieve_data/M4_2_2_leveraging_pagination.html#directly-using-page_size","text":"Suppose you want to obtain the entry_id of the 120 most recently uploaded entries while maintaining the same query criteria. In the previous example, the pagination had \"page_size\": 1 , which retrieved only one entry. NOMAD's API allows setting the page_size up to 10000. We can also specify parameters such as order_by and order . To achieve this, you can modify the pagination in your POST request: set the page_size to 120, and change order_by and order to \"upload_create_time\" and \"desc\", respectively. import requests import json url = \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" headers = { \"Content-Type\" : \"application/json\" , \"Accept\" : \"application/json\" } request_data = { \"query\" : { \"results.material.elements\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 120 , \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" }, \"required\" : { \"include\" : [ \"entry_id\" ] } } response = requests . post ( url , headers = headers , data = json . dumps ( request_data )) response_json = response . json () for entry in response_json [ 'data' ]: print ( entry [ 'entry_id' ]) In this specific example, you are only querying the entry_id , which is a string of a few dozen bytes. Setting \"page_size\": 10000 will give a response with a size of a few hundred megabytes. But what if we wanted to query larger data (e.g., raw files) or inspect a larger number of entries (e.g., 120000 entries)?","title":"Directly, Using page_size"},{"location":"Module4/M4_2_retrieve_data/M4_2_2_leveraging_pagination.html#in-batches-using-a-cursor","text":"If we need to query larger datasets or inspect a larger number of entries (e.g., 120000 entries), it is practical to retrieve data iteratively to avoid crashes. The next_page_after_value in the pagination of response can be used as a cursor to iteratively paginate through the results. How? The page_after_value is an attribute that defines the position after which the page begins, and is used to navigate through the total list of results. When requesting the first page, no value should be provided for page_after_value parameter, therefore, we will set it to None . Each response will contain a value next_page_after_value (see the response on top of this page), which can be used to obtain the next page (by setting the page_after_value in our next request to this value). By treating next_page_after_value as a cursor \u2014indicating where the next page of results starts\u2014 and applying some Python programming techniques, you can retrieve large datasets piece by piece, avoiding system overload. The following example shows retrieving 120 entries in batches of 5 each. import requests import json url = \"http://nomad-lab.eu/prod/v1/api/v1/entries/query\" page_after_value = None entries_retrieved = 0 desired_number_of_entries = 120 headers = { \"Content-Type\" : \"application/json\" , \"Accept\" : \"application/json\" } request_data = { \"query\" : { \"results.material.elements\" : { \"all\" : [ \"Ti\" , \"O\" ] } }, \"pagination\" : { \"page_size\" : 5 , \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" , \"page_after_value\" : page_after_value }, \"required\" : { \"include\" : [ \"entry_id\" ] } } while entries_retrieved < desired_number_of_entries : response = requests . post ( url , headers = headers , data = json . dumps ( request_data )) response_json = response . json () request_data [ 'pagination' ][ 'page_after_value' ] = response_json [ 'pagination' ][ 'next_page_after_value' ] for entry in response_json [ 'data' ]: print ( entry [ 'entry_id' ]) entries_retrieved += 1 if entries_retrieved >= desired_number_of_entries : break","title":"In Batches, Using a Cursor"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_0_nomad_api_helpers.html","text":"NOMAD API Helpers \u00b6 NOMAD offers several API helpers to facilitate data retrieval, processing, and interaction. These tools give you flexible and powerful access to NOMAD's data resources and functionalities. Almost everything you see in the GUI can be accessed via the API. API button on NOMAD pages \u00b6 When exploring data in NOMAD, each page in the GUI includes an API button, ( API ), or it's symbol, ( <> ). This shows the GET request used to retrieve the data for that page. Often, there is an additional POST request available, for instance: Explore Page contains a POST request to the https://nomad-lab.eu/prod/v1/api/v1/entries/query endpoint, including the chosen filters. Entry Pages : Include a POST request to the https://nomad-lab.eu/prod/v1/api/v1/entries/{entry_id}/archive/query endoint, which allows fetching archive data (processed data) of that entry. NOMAD API dashboard \u00b6 The NOMAD API dashboard is a powerful tool for accessing and experimenting with NOMAD's data resources. It provides detailed documentation and interactive features that make it possible to learn and use the API effectively. Whether you are developing applications, automating data upload or retrieval, or just exploring data, the API dashboard is an important resource. Accessing the API dashboard \u00b6 The API dashboard can be accessed from the 'ANALYZE' menu under NOMAD APIs. Features of the NOMAD API dashboard \u00b6 Rich documentation : It offers comprehensive descriptions of all available resources, relevant endpoints, and possible operations. Each endpoint is documented with its available methods (GET, POST, DELETE, etc.), required parameters, request bodies, and example responses. Schemas : For each API endpoint, the dashboard provides detailed schema information, helping to understand the structure of requests and responses. This is particularly useful for making correct API calls and interpreting the responses. Testing in browser : Based on OpenAPI, the dashboard provides an interactive experience where you can explore all API endpoints. The 'Try it out' feature allows you to test API calls directly in the browser. You can modify the request parameters and see real-time responses from the NOMAD backend. This feature is very useful for testing, debugging, and learning how different API calls work. The API section of the NOMAD documentation provides further information.","title":"NOMAD API Helpers"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_0_nomad_api_helpers.html#nomad-api-helpers","text":"NOMAD offers several API helpers to facilitate data retrieval, processing, and interaction. These tools give you flexible and powerful access to NOMAD's data resources and functionalities. Almost everything you see in the GUI can be accessed via the API.","title":"NOMAD API Helpers"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_0_nomad_api_helpers.html#api-button-on-nomad-pages","text":"When exploring data in NOMAD, each page in the GUI includes an API button, ( API ), or it's symbol, ( <> ). This shows the GET request used to retrieve the data for that page. Often, there is an additional POST request available, for instance: Explore Page contains a POST request to the https://nomad-lab.eu/prod/v1/api/v1/entries/query endpoint, including the chosen filters. Entry Pages : Include a POST request to the https://nomad-lab.eu/prod/v1/api/v1/entries/{entry_id}/archive/query endoint, which allows fetching archive data (processed data) of that entry.","title":"API button on NOMAD pages"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_0_nomad_api_helpers.html#nomad-api-dashboard","text":"The NOMAD API dashboard is a powerful tool for accessing and experimenting with NOMAD's data resources. It provides detailed documentation and interactive features that make it possible to learn and use the API effectively. Whether you are developing applications, automating data upload or retrieval, or just exploring data, the API dashboard is an important resource.","title":"NOMAD API dashboard"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_0_nomad_api_helpers.html#accessing-the-api-dashboard","text":"The API dashboard can be accessed from the 'ANALYZE' menu under NOMAD APIs.","title":"Accessing the API dashboard"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_0_nomad_api_helpers.html#features-of-the-nomad-api-dashboard","text":"Rich documentation : It offers comprehensive descriptions of all available resources, relevant endpoints, and possible operations. Each endpoint is documented with its available methods (GET, POST, DELETE, etc.), required parameters, request bodies, and example responses. Schemas : For each API endpoint, the dashboard provides detailed schema information, helping to understand the structure of requests and responses. This is particularly useful for making correct API calls and interpreting the responses. Testing in browser : Based on OpenAPI, the dashboard provides an interactive experience where you can explore all API endpoints. The 'Try it out' feature allows you to test API calls directly in the browser. You can modify the request parameters and see real-time responses from the NOMAD backend. This feature is very useful for testing, debugging, and learning how different API calls work. The API section of the NOMAD documentation provides further information.","title":"Features of the NOMAD API dashboard"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_1_helper_explore.html","text":"Using the API helper in the Explore Page \u00b6 In this section, you will learn how to: Find the exact POST request, after you applied filters in explore page. Inspect the response to find your requested piece of data. Download the API response using the GUI. Let's consider our first basic example . Suppose we want to find the entry_id of entries that contain both Ti and O in their chemical formula, this time using the NOMAD GUI. Finding the POST Request \u00b6 Open NOMAD, from the EXPLORE menu select Entries to go to the explore page . On the left sidebar under FILTERS , click on Elements / Formula and select 'Ti' and 'O' in the periodic table that opens as a submenu. On top of the periodic table you can see how many entries match your search criteria. Locate the API symbol ( <> ) at the top of the page and click on it. 1 A new page opens showing a query independent of the actual request. Scroll down, under Request , you will find the related POST request to the entries/query endpoint. This request body, similar to the request_data in [our first example] , is a JSON object containing owner , query , aggregations , pagination , and required parts. The request body should look like this: { \"owner\" : \"visible\" , \"query\" : { \"results.material.elements:all\" : [ \"Ti\" , \"O\" ] }, \"aggregations\" : { ... }, \"pagination\" : { \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" , \"page_size\" : 20 }, \"required\" : { \"exclude\" : [ \"quantities\" , \"sections\" , \"files\" ] } } The owner is set to visible , meaning all entries visible to you. The query includes the filters applied in the GUI (materials containing Ti and O). The aggregations define how data is displayed, e.g., histograms. The required section excludes specific data fields ( quantities , sections , and files ) Inspecting the Response \u00b6 Scroll down further to reach Response . The response body in the GUI in the compact view looks like this: The data section contains our requested data. Once expanded, you see entries listed from 0 to 19 (since page_size is 20). Once you expand each of these entries, you can find its entry_id close to the bottom of the expanded list. Downloading the Response Using the GUI \u00b6 To get the response data in JSON format, use the API Dashboard: Copy the POST request body to your clipboard. Navigate to the API Dashboard, find the entries/query endpoint under entries/metadata . Click on Try it out button, replace the existing body with your copied request body, and click Execute . If you want faster results, reduce the page_size to a smaller value, e.g., 1. After execution, scroll down and you will see a successful response (code 200). Look for the entry_id under data in the response. You can download this response as a JSON file using the Download button at the bottom-right of the response window. Notice, the response data we received contains various other information in addition to our desired one, the entry_id . It actually contains all the information that the entries/query endpoint provides, excluding quantities , sections , and files . To retrieve only the entry_id , modify the required part to include only entry_id (in a python list), set page_size to 1, and remove aggregations . Your request body should look like this: { \"owner\" : \"visible\" , \"query\" : { \"results.material.elements:all\" : [ \"Ti\" , \"O\" ] }, \"pagination\" : { \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" , \"page_size\" : 1 }, \"required\" : { \"include\" : [ \"entry_id\" ] } } Executing this will give a response similar to: { \"owner\" : \"visible\" , \"query\" : { \"results.material.elements:all\" : [ \"Ti\" , \"O\" ] }, \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" , \"total\" : 41544 , \"next_page_after_value\" : \"1717097142713:zIaQ0vsZ0bx4tE8n6L7pDK7SyM-k\" }, \"required\" : { \"include\" : [ \"entry_id\" , \"upload_create_time\" ] }, \"data\" : [ { \"upload_create_time\" : \"2024-05-30T19:25:42.713000+00:00\" , \"entry_id\" : \"zIaQ0vsZ0bx4tE8n6L7pDK7SyM-k\" } ] } You can download this JSON object and use it programmatically to extract the entry_id . Remember, the API dashboard is a helper tool, not the most efficient or fastest way to use the API. It helps you understand the correct request body and the relevant endpoint . To explore available information from this endpoint (or other endpoints), we can remove the required field completely, or set it to \"required\": {} , and use \"page_size\": 1 to obtain all possible information from the endpoint. The placement of the API symbol may differ slightly from the animated screenshot provided, as it is currently located on the top right of the explore page in the updated GUI. \u21a9","title":"In Explore Page"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_1_helper_explore.html#using-the-api-helper-in-the-explore-page","text":"In this section, you will learn how to: Find the exact POST request, after you applied filters in explore page. Inspect the response to find your requested piece of data. Download the API response using the GUI. Let's consider our first basic example . Suppose we want to find the entry_id of entries that contain both Ti and O in their chemical formula, this time using the NOMAD GUI.","title":"Using the API helper in the Explore Page"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_1_helper_explore.html#finding-the-post-request","text":"Open NOMAD, from the EXPLORE menu select Entries to go to the explore page . On the left sidebar under FILTERS , click on Elements / Formula and select 'Ti' and 'O' in the periodic table that opens as a submenu. On top of the periodic table you can see how many entries match your search criteria. Locate the API symbol ( <> ) at the top of the page and click on it. 1 A new page opens showing a query independent of the actual request. Scroll down, under Request , you will find the related POST request to the entries/query endpoint. This request body, similar to the request_data in [our first example] , is a JSON object containing owner , query , aggregations , pagination , and required parts. The request body should look like this: { \"owner\" : \"visible\" , \"query\" : { \"results.material.elements:all\" : [ \"Ti\" , \"O\" ] }, \"aggregations\" : { ... }, \"pagination\" : { \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" , \"page_size\" : 20 }, \"required\" : { \"exclude\" : [ \"quantities\" , \"sections\" , \"files\" ] } } The owner is set to visible , meaning all entries visible to you. The query includes the filters applied in the GUI (materials containing Ti and O). The aggregations define how data is displayed, e.g., histograms. The required section excludes specific data fields ( quantities , sections , and files )","title":"Finding the POST Request"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_1_helper_explore.html#inspecting-the-response","text":"Scroll down further to reach Response . The response body in the GUI in the compact view looks like this: The data section contains our requested data. Once expanded, you see entries listed from 0 to 19 (since page_size is 20). Once you expand each of these entries, you can find its entry_id close to the bottom of the expanded list.","title":"Inspecting the Response"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_1_helper_explore.html#downloading-the-response-using-the-gui","text":"To get the response data in JSON format, use the API Dashboard: Copy the POST request body to your clipboard. Navigate to the API Dashboard, find the entries/query endpoint under entries/metadata . Click on Try it out button, replace the existing body with your copied request body, and click Execute . If you want faster results, reduce the page_size to a smaller value, e.g., 1. After execution, scroll down and you will see a successful response (code 200). Look for the entry_id under data in the response. You can download this response as a JSON file using the Download button at the bottom-right of the response window. Notice, the response data we received contains various other information in addition to our desired one, the entry_id . It actually contains all the information that the entries/query endpoint provides, excluding quantities , sections , and files . To retrieve only the entry_id , modify the required part to include only entry_id (in a python list), set page_size to 1, and remove aggregations . Your request body should look like this: { \"owner\" : \"visible\" , \"query\" : { \"results.material.elements:all\" : [ \"Ti\" , \"O\" ] }, \"pagination\" : { \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" , \"page_size\" : 1 }, \"required\" : { \"include\" : [ \"entry_id\" ] } } Executing this will give a response similar to: { \"owner\" : \"visible\" , \"query\" : { \"results.material.elements:all\" : [ \"Ti\" , \"O\" ] }, \"pagination\" : { \"page_size\" : 1 , \"order_by\" : \"upload_create_time\" , \"order\" : \"desc\" , \"total\" : 41544 , \"next_page_after_value\" : \"1717097142713:zIaQ0vsZ0bx4tE8n6L7pDK7SyM-k\" }, \"required\" : { \"include\" : [ \"entry_id\" , \"upload_create_time\" ] }, \"data\" : [ { \"upload_create_time\" : \"2024-05-30T19:25:42.713000+00:00\" , \"entry_id\" : \"zIaQ0vsZ0bx4tE8n6L7pDK7SyM-k\" } ] } You can download this JSON object and use it programmatically to extract the entry_id . Remember, the API dashboard is a helper tool, not the most efficient or fastest way to use the API. It helps you understand the correct request body and the relevant endpoint . To explore available information from this endpoint (or other endpoints), we can remove the required field completely, or set it to \"required\": {} , and use \"page_size\": 1 to obtain all possible information from the endpoint. The placement of the API symbol may differ slightly from the animated screenshot provided, as it is currently located on the top right of the explore page in the updated GUI. \u21a9","title":"Downloading the Response Using the GUI"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_2_helper_entry_page.html","text":"Using the API helper on Entry Pages \u00b6 In our previous example , you learned how to locate and download the entry_id of multiple entries using the API helper in the Explore page. In this example, you will learn how to: - Use the API Helper on entry pages to find specific data paths and access referenced data. - Navigate through NOMAD\u2019s data structure to locate desired fields such as density of states (dos). - Construct API queries for retrieving specific fields from NOMAD's archive data. - Execute API queries using the API Dashboard. Let's consider a more specific scenario: suppose our task is to access the density of states (dos) of a single entry that contain both Ti and O, and simultaneously include calculations for both dos and electronic band structure. Follow the steps below to achieve this: Step 1: Using the GUI to apply filters \u00b6 Open the NOMAD GUI and navigate to the Explore page. On the left sidebar under FILTERS , Material , click Elements / Formula and select Ti and O from the periodic table. On the left sidebar under FILTERS , Properties , Electronic select both Density of states and Band structure . The GUI will now display materials that contain both Ti and O, and have the selected electronic properties, namely dos and electronic band structure. Step 2: Find the relevant endpoing from API helper. \u00b6 Now we should go to an entry page. Let's choose the first entry that appears in the filtered results. The principle is similar for all the entries, but if you would prefer to look at the same entry of this example, you can access the entry with the entry_id \"kAarJxUt4bGajfLZFyqEPnxRK59_\". To do this, you can copy entry_id = kAarJxUt4bGajfLZFyqEPnxRK59_ and paste it in the search bar in the Explore page. On the entry page, locate and click the API button at the bottom of the sidebar. A new page opens. Scroll down to locate the POST request. Once you found the POST request, let's have a closer look to the URL in front of it: \"https://nomad-lab.eu/prod/v1/api/v1/entries/kAarJxUt4bGajfLZFyqEPnxRK59_/archive/query\". You can see that there is a key which looks like an entry_id (we know in our case it is \"kAarJxUt4bGajfLZFyqEPnxRK59_\"). Also, you can distinguish the API endpoint entries/{entry_id}/archive/query implying that this endpoint allows you to query the archive data, i.e., processed data , given you have its entry_id . Great, we achieved the objective of this step, which was finding the relevant endpoint. For our next step, we would need the entry_id of this entry, so let's copy it to the clipboard. Step 3: Using the API Dashboard \u00b6 Open the API dashboard in a new browser tab. Navigate to the endpoint entries/{entry_id}/archive/query and expand it. The description says \"Returns a partial archive for the given entry_id based on the required specified in the body.\" It seems like what we are interested in. It just needs an entry_id and required part. There is no need for a pagination , owner etc. Click on 'Try it out'. The entry_id can be copied from entry page or the URL of the POST request. In our example it is \"kAarJxUt4bGajfLZFyqEPnxRK59_\". In order to write the required part, we should first locate the desired data. Step 4: Locating the desired data \u00b6 Since we are only interested in the density of states (dos), we need to specify this in the required field. Intuitively, we look for the dos data in the data section of the response: Go back to the API Helper on the entry page. Scroll down to see the RESPONSE under the POST request. Expand the 'data' section. Expand the 'archive' section. Expand the 'results' section. Expand the 'properties' section. Expand the 'electronic' section. Expand the 'dos_electronic' section. Expand the '0' section. Here you will find \"energies\", \"total\", \"spin_polarized\", \"energy_fermi\", and \"band_gap\". If you are looking at a different entry, you might see other sections. However, it does not matter. The key point is that we are able to locate the desired data that we would like to access via API. Note the path to \"energies\", which is /run/0/calculation/0/dos_electronic/0/energies . The data of interest are not here, but are actually referenced here with their path . Now let's quickly verify if the dos data are in /run/0/calculation/0/dos_electronic/0/energies . Go back to the Response under the POST request in the API Helper of the entry page. Expand the 'data' section. Expand the 'archive' section. Expand the 'run' section. Expand the first element ( 0 ) of the 'run' section. Expand the 'calculation' section. Expand the first element ( 0 ) of the 'calculation' section. Expand the 'dos_electronic' section. Expand the first element ( 0 ) of the 'dos_electronic' section. Expand the 'energies' section to verify that the actual data is present. Step 5: Constructing the required and executing the query \u00b6 Switch to the API dashboard and under the endpoint entries/{entry_id}/archive/query click on Try it out . Paste the entry_id to its place, if you havn't done it yet. Remember, the path to the data is /run/0/calculation/0/dos_electronic/0/energies . We need to construct the required field to reflect this path to be able to retrieve dos data. This is how it is done: { \"required\" : { \"run[0]\" : { \"calculation[0]\" : { \"dos_electronic[0]\" : { \"energies\" : \"*\" } } } } } This structure tells the NOMAD backend to go to the first element of the run section (consider run as a python list, then its first element will be \"run[0]\" ). From there, go to the first element of the calculation section ( \"calculation[0]\" ). Then go to the first element of the dos_electronic section ( \"dos_electronic[0]\" ). Finally, retrieve everything in the energies section ( \"energies\": \"*\" ). After clicking on Execute , the response is back via code 200 and can be downloaded as a JSON file. Well done! You could retrieve data from data containers (sections) deep in NOMAD data architecture. Formatting JSON When writing in JSON format (e.g., writing the required part), indentation is not essential. One could write the same required as following: { \"required\" : { \"run[0]\" : { \"calculation[0]\" : { \"dos_electronic[0]\" : { \"energies\" : \"*\" }}}}} Indentation just helps to easily identify the structure and hierarchy of the JSON data. A common practice is to use two or four spaces for each level of indentation you can write it. You could write everything in 1 line, or alternatively get your JSON formatted using one of several available online tools, e.g., this one . These tools often allow checking the correctness of JSON format, e.g., missing curled brackets etc. How do you retrieve dos data for 10 entries? What would you do if you want to do the same procedure for 10 entries programmatically? Hint The API dashboard also shows the curl request that can be sent to retrieve this data. You already learned how to use API via the curl here You have also learned how to extract the entry_id of the entries which match your search criteria in a former example","title":"In Entry Page"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_2_helper_entry_page.html#using-the-api-helper-on-entry-pages","text":"In our previous example , you learned how to locate and download the entry_id of multiple entries using the API helper in the Explore page. In this example, you will learn how to: - Use the API Helper on entry pages to find specific data paths and access referenced data. - Navigate through NOMAD\u2019s data structure to locate desired fields such as density of states (dos). - Construct API queries for retrieving specific fields from NOMAD's archive data. - Execute API queries using the API Dashboard. Let's consider a more specific scenario: suppose our task is to access the density of states (dos) of a single entry that contain both Ti and O, and simultaneously include calculations for both dos and electronic band structure. Follow the steps below to achieve this:","title":"Using the API helper on Entry Pages"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_2_helper_entry_page.html#step-1-using-the-gui-to-apply-filters","text":"Open the NOMAD GUI and navigate to the Explore page. On the left sidebar under FILTERS , Material , click Elements / Formula and select Ti and O from the periodic table. On the left sidebar under FILTERS , Properties , Electronic select both Density of states and Band structure . The GUI will now display materials that contain both Ti and O, and have the selected electronic properties, namely dos and electronic band structure.","title":"Step 1: Using the GUI to apply filters"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_2_helper_entry_page.html#step-2-find-the-relevant-endpoing-from-api-helper","text":"Now we should go to an entry page. Let's choose the first entry that appears in the filtered results. The principle is similar for all the entries, but if you would prefer to look at the same entry of this example, you can access the entry with the entry_id \"kAarJxUt4bGajfLZFyqEPnxRK59_\". To do this, you can copy entry_id = kAarJxUt4bGajfLZFyqEPnxRK59_ and paste it in the search bar in the Explore page. On the entry page, locate and click the API button at the bottom of the sidebar. A new page opens. Scroll down to locate the POST request. Once you found the POST request, let's have a closer look to the URL in front of it: \"https://nomad-lab.eu/prod/v1/api/v1/entries/kAarJxUt4bGajfLZFyqEPnxRK59_/archive/query\". You can see that there is a key which looks like an entry_id (we know in our case it is \"kAarJxUt4bGajfLZFyqEPnxRK59_\"). Also, you can distinguish the API endpoint entries/{entry_id}/archive/query implying that this endpoint allows you to query the archive data, i.e., processed data , given you have its entry_id . Great, we achieved the objective of this step, which was finding the relevant endpoint. For our next step, we would need the entry_id of this entry, so let's copy it to the clipboard.","title":"Step 2: Find the relevant endpoing from API helper."},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_2_helper_entry_page.html#step-3-using-the-api-dashboard","text":"Open the API dashboard in a new browser tab. Navigate to the endpoint entries/{entry_id}/archive/query and expand it. The description says \"Returns a partial archive for the given entry_id based on the required specified in the body.\" It seems like what we are interested in. It just needs an entry_id and required part. There is no need for a pagination , owner etc. Click on 'Try it out'. The entry_id can be copied from entry page or the URL of the POST request. In our example it is \"kAarJxUt4bGajfLZFyqEPnxRK59_\". In order to write the required part, we should first locate the desired data.","title":"Step 3: Using the API Dashboard"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_2_helper_entry_page.html#step-4-locating-the-desired-data","text":"Since we are only interested in the density of states (dos), we need to specify this in the required field. Intuitively, we look for the dos data in the data section of the response: Go back to the API Helper on the entry page. Scroll down to see the RESPONSE under the POST request. Expand the 'data' section. Expand the 'archive' section. Expand the 'results' section. Expand the 'properties' section. Expand the 'electronic' section. Expand the 'dos_electronic' section. Expand the '0' section. Here you will find \"energies\", \"total\", \"spin_polarized\", \"energy_fermi\", and \"band_gap\". If you are looking at a different entry, you might see other sections. However, it does not matter. The key point is that we are able to locate the desired data that we would like to access via API. Note the path to \"energies\", which is /run/0/calculation/0/dos_electronic/0/energies . The data of interest are not here, but are actually referenced here with their path . Now let's quickly verify if the dos data are in /run/0/calculation/0/dos_electronic/0/energies . Go back to the Response under the POST request in the API Helper of the entry page. Expand the 'data' section. Expand the 'archive' section. Expand the 'run' section. Expand the first element ( 0 ) of the 'run' section. Expand the 'calculation' section. Expand the first element ( 0 ) of the 'calculation' section. Expand the 'dos_electronic' section. Expand the first element ( 0 ) of the 'dos_electronic' section. Expand the 'energies' section to verify that the actual data is present.","title":"Step 4: Locating the desired data"},{"location":"Module4/M4_2_retrieve_data/M4_2_3_api_helpers/M4_2_3_2_helper_entry_page.html#step-5-constructing-the-required-and-executing-the-query","text":"Switch to the API dashboard and under the endpoint entries/{entry_id}/archive/query click on Try it out . Paste the entry_id to its place, if you havn't done it yet. Remember, the path to the data is /run/0/calculation/0/dos_electronic/0/energies . We need to construct the required field to reflect this path to be able to retrieve dos data. This is how it is done: { \"required\" : { \"run[0]\" : { \"calculation[0]\" : { \"dos_electronic[0]\" : { \"energies\" : \"*\" } } } } } This structure tells the NOMAD backend to go to the first element of the run section (consider run as a python list, then its first element will be \"run[0]\" ). From there, go to the first element of the calculation section ( \"calculation[0]\" ). Then go to the first element of the dos_electronic section ( \"dos_electronic[0]\" ). Finally, retrieve everything in the energies section ( \"energies\": \"*\" ). After clicking on Execute , the response is back via code 200 and can be downloaded as a JSON file. Well done! You could retrieve data from data containers (sections) deep in NOMAD data architecture. Formatting JSON When writing in JSON format (e.g., writing the required part), indentation is not essential. One could write the same required as following: { \"required\" : { \"run[0]\" : { \"calculation[0]\" : { \"dos_electronic[0]\" : { \"energies\" : \"*\" }}}}} Indentation just helps to easily identify the structure and hierarchy of the JSON data. A common practice is to use two or four spaces for each level of indentation you can write it. You could write everything in 1 line, or alternatively get your JSON formatted using one of several available online tools, e.g., this one . These tools often allow checking the correctness of JSON format, e.g., missing curled brackets etc. How do you retrieve dos data for 10 entries? What would you do if you want to do the same procedure for 10 entries programmatically? Hint The API dashboard also shows the curl request that can be sent to retrieve this data. You already learned how to use API via the curl here You have also learned how to extract the entry_id of the entries which match your search criteria in a former example","title":"Step 5: Constructing the required and executing the query"},{"location":"Module4/M4_3_upload_data/M4_3_1_authentication.html","text":"Authentication and Checking Uploads \u00b6 Authentication \u00b6 Most of the API operations with NOMAD can be freely used without needing a login or credentials. However, to upload, edit, or view your own data or those shared with you, the API needs to authenticate you. The NOMAD API uses OAuth and tokens to authenticate users. This guide will walk you through the steps of getting access tokens, using the NOMAD Python package for authentication, and managing app tokens. What is OAuth? OAuth is an open standard for access delegation commonly used as a way to grant websites or applications limited access to user information without exposing passwords. It allows third-party services to exchange information securely and efficiently. What are Tokens? In general, tokens are pieces of data that are used to authorize access to an API. In the NOMAD API, tokens are also used to authenticate users. There are two main types of tokens: 1. Access Tokens : Short-lived tokens used for API requests. 2. App Tokens : Tokens with a user-defined expiration time, used for longer sessions. Depending on what you want to do (e.g., uploading data) you may need to authenticate yourself using your username and password in the following use cases: - Using the API dashboard - Interacting with NOMAD API using Python. - Working with the NOMAD Python package. To use authentication in the API dashboard , simply use the Authorize button When interacting with NOMAD using Python you need access tokens. You can obtain access tokens by sending a GET request to the NOMAD authentication endpoint (/auth/token', see the API dashboard ). In the following example, we first obtain our access token from the /auth/token endpoint, and use it later as a header parameter when sending other requests. Simply replace 'myname' and 'mypassword' with your actual username and password and run it. import requests import json response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/auth/token' , params = dict ( username = 'myname' , password = 'mypassword' ) ) token = response . json ()[ 'access_token' ] How to obtain ppp tokens in NOMAD If the short-term expiration of the default access token (obtained from the /auth/token endpoint) does not suit your needs, you can request an app token with a user-defined expiration from the /auth/app_token endpoint: response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/auth/app_token?expires_in=86400' , headers = { 'Authorization' : f 'Bearer { token } ' }) app_token = response . json ()[ 'access_token' ] If you have the NOMAD Python package installed, you can use its Auth implementation to directly authenticate yourself in the requests you send. Here is an example on how the username and password is given, when using the Auth e.g., to to see the user uploads: import requests from nomad.client import Auth response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/uploads' , auth = Auth ( user = 'myname or email' , password = 'mypassword' ) ) uploads = response . json ()[ 'data' ] Checking Uploads \u00b6 Following example shows how you can check your uploads in NOMAD. Send a GET request to the /auth/token endpoint and save your access token (see above). Send a GET request to the /uploads endpoint contains your 'uploads' information. Let's print them! import requests import json response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/auth/token' , params = dict ( username = 'my_username' , password = 'my_password' )) token = response . json ()[ 'access_token' ] response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/uploads' , headers = { 'Authorization' : f 'Bearer { token } ' } ) uploads = response . json ()[ 'data' ] print ( json . dumps ( uploads , indent = 4 )) The output of the above snippet should look like the following. It is a Python list with several elements (in this example only 1). Each element is a JSON (or Python dict) containing information about each of your uploads. [ { \"process_running\" : false , \"current_process\" : \"process_upload\" , \"process_status\" : \"SUCCESS\" , \"last_status_message\" : \"Process process_upload completed successfully\" , \"errors\" : [], \"warnings\" : [], \"complete_time\" : \"2024-07-16T13:57:53.459000\" , \"upload_id\" : \"VWGLXmy4S2-cdeteN2Xxaw\" , \"upload_name\" : \"FAIRmat_logo.png\" , \"upload_create_time\" : \"2024-07-16T13:57:53.308000\" , \"main_author\" : \"ebb26223-0cec-4d81-98f5-3b25db945b54\" , \"coauthors\" : [], \"coauthor_groups\" : [], \"reviewers\" : [], \"reviewer_groups\" : [], \"writers\" : [ \"ebb26223-0cec-4d81-98f5-3b25db945b54\" ], \"writer_groups\" : [], \"viewers\" : [ \"ebb26223-0cec-4d81-98f5-3b25db945b54\" ], \"viewer_groups\" : [], \"published\" : false , \"published_to\" : [], \"with_embargo\" : false , \"embargo_length\" : 0 , \"license\" : \"CC BY 4.0\" , \"entries\" : 0 , \"upload_files_server_path\" : \"/nomad/prod/fs/staging/V/VWGLXmy4S2-cdeteN2Xxaw\" } ] Next we will explore how we can use other API endpoints to create datasets and upload files.","title":"Authentication"},{"location":"Module4/M4_3_upload_data/M4_3_1_authentication.html#authentication-and-checking-uploads","text":"","title":"Authentication and Checking Uploads"},{"location":"Module4/M4_3_upload_data/M4_3_1_authentication.html#authentication","text":"Most of the API operations with NOMAD can be freely used without needing a login or credentials. However, to upload, edit, or view your own data or those shared with you, the API needs to authenticate you. The NOMAD API uses OAuth and tokens to authenticate users. This guide will walk you through the steps of getting access tokens, using the NOMAD Python package for authentication, and managing app tokens. What is OAuth? OAuth is an open standard for access delegation commonly used as a way to grant websites or applications limited access to user information without exposing passwords. It allows third-party services to exchange information securely and efficiently. What are Tokens? In general, tokens are pieces of data that are used to authorize access to an API. In the NOMAD API, tokens are also used to authenticate users. There are two main types of tokens: 1. Access Tokens : Short-lived tokens used for API requests. 2. App Tokens : Tokens with a user-defined expiration time, used for longer sessions. Depending on what you want to do (e.g., uploading data) you may need to authenticate yourself using your username and password in the following use cases: - Using the API dashboard - Interacting with NOMAD API using Python. - Working with the NOMAD Python package. To use authentication in the API dashboard , simply use the Authorize button When interacting with NOMAD using Python you need access tokens. You can obtain access tokens by sending a GET request to the NOMAD authentication endpoint (/auth/token', see the API dashboard ). In the following example, we first obtain our access token from the /auth/token endpoint, and use it later as a header parameter when sending other requests. Simply replace 'myname' and 'mypassword' with your actual username and password and run it. import requests import json response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/auth/token' , params = dict ( username = 'myname' , password = 'mypassword' ) ) token = response . json ()[ 'access_token' ] How to obtain ppp tokens in NOMAD If the short-term expiration of the default access token (obtained from the /auth/token endpoint) does not suit your needs, you can request an app token with a user-defined expiration from the /auth/app_token endpoint: response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/auth/app_token?expires_in=86400' , headers = { 'Authorization' : f 'Bearer { token } ' }) app_token = response . json ()[ 'access_token' ] If you have the NOMAD Python package installed, you can use its Auth implementation to directly authenticate yourself in the requests you send. Here is an example on how the username and password is given, when using the Auth e.g., to to see the user uploads: import requests from nomad.client import Auth response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/uploads' , auth = Auth ( user = 'myname or email' , password = 'mypassword' ) ) uploads = response . json ()[ 'data' ]","title":"Authentication"},{"location":"Module4/M4_3_upload_data/M4_3_1_authentication.html#checking-uploads","text":"Following example shows how you can check your uploads in NOMAD. Send a GET request to the /auth/token endpoint and save your access token (see above). Send a GET request to the /uploads endpoint contains your 'uploads' information. Let's print them! import requests import json response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/auth/token' , params = dict ( username = 'my_username' , password = 'my_password' )) token = response . json ()[ 'access_token' ] response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/uploads' , headers = { 'Authorization' : f 'Bearer { token } ' } ) uploads = response . json ()[ 'data' ] print ( json . dumps ( uploads , indent = 4 )) The output of the above snippet should look like the following. It is a Python list with several elements (in this example only 1). Each element is a JSON (or Python dict) containing information about each of your uploads. [ { \"process_running\" : false , \"current_process\" : \"process_upload\" , \"process_status\" : \"SUCCESS\" , \"last_status_message\" : \"Process process_upload completed successfully\" , \"errors\" : [], \"warnings\" : [], \"complete_time\" : \"2024-07-16T13:57:53.459000\" , \"upload_id\" : \"VWGLXmy4S2-cdeteN2Xxaw\" , \"upload_name\" : \"FAIRmat_logo.png\" , \"upload_create_time\" : \"2024-07-16T13:57:53.308000\" , \"main_author\" : \"ebb26223-0cec-4d81-98f5-3b25db945b54\" , \"coauthors\" : [], \"coauthor_groups\" : [], \"reviewers\" : [], \"reviewer_groups\" : [], \"writers\" : [ \"ebb26223-0cec-4d81-98f5-3b25db945b54\" ], \"writer_groups\" : [], \"viewers\" : [ \"ebb26223-0cec-4d81-98f5-3b25db945b54\" ], \"viewer_groups\" : [], \"published\" : false , \"published_to\" : [], \"with_embargo\" : false , \"embargo_length\" : 0 , \"license\" : \"CC BY 4.0\" , \"entries\" : 0 , \"upload_files_server_path\" : \"/nomad/prod/fs/staging/V/VWGLXmy4S2-cdeteN2Xxaw\" } ] Next we will explore how we can use other API endpoints to create datasets and upload files.","title":"Checking Uploads"},{"location":"Module4/M4_3_upload_data/M4_3_2_upload.html","text":"Creating Datasets and Uploading Files \u00b6 Creating Datasets \u00b6 Following example shows how you can create a dataset using NOMAD's API. 1. Send a GET request to the /auth/token endpoint to obtain your access token (replace 'my_username' and 'my_password' with your actual username and password). This step is one to one repeated from the last page . import requests import json response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/auth/token' , params = dict ( username = 'my_username' , password = 'my_password' )) token = response . json ()[ 'access_token' ] 2. Once you have your access_token , send a POST request to the /datasets/ endpoint, include your token in the header. Also include the dataset name (replace 'replace_the_dataset_name_here' with a desired name.) base_url = 'https://nomad-lab.eu/prod/v1/api/v1' endpoint = '/datasets/' my_dataset = 'replace_the_dataset_name_here' response = requests . post ( base_url + endpoint , headers = { 'Authorization' : f 'Bearer { token } ' , 'Accept' : 'application/json' }, json = { \"dataset_name\" : my_dataset } ) The dataset is now created. Let's print the response we received which contains information about the dataset we just created in a readable manner. print ( json . dumps ( response . json (), indent = 4 )) The print should look something like the following: { \"dataset_id\" : \"3uMiCF_4TEqhaoA_I3X1Yw\" , \"data\" : { \"dataset_id\" : \"3uMiCF_4TEqhaoA_I3X1Yw\" , \"dataset_name\" : \"my_dataset\" , \"user_id\" : \"ebb26223-0cec-4d81-98f5-3b25db945b54\" , \"dataset_create_time\" : \"2024-07-16T15:16:30.783936+00:00\" , \"dataset_modified_time\" : \"2024-07-16T15:16:30.783936+00:00\" , \"dataset_type\" : \"owned\" } } Uploading Files \u00b6 Following example shows how you can upload a file (here the FAIRmat_logo.png) using NOMAD's API. Get your access token from the /auth/token endpoint and save it in token (If you did already, you don't have to repeat it!). Once you have your token , you need to send a POST request to the /uploads endpoint and provide a file name. To simplyfiy the process, place the file you plan to upload in the directory that you are running the code. Here, opening the file in binary read mode ('rb') is essential when you need to upload or process binary files such as images, zip files, or any other non-text files. base_url = 'https://nomad-lab.eu/prod/v1/api/v1' endpoint = '/uploads' file_name = 'FAIRmat_logo.png' with open ( file_name , 'rb' ) as f : response = requests . post ( base_url + endpoint , headers = { 'Authorization' : f 'Bearer { token } ' , 'Accept' : 'application/json' }, files = { 'file' : ( file_name , f )} ) You can now check the response, e.g., print it to see if the file has been uploaded successfully: print ( json . dumps ( response . json (), indent = 4 )) Congratulations! You have learned the principles of working with the NOMAD API and can now leverage your programming skills to explore the NOMAD API in more details. NOMAD Documentation also provides further information and examples.","title":"Creating Datasets"},{"location":"Module4/M4_3_upload_data/M4_3_2_upload.html#creating-datasets-and-uploading-files","text":"","title":"Creating Datasets and Uploading Files"},{"location":"Module4/M4_3_upload_data/M4_3_2_upload.html#creating-datasets","text":"Following example shows how you can create a dataset using NOMAD's API. 1. Send a GET request to the /auth/token endpoint to obtain your access token (replace 'my_username' and 'my_password' with your actual username and password). This step is one to one repeated from the last page . import requests import json response = requests . get ( 'https://nomad-lab.eu/prod/v1/api/v1/auth/token' , params = dict ( username = 'my_username' , password = 'my_password' )) token = response . json ()[ 'access_token' ] 2. Once you have your access_token , send a POST request to the /datasets/ endpoint, include your token in the header. Also include the dataset name (replace 'replace_the_dataset_name_here' with a desired name.) base_url = 'https://nomad-lab.eu/prod/v1/api/v1' endpoint = '/datasets/' my_dataset = 'replace_the_dataset_name_here' response = requests . post ( base_url + endpoint , headers = { 'Authorization' : f 'Bearer { token } ' , 'Accept' : 'application/json' }, json = { \"dataset_name\" : my_dataset } ) The dataset is now created. Let's print the response we received which contains information about the dataset we just created in a readable manner. print ( json . dumps ( response . json (), indent = 4 )) The print should look something like the following: { \"dataset_id\" : \"3uMiCF_4TEqhaoA_I3X1Yw\" , \"data\" : { \"dataset_id\" : \"3uMiCF_4TEqhaoA_I3X1Yw\" , \"dataset_name\" : \"my_dataset\" , \"user_id\" : \"ebb26223-0cec-4d81-98f5-3b25db945b54\" , \"dataset_create_time\" : \"2024-07-16T15:16:30.783936+00:00\" , \"dataset_modified_time\" : \"2024-07-16T15:16:30.783936+00:00\" , \"dataset_type\" : \"owned\" } }","title":"Creating Datasets"},{"location":"Module4/M4_3_upload_data/M4_3_2_upload.html#uploading-files","text":"Following example shows how you can upload a file (here the FAIRmat_logo.png) using NOMAD's API. Get your access token from the /auth/token endpoint and save it in token (If you did already, you don't have to repeat it!). Once you have your token , you need to send a POST request to the /uploads endpoint and provide a file name. To simplyfiy the process, place the file you plan to upload in the directory that you are running the code. Here, opening the file in binary read mode ('rb') is essential when you need to upload or process binary files such as images, zip files, or any other non-text files. base_url = 'https://nomad-lab.eu/prod/v1/api/v1' endpoint = '/uploads' file_name = 'FAIRmat_logo.png' with open ( file_name , 'rb' ) as f : response = requests . post ( base_url + endpoint , headers = { 'Authorization' : f 'Bearer { token } ' , 'Accept' : 'application/json' }, files = { 'file' : ( file_name , f )} ) You can now check the response, e.g., print it to see if the file has been uploaded successfully: print ( json . dumps ( response . json (), indent = 4 )) Congratulations! You have learned the principles of working with the NOMAD API and can now leverage your programming skills to explore the NOMAD API in more details. NOMAD Documentation also provides further information and examples.","title":"Uploading Files"},{"location":"Module4/M4_x_examples/M4_x_0_overview.html","text":"placeholder for examples overview","title":"Overview"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_0_perovskites_crabnet_overview.html","text":"Overview \u00b6 In this section, we will work through a real scientific example to predict the bandgap of perovskite materials from their composition. Perovskites are a class of materials that have gained significant attention in recent years, particularly in the field of solar cells, due to their promising optoelectronic properties. Predicting the bandgap from composition allows researchers to efficiently screen and design new materials with desired properties. The Scientific Question \u00b6 The key question we aim to address is: How can we predict the bandgap of perovskite materials based on their chemical composition using machine learning (ML) and data on NOMAD? Steps to Address the Question: \u00b6 To answer this question, we will break down the process into three main steps: Retrieve Data from NOMAD via API : We will begin by querying the NOMAD database to retrieve data on perovskite materials. This data include information about the materials' chemical compositions and their corresponding bandgap values. We will save the data as a CSV file for further processing. Clean and Prepare Data for ML : We will clean the data by removing duplicates, handling missing values, and grouping data by unique chemical formulas. We will also explore the distribution of elements in the dataset to understand its diversity and potential biases, i.e., ensure our dataset quality. Build, Train, and Evaluate an ML Model : We will use CrabNet, a deep learning model designed for predicting material properties based on chemical composition, to predict the bandgap of perovskite materials. The model will be trained on the cleaned dataset and its performance evaluated using validation and test datasets. Finally, we will use the trained model to predict the bandgap for new materials. This workflow serves as a real-world example of using NOMAD data and ML for materials science research. By following this module, you\u2019ll learn how to integrate NOMAD's API with ML tools to tackle scientific challenges. This example can be considered a real scientific case study. The approach and methods we use here are applicable to actual research problems in materials science. By following through this module, you will not only gain practical experience with using NOMAD's API but also gain some idea of how large data which is retrieved from NOMAD can be used for machine learning and understand how these tools can be used to address significant scientific questions in the field.","title":"Overview"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_0_perovskites_crabnet_overview.html#overview","text":"In this section, we will work through a real scientific example to predict the bandgap of perovskite materials from their composition. Perovskites are a class of materials that have gained significant attention in recent years, particularly in the field of solar cells, due to their promising optoelectronic properties. Predicting the bandgap from composition allows researchers to efficiently screen and design new materials with desired properties.","title":"Overview"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_0_perovskites_crabnet_overview.html#the-scientific-question","text":"The key question we aim to address is: How can we predict the bandgap of perovskite materials based on their chemical composition using machine learning (ML) and data on NOMAD?","title":"The Scientific Question"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_0_perovskites_crabnet_overview.html#steps-to-address-the-question","text":"To answer this question, we will break down the process into three main steps: Retrieve Data from NOMAD via API : We will begin by querying the NOMAD database to retrieve data on perovskite materials. This data include information about the materials' chemical compositions and their corresponding bandgap values. We will save the data as a CSV file for further processing. Clean and Prepare Data for ML : We will clean the data by removing duplicates, handling missing values, and grouping data by unique chemical formulas. We will also explore the distribution of elements in the dataset to understand its diversity and potential biases, i.e., ensure our dataset quality. Build, Train, and Evaluate an ML Model : We will use CrabNet, a deep learning model designed for predicting material properties based on chemical composition, to predict the bandgap of perovskite materials. The model will be trained on the cleaned dataset and its performance evaluated using validation and test datasets. Finally, we will use the trained model to predict the bandgap for new materials. This workflow serves as a real-world example of using NOMAD data and ML for materials science research. By following this module, you\u2019ll learn how to integrate NOMAD's API with ML tools to tackle scientific challenges. This example can be considered a real scientific case study. The approach and methods we use here are applicable to actual research problems in materials science. By following through this module, you will not only gain practical experience with using NOMAD's API but also gain some idea of how large data which is retrieved from NOMAD can be used for machine learning and understand how these tools can be used to address significant scientific questions in the field.","title":"Steps to Address the Question:"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_1_perovskites_crabnet_retrieve_data_api.html","text":"Compositional Property Prediction of Perovskites with CrabNet \u00b6 This example demonstrates how to use CrabNet to predict the bandgap of perovskites using data from the Perovskite Database , available in NOMAD for training. Authored by Jos\u00e9 A. M\u00e1rquez et al. This example is based on the Jupyter notebook crabnet-perovskite-bandgap-prediction.ipynb and can be reproduced using the provided code snippets. Installing Needed Libraries \u00b6 We will start by running a couple of pip installers. Skip this part if you have the libraries installed in your environment. ! pip install torch ! pip install crabnet ! pip install pandas Retrieve Data using NOMAD API \u00b6 Skip the API Call Section if You Have the Data If a .csv file in the data folder already exists, you can skip the rest of the steps below and proceed directly to next page on data cleaning . However, make sure that you have installed the necessary libraries and imported them in your notebook before proceeding. import requests base_url = 'https://nomad-lab.eu/prod/v1/api/v1' def extract_values ( entry ): try : bandgaps . append ( entry [ 'results' ][ 'properties' ][ 'electronic' ][ 'band_structure_electronic' ][ 0 ][ 'band_gap' ][ 0 ][ 'value' ]) except : bandgaps . append ( 'None' ) try : reduced_formulas . append ( entry [ 'results' ][ 'material' ][ 'chemical_formula_reduced' ]) iupac_formulas . append ( entry [ 'results' ][ 'material' ][ 'chemical_formula_iupac' ]) descriptive_formulas . append ( entry [ 'results' ][ 'material' ][ 'chemical_formula_descriptive' ]) except : reduced_formulas . append ( 'None' ) iupac_formulas . append ( 'None' ) descriptive_formulas . append ( 'None' ) try : vocs . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'open_circuit_voltage' ]) except : vocs . append ( 'None' ) try : jscs . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'short_circuit_current_density' ]) except : jscs . append ( 'None' ) try : ffs . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'fill_factor' ]) except : ffs . append ( 'None' ) try : pces . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'efficiency' ]) except : pces . append ( 'None' ) try : device_stack . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'device_stack' ]) except : device_stack . append ( 'None' ) try : htl . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'hole_transport_layer' ]) except : htl . append ( 'None' ) try : etl . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'electron_transport_layer' ]) except : etl . append ( 'None' ) try : structural_dimensionality . append ( entry [ 'results' ][ 'material' ][ 'structural_type' ]) except : structural_dimensionality . append ( 'None' ) return bandgaps , reduced_formulas , iupac_formulas , descriptive_formulas , vocs , jscs , ffs , pces , device_stack , htl , etl , structural_dimensionality bandgaps = [] vocs = [] jscs = [] ffs = [] pces = [] reduced_formulas = [] iupac_formulas = [] descriptive_formulas = [] htl = [] etl = [] device_stack = [] structural_dimensionality = [] page_after_value = None while True : data = requests . post ( f ' { base_url } /entries/query' , json = { \"owner\" : \"visible\" , \"aggregations\" : {}, \"query\" : { \"and\" : [ { \"results.material.elements:all\" : [ \"Sn\" ]}, { \"sections:all\" : [ \"nomad.datamodel.results.SolarCell\" ]} ] }, 'required' : { 'results' : { 'material' : { 'chemical_formula_reduced' : '*' , 'structural_type' : '*' }, 'properties' : { 'optoelectronic' : { 'band_gap' : '*' , 'solar_cell' : { 'open_circuit_voltage' : '*' , 'short_circuit_current_density' : '*' , 'fill_factor' : '*' , 'efficiency' : '*' }}}, }, }, 'pagination' : { 'page_size' : 1000 , 'page_after_value' : page_after_value } }) . json () if 'next_page_after_value' not in data [ 'pagination' ] . keys (): for entry in data [ 'data' ]: if 'results' not in entry . keys (): continue elif 'chemical_formula_reduced' not in entry [ 'results' ][ 'material' ] . keys (): continue else : extract_values ( entry ) break page_after_value = data [ 'pagination' ][ 'next_page_after_value' ] for entry in data [ 'data' ]: if 'results' not in entry . keys (): continue elif 'chemical_formula_reduced' not in entry [ 'results' ][ 'material' ] . keys (): continue else : extract_values ( entry ) You can verify that the data was fetched correctly by printing some of the values: print ( bandgaps [ 0 ]) print ( len ( bandgaps )) Save Data \u00b6 Finally, we'll convert the data into a Pandas DataFrame and save it to a CSV file for future use. import pandas as pd import os # Convert data into a Pandas DataFrame df = pd . DataFrame ({ 'reduced_formulas' : reduced_formulas , 'iupac_formulas' : iupac_formulas , 'descriptive_formulas' : descriptive_formulas , 'bandgap' : bandgaps , 'voc' : vocs , 'jsc' : jscs , 'ff' : ffs , 'pce' : pces , 'device_stack' : device_stack , 'htl' : htl , 'etl' : etl , 'structural_dimensionality' : structural_dimensionality }) # Save to a CSV file if not os . path . exists ( 'data' ): os . makedirs ( 'data' ) df . to_csv ( 'data/perovskite_bandgap_devices.csv' , index = False ) # Convert 'bandgap' to numeric and apply unit conversion df [ 'bandgap' ] = pd . to_numeric ( df [ 'bandgap' ], errors = 'coerce' ) df [ 'bandgap' ] = df [ 'bandgap' ] * 6.24150974e18 # Display the DataFrame df . head () We have saved the the data we obtaine from NOMAD API with the name perovskite_bandgap_devices.csv into the folder data where your notebook is running. Next we will proceed with cleaning this data.","title":"Retrieve Data"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_1_perovskites_crabnet_retrieve_data_api.html#compositional-property-prediction-of-perovskites-with-crabnet","text":"This example demonstrates how to use CrabNet to predict the bandgap of perovskites using data from the Perovskite Database , available in NOMAD for training. Authored by Jos\u00e9 A. M\u00e1rquez et al. This example is based on the Jupyter notebook crabnet-perovskite-bandgap-prediction.ipynb and can be reproduced using the provided code snippets.","title":"Compositional Property Prediction of Perovskites with CrabNet"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_1_perovskites_crabnet_retrieve_data_api.html#installing-needed-libraries","text":"We will start by running a couple of pip installers. Skip this part if you have the libraries installed in your environment. ! pip install torch ! pip install crabnet ! pip install pandas","title":"Installing Needed Libraries"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_1_perovskites_crabnet_retrieve_data_api.html#retrieve-data-using-nomad-api","text":"Skip the API Call Section if You Have the Data If a .csv file in the data folder already exists, you can skip the rest of the steps below and proceed directly to next page on data cleaning . However, make sure that you have installed the necessary libraries and imported them in your notebook before proceeding. import requests base_url = 'https://nomad-lab.eu/prod/v1/api/v1' def extract_values ( entry ): try : bandgaps . append ( entry [ 'results' ][ 'properties' ][ 'electronic' ][ 'band_structure_electronic' ][ 0 ][ 'band_gap' ][ 0 ][ 'value' ]) except : bandgaps . append ( 'None' ) try : reduced_formulas . append ( entry [ 'results' ][ 'material' ][ 'chemical_formula_reduced' ]) iupac_formulas . append ( entry [ 'results' ][ 'material' ][ 'chemical_formula_iupac' ]) descriptive_formulas . append ( entry [ 'results' ][ 'material' ][ 'chemical_formula_descriptive' ]) except : reduced_formulas . append ( 'None' ) iupac_formulas . append ( 'None' ) descriptive_formulas . append ( 'None' ) try : vocs . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'open_circuit_voltage' ]) except : vocs . append ( 'None' ) try : jscs . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'short_circuit_current_density' ]) except : jscs . append ( 'None' ) try : ffs . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'fill_factor' ]) except : ffs . append ( 'None' ) try : pces . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'efficiency' ]) except : pces . append ( 'None' ) try : device_stack . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'device_stack' ]) except : device_stack . append ( 'None' ) try : htl . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'hole_transport_layer' ]) except : htl . append ( 'None' ) try : etl . append ( entry [ 'results' ][ 'properties' ][ 'optoelectronic' ][ 'solar_cell' ][ 'electron_transport_layer' ]) except : etl . append ( 'None' ) try : structural_dimensionality . append ( entry [ 'results' ][ 'material' ][ 'structural_type' ]) except : structural_dimensionality . append ( 'None' ) return bandgaps , reduced_formulas , iupac_formulas , descriptive_formulas , vocs , jscs , ffs , pces , device_stack , htl , etl , structural_dimensionality bandgaps = [] vocs = [] jscs = [] ffs = [] pces = [] reduced_formulas = [] iupac_formulas = [] descriptive_formulas = [] htl = [] etl = [] device_stack = [] structural_dimensionality = [] page_after_value = None while True : data = requests . post ( f ' { base_url } /entries/query' , json = { \"owner\" : \"visible\" , \"aggregations\" : {}, \"query\" : { \"and\" : [ { \"results.material.elements:all\" : [ \"Sn\" ]}, { \"sections:all\" : [ \"nomad.datamodel.results.SolarCell\" ]} ] }, 'required' : { 'results' : { 'material' : { 'chemical_formula_reduced' : '*' , 'structural_type' : '*' }, 'properties' : { 'optoelectronic' : { 'band_gap' : '*' , 'solar_cell' : { 'open_circuit_voltage' : '*' , 'short_circuit_current_density' : '*' , 'fill_factor' : '*' , 'efficiency' : '*' }}}, }, }, 'pagination' : { 'page_size' : 1000 , 'page_after_value' : page_after_value } }) . json () if 'next_page_after_value' not in data [ 'pagination' ] . keys (): for entry in data [ 'data' ]: if 'results' not in entry . keys (): continue elif 'chemical_formula_reduced' not in entry [ 'results' ][ 'material' ] . keys (): continue else : extract_values ( entry ) break page_after_value = data [ 'pagination' ][ 'next_page_after_value' ] for entry in data [ 'data' ]: if 'results' not in entry . keys (): continue elif 'chemical_formula_reduced' not in entry [ 'results' ][ 'material' ] . keys (): continue else : extract_values ( entry ) You can verify that the data was fetched correctly by printing some of the values: print ( bandgaps [ 0 ]) print ( len ( bandgaps ))","title":"Retrieve Data using NOMAD API"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_1_perovskites_crabnet_retrieve_data_api.html#save-data","text":"Finally, we'll convert the data into a Pandas DataFrame and save it to a CSV file for future use. import pandas as pd import os # Convert data into a Pandas DataFrame df = pd . DataFrame ({ 'reduced_formulas' : reduced_formulas , 'iupac_formulas' : iupac_formulas , 'descriptive_formulas' : descriptive_formulas , 'bandgap' : bandgaps , 'voc' : vocs , 'jsc' : jscs , 'ff' : ffs , 'pce' : pces , 'device_stack' : device_stack , 'htl' : htl , 'etl' : etl , 'structural_dimensionality' : structural_dimensionality }) # Save to a CSV file if not os . path . exists ( 'data' ): os . makedirs ( 'data' ) df . to_csv ( 'data/perovskite_bandgap_devices.csv' , index = False ) # Convert 'bandgap' to numeric and apply unit conversion df [ 'bandgap' ] = pd . to_numeric ( df [ 'bandgap' ], errors = 'coerce' ) df [ 'bandgap' ] = df [ 'bandgap' ] * 6.24150974e18 # Display the DataFrame df . head () We have saved the the data we obtaine from NOMAD API with the name perovskite_bandgap_devices.csv into the folder data where your notebook is running. Next we will proceed with cleaning this data.","title":"Save Data"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_2_perovskites_crabnet_load_and_clean_data.html","text":"Prepare Data for Modeling \u00b6 In the previous section , we retrieved perovskite solar cell data using the NOMAD API, converted it to a Pandas DataFrame, and saved it as perovskite_bandgap_devices.csv in the data folder. If you already have the data, you can start from this section, where we will import and clean it. EDA and Data Cleaning \u00b6 Exploratory Data Analysis (EDA) is a useful step in understanding and preparing datasets for modeling by summarizing data, checking for anomalies, finding patterns and relationships. Insights from NOMAD GUI \u00b6 The distribution of chemical diversity of the dataset can be explored using the periodic table in the NOMAD solar cell app . The bottom-right corner of each element displays the number of entries (solar cells) that include the element in the absorber. The dataset is notably imbalanced, with the majority of entries being Pb-based, containing C, N, H (hybrid perovskites), and halides like I and Br. Load Data \u00b6 Let's load the data into a Pandas DataFrame, make sure the bandgap column is numeric, and adjust its units from Joules to electronvolts. import pandas as pd df = pd . read_csv ( 'data/perovskite_bandgap_devices.csv' ) df [ 'bandgap' ] = pd . to_numeric ( df [ 'bandgap' ], errors = 'coerce' ) df [ 'bandgap' ] = df [ 'bandgap' ] * 6.24150974e18 df . head () Let's have a look on the statistical summary of the dataset: df . describe () We can see that the dataset contains a diverse range of bandgap values (1.16 to 2.70 eV). Some parameters such as open circuit voltage (voc) and fill factor (ff), include extreme or zero values, which might need attention for data cleaning. Remove NaN s \u00b6 The dataset might include missing values, let's check if there are any: df . isna () . sum () Let's now keep part of the dataframe, where the value for bandgap is not Na . and remove them: df = df [ df [ 'bandgap' ] . notna ()] Group Repeated Formulas \u00b6 Even before checking the dataset, we can already guess that are many repeated formulas and bandgap values in the DataFrame. But let's verify it: df [ 'reduced_formulas' ] . value_counts () . head ( 10 ) The dataset contains many repeated formulas and corresponding bandgap values. To make sure each formula is unique, we will use the groupby_formula function from CrabNet to group entries by their formulas and use the mean values for their bandgap. We make a new final DataFrame for our model, call it df_reduced_formula and adjust colum names etc. Let's rename columns and use the gourpby_formula function from CrabNet: from crabnet.utils.data import groupby_formula # Rename the column 'bandgap' to 'target' df . rename ( columns = { 'bandgap' : 'target' }, inplace = True ) df . rename ( columns = { 'reduced_formulas' : 'formula' }, inplace = True ) # Group repeated formulas and take the mean of the target df_grouped_formula = groupby_formula ( df , how = 'mean' ) df_grouped_formula . head () Let's check the shape of the datafram: df_grouped_formula . shape Check Element Prevalence \u00b6 We will use pymatviz ( link to docs ) for visualizing the element prevalence. Run the following snippet if the package is not installed in the environment yet, otherwise skip it. You can simply try ! pip install pymatviz in case of problems, try it from its developer repo ! pip install git + https : // github . com / janosh / pymatviz Once installed, we import and use it: from pymatviz import ptable_heatmap_plotly ptable_heatmap_plotly ( df_grouped_formula [ 'formula' ], log = True , colorscale = 'BuPu' , font_colors = 'black' , fmt = '.3g' , colorbar = dict ( orientation = \"v\" , title = 'Element Prevalence' )) As you can see the current data is heavily based on hybrid halide perovskites, so we expect the model to perform better when predicting these materials. Let's continue and build the model in the next section!","title":"Clean Data"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_2_perovskites_crabnet_load_and_clean_data.html#prepare-data-for-modeling","text":"In the previous section , we retrieved perovskite solar cell data using the NOMAD API, converted it to a Pandas DataFrame, and saved it as perovskite_bandgap_devices.csv in the data folder. If you already have the data, you can start from this section, where we will import and clean it.","title":"Prepare Data for Modeling"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_2_perovskites_crabnet_load_and_clean_data.html#eda-and-data-cleaning","text":"Exploratory Data Analysis (EDA) is a useful step in understanding and preparing datasets for modeling by summarizing data, checking for anomalies, finding patterns and relationships.","title":"EDA and Data Cleaning"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_2_perovskites_crabnet_load_and_clean_data.html#insights-from-nomad-gui","text":"The distribution of chemical diversity of the dataset can be explored using the periodic table in the NOMAD solar cell app . The bottom-right corner of each element displays the number of entries (solar cells) that include the element in the absorber. The dataset is notably imbalanced, with the majority of entries being Pb-based, containing C, N, H (hybrid perovskites), and halides like I and Br.","title":"Insights from NOMAD GUI"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_2_perovskites_crabnet_load_and_clean_data.html#load-data","text":"Let's load the data into a Pandas DataFrame, make sure the bandgap column is numeric, and adjust its units from Joules to electronvolts. import pandas as pd df = pd . read_csv ( 'data/perovskite_bandgap_devices.csv' ) df [ 'bandgap' ] = pd . to_numeric ( df [ 'bandgap' ], errors = 'coerce' ) df [ 'bandgap' ] = df [ 'bandgap' ] * 6.24150974e18 df . head () Let's have a look on the statistical summary of the dataset: df . describe () We can see that the dataset contains a diverse range of bandgap values (1.16 to 2.70 eV). Some parameters such as open circuit voltage (voc) and fill factor (ff), include extreme or zero values, which might need attention for data cleaning.","title":"Load Data"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_2_perovskites_crabnet_load_and_clean_data.html#remove-nans","text":"The dataset might include missing values, let's check if there are any: df . isna () . sum () Let's now keep part of the dataframe, where the value for bandgap is not Na . and remove them: df = df [ df [ 'bandgap' ] . notna ()]","title":"Remove NaNs"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_2_perovskites_crabnet_load_and_clean_data.html#group-repeated-formulas","text":"Even before checking the dataset, we can already guess that are many repeated formulas and bandgap values in the DataFrame. But let's verify it: df [ 'reduced_formulas' ] . value_counts () . head ( 10 ) The dataset contains many repeated formulas and corresponding bandgap values. To make sure each formula is unique, we will use the groupby_formula function from CrabNet to group entries by their formulas and use the mean values for their bandgap. We make a new final DataFrame for our model, call it df_reduced_formula and adjust colum names etc. Let's rename columns and use the gourpby_formula function from CrabNet: from crabnet.utils.data import groupby_formula # Rename the column 'bandgap' to 'target' df . rename ( columns = { 'bandgap' : 'target' }, inplace = True ) df . rename ( columns = { 'reduced_formulas' : 'formula' }, inplace = True ) # Group repeated formulas and take the mean of the target df_grouped_formula = groupby_formula ( df , how = 'mean' ) df_grouped_formula . head () Let's check the shape of the datafram: df_grouped_formula . shape","title":"Group Repeated Formulas"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_2_perovskites_crabnet_load_and_clean_data.html#check-element-prevalence","text":"We will use pymatviz ( link to docs ) for visualizing the element prevalence. Run the following snippet if the package is not installed in the environment yet, otherwise skip it. You can simply try ! pip install pymatviz in case of problems, try it from its developer repo ! pip install git + https : // github . com / janosh / pymatviz Once installed, we import and use it: from pymatviz import ptable_heatmap_plotly ptable_heatmap_plotly ( df_grouped_formula [ 'formula' ], log = True , colorscale = 'BuPu' , font_colors = 'black' , fmt = '.3g' , colorbar = dict ( orientation = \"v\" , title = 'Element Prevalence' )) As you can see the current data is heavily based on hybrid halide perovskites, so we expect the model to perform better when predicting these materials. Let's continue and build the model in the next section!","title":"Check Element Prevalence"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_3_perovskites_crabnet_ml.html","text":"Build and Train the Model \u00b6 We randomize the dataset and split it into training, validation, and test sets in a ratio of 80%, 10%, and 10%, respectively. import numpy as np train_df , val_df , test_df = np . split ( df_grouped_formula . sample ( frac = 1 , random_state = 42 ), [ int ( 0.8 * len ( df_grouped_formula )), int ( 0.9 * len ( df_grouped_formula )) ] ) We then fit the model using the CrabNet implementation. from crabnet.crabnet_ import CrabNet crabnet_bandgap = CrabNet ( mat_prop = \"bandgap\" , model_name = 'perovskite_bg_prediction' , elem_prop = 'mat2vec' , learningcurve = True ) crabnet_bandgap . fit ( train_df , val_df ) Evaluate the Model \u00b6 After training, we evaluate the model using the validation data. from crabnet.utils.figures import act_pred from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_error # Train data train_df_zeros = pd . DataFrame (({ \"formula\" : train_df [ 'formula' ], \"target\" : [ 0.0 ] * len ( train_df [ 'formula' ])})) train_df_predicted , train_df_predicted_sigma = crabnet_bandgap . predict ( train_df_zeros , return_uncertainty = True ) act_pred ( train_df [ 'target' ], train_df_predicted ) r2 = r2_score ( train_df [ 'target' ], train_df_predicted ) print ( f 'R2 score: { r2 } ' ) mse = mean_squared_error ( train_df [ 'target' ], train_df_predicted ) print ( f 'MSE: { mse } ' ) mae = mean_absolute_error ( train_df [ 'target' ], train_df_predicted ) print ( f 'MAE: { mae } eV' ) for validation data we have: # Validation data val_df_zeros = pd . DataFrame (({ \"formula\" : val_df [ 'formula' ], \"target\" : [ 0.0 ] * len ( val_df [ 'formula' ])})) val_df_predicted , val_df_predicted_sigma = crabnet_bandgap . predict ( val_df_zeros , return_uncertainty = True ) act_pred ( val_df [ 'target' ], val_df_predicted ) r2 = r2_score ( val_df [ 'target' ], val_df_predicted ) print ( f 'R2 score: { r2 } ' ) mse = mean_squared_error ( val_df [ 'target' ], val_df_predicted ) print ( f 'MSE: { mse } ' ) mae = mean_absolute_error ( val_df [ 'target' ], val_df_predicted ) print ( f 'MAE: { mae } eV' ) and finally for test data: # Test data test_df_zeros = pd . DataFrame (({ \"formula\" : test_df [ 'formula' ], \"target\" : [ 0.0 ] * len ( test_df [ 'formula' ])})) test_df_predicted , test_df_predicted_sigma = crabnet_bandgap . predict ( test_df_zeros , return_uncertainty = True ) act_pred ( test_df [ 'target' ], test_df_predicted ) r2 = r2_score ( test_df [ 'target' ], test_df_predicted ) print ( f 'R2 score: { r2 } ' ) mse = mean_squared_error ( test_df [ 'target' ], test_df_predicted ) print ( f 'MSE: { mse } ' ) mae = mean_absolute_error ( test_df [ 'target' ], test_df_predicted ) print ( f 'MAE: { mae } eV' ) Use the Model for Prediction from Individual Formulas \u00b6 Now we are going to run some predictions. We will start loading the model just in case you want to start direcly here in a new session. from crabnet.kingcrab import SubCrab from crabnet.crabnet_ import CrabNet import pandas as pd # only if you jump to this cell directly import numpy as np # Instantiate SubCrab sub_crab_model = SubCrab () # Instantiate CrabNet and set its model to SubCrab crabnet_model = CrabNet () crabnet_model . model = sub_crab_model # Load the pre-trained network file_path = r 'perovskite_bg_prediction.pth' crabnet_model . load_network ( file_path ) Then define a function and run it for predicting the bandgap from individual formulas: # Function to predict the bandgap of a given formula def predict_bandgap ( formula ): input_df = pd . DataFrame ({ \"formula\" : [ formula ], \"target\" : [ 0.0 ]}) prediction , prediction_sigma = crabnet_bandgap . predict ( input_df , return_uncertainty = True ) return prediction , prediction_sigma # Main script to take user input and display predictions while True : formula = input ( \"Enter a formula (e.g., CsPbBr3, CH3NH3PbI3) or type 'exit' to quit: \" ) if formula . lower () == 'exit' : print ( \"Exiting prediction tool. Goodbye!\" ) break try : prediction , prediction_sigma = predict_bandgap ( formula ) print ( f \"Predicted bandgap: { np . round ( prediction [ 0 ], 3 ) } +/- { np . round ( prediction_sigma [ 0 ], 3 ) } eV\" ) except Exception as e : print ( f \"Error during prediction: { e } \" ) Bonus: Interactive Bandgap Predictor with Dataset Check This interactive widget allows you to input a chemical formula, predict its bandgap using the trained model, and check if the formula exists in the dataset. If it does, the widget displays the average bandgap value used during training. import pandas as pd import numpy as np from ipywidgets import Text , Button , VBox , HBox , Output from IPython.display import display # Function to predict the bandgap of a given formula def predict_bandgap ( formula ): val_df = pd . DataFrame ({ \"formula\" : [ formula ], \"target\" : [ 0.0 ]}) prediction , prediction_sigma = crabnet_bandgap . predict ( val_df , return_uncertainty = True ) return prediction , prediction_sigma # Function to check if the formula exists in the dataset def check_formula_in_dataset ( formula ): if formula in df_grouped_formula [ 'formula' ] . values : avg_bandgap = df_grouped_formula . loc [ df_grouped_formula [ 'formula' ] == formula , 'target' ] . values [ 0 ] return avg_bandgap else : return None # Setting up the widget interface formula_input = Text ( value = '' , placeholder = 'Enter formula (e.g., CsPbBr3)' , description = 'Formula:' , ) predict_button = Button ( description = 'Predict Bandgap' , button_style = 'success' ) output = Output () def on_click ( b ): with output : output . clear_output () try : formula = formula_input . value . strip () if not formula : print ( \"Please enter a valid chemical formula.\" ) return # Prediction prediction , sigma = predict_bandgap ( formula ) print ( f \"Predicted Bandgap: { np . round ( prediction [ 0 ], 3 ) } \u00b1 { np . round ( sigma [ 0 ], 3 ) } eV\" ) # Dataset check avg_bandgap = check_formula_in_dataset ( formula ) if avg_bandgap is not None : print ( f \"The averaged literature bandgap for { formula } is { avg_bandgap : .3f } eV (from dataset).\" ) else : print ( f \"The formula ' { formula } ' is not contained in the dataset.\" ) except Exception as e : print ( f \"Error: { e } \" ) predict_button . on_click ( on_click ) display ( VBox ([ HBox ([ formula_input , predict_button ]), output ]))","title":"Bandgap Prediction"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_3_perovskites_crabnet_ml.html#build-and-train-the-model","text":"We randomize the dataset and split it into training, validation, and test sets in a ratio of 80%, 10%, and 10%, respectively. import numpy as np train_df , val_df , test_df = np . split ( df_grouped_formula . sample ( frac = 1 , random_state = 42 ), [ int ( 0.8 * len ( df_grouped_formula )), int ( 0.9 * len ( df_grouped_formula )) ] ) We then fit the model using the CrabNet implementation. from crabnet.crabnet_ import CrabNet crabnet_bandgap = CrabNet ( mat_prop = \"bandgap\" , model_name = 'perovskite_bg_prediction' , elem_prop = 'mat2vec' , learningcurve = True ) crabnet_bandgap . fit ( train_df , val_df )","title":"Build and Train the Model"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_3_perovskites_crabnet_ml.html#evaluate-the-model","text":"After training, we evaluate the model using the validation data. from crabnet.utils.figures import act_pred from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_error # Train data train_df_zeros = pd . DataFrame (({ \"formula\" : train_df [ 'formula' ], \"target\" : [ 0.0 ] * len ( train_df [ 'formula' ])})) train_df_predicted , train_df_predicted_sigma = crabnet_bandgap . predict ( train_df_zeros , return_uncertainty = True ) act_pred ( train_df [ 'target' ], train_df_predicted ) r2 = r2_score ( train_df [ 'target' ], train_df_predicted ) print ( f 'R2 score: { r2 } ' ) mse = mean_squared_error ( train_df [ 'target' ], train_df_predicted ) print ( f 'MSE: { mse } ' ) mae = mean_absolute_error ( train_df [ 'target' ], train_df_predicted ) print ( f 'MAE: { mae } eV' ) for validation data we have: # Validation data val_df_zeros = pd . DataFrame (({ \"formula\" : val_df [ 'formula' ], \"target\" : [ 0.0 ] * len ( val_df [ 'formula' ])})) val_df_predicted , val_df_predicted_sigma = crabnet_bandgap . predict ( val_df_zeros , return_uncertainty = True ) act_pred ( val_df [ 'target' ], val_df_predicted ) r2 = r2_score ( val_df [ 'target' ], val_df_predicted ) print ( f 'R2 score: { r2 } ' ) mse = mean_squared_error ( val_df [ 'target' ], val_df_predicted ) print ( f 'MSE: { mse } ' ) mae = mean_absolute_error ( val_df [ 'target' ], val_df_predicted ) print ( f 'MAE: { mae } eV' ) and finally for test data: # Test data test_df_zeros = pd . DataFrame (({ \"formula\" : test_df [ 'formula' ], \"target\" : [ 0.0 ] * len ( test_df [ 'formula' ])})) test_df_predicted , test_df_predicted_sigma = crabnet_bandgap . predict ( test_df_zeros , return_uncertainty = True ) act_pred ( test_df [ 'target' ], test_df_predicted ) r2 = r2_score ( test_df [ 'target' ], test_df_predicted ) print ( f 'R2 score: { r2 } ' ) mse = mean_squared_error ( test_df [ 'target' ], test_df_predicted ) print ( f 'MSE: { mse } ' ) mae = mean_absolute_error ( test_df [ 'target' ], test_df_predicted ) print ( f 'MAE: { mae } eV' )","title":"Evaluate the Model"},{"location":"Module4/M4_x_examples/M4_x_1_perovskites_crabnet_ml/M4_x_3_perovskites_crabnet_ml.html#use-the-model-for-prediction-from-individual-formulas","text":"Now we are going to run some predictions. We will start loading the model just in case you want to start direcly here in a new session. from crabnet.kingcrab import SubCrab from crabnet.crabnet_ import CrabNet import pandas as pd # only if you jump to this cell directly import numpy as np # Instantiate SubCrab sub_crab_model = SubCrab () # Instantiate CrabNet and set its model to SubCrab crabnet_model = CrabNet () crabnet_model . model = sub_crab_model # Load the pre-trained network file_path = r 'perovskite_bg_prediction.pth' crabnet_model . load_network ( file_path ) Then define a function and run it for predicting the bandgap from individual formulas: # Function to predict the bandgap of a given formula def predict_bandgap ( formula ): input_df = pd . DataFrame ({ \"formula\" : [ formula ], \"target\" : [ 0.0 ]}) prediction , prediction_sigma = crabnet_bandgap . predict ( input_df , return_uncertainty = True ) return prediction , prediction_sigma # Main script to take user input and display predictions while True : formula = input ( \"Enter a formula (e.g., CsPbBr3, CH3NH3PbI3) or type 'exit' to quit: \" ) if formula . lower () == 'exit' : print ( \"Exiting prediction tool. Goodbye!\" ) break try : prediction , prediction_sigma = predict_bandgap ( formula ) print ( f \"Predicted bandgap: { np . round ( prediction [ 0 ], 3 ) } +/- { np . round ( prediction_sigma [ 0 ], 3 ) } eV\" ) except Exception as e : print ( f \"Error during prediction: { e } \" ) Bonus: Interactive Bandgap Predictor with Dataset Check This interactive widget allows you to input a chemical formula, predict its bandgap using the trained model, and check if the formula exists in the dataset. If it does, the widget displays the average bandgap value used during training. import pandas as pd import numpy as np from ipywidgets import Text , Button , VBox , HBox , Output from IPython.display import display # Function to predict the bandgap of a given formula def predict_bandgap ( formula ): val_df = pd . DataFrame ({ \"formula\" : [ formula ], \"target\" : [ 0.0 ]}) prediction , prediction_sigma = crabnet_bandgap . predict ( val_df , return_uncertainty = True ) return prediction , prediction_sigma # Function to check if the formula exists in the dataset def check_formula_in_dataset ( formula ): if formula in df_grouped_formula [ 'formula' ] . values : avg_bandgap = df_grouped_formula . loc [ df_grouped_formula [ 'formula' ] == formula , 'target' ] . values [ 0 ] return avg_bandgap else : return None # Setting up the widget interface formula_input = Text ( value = '' , placeholder = 'Enter formula (e.g., CsPbBr3)' , description = 'Formula:' , ) predict_button = Button ( description = 'Predict Bandgap' , button_style = 'success' ) output = Output () def on_click ( b ): with output : output . clear_output () try : formula = formula_input . value . strip () if not formula : print ( \"Please enter a valid chemical formula.\" ) return # Prediction prediction , sigma = predict_bandgap ( formula ) print ( f \"Predicted Bandgap: { np . round ( prediction [ 0 ], 3 ) } \u00b1 { np . round ( sigma [ 0 ], 3 ) } eV\" ) # Dataset check avg_bandgap = check_formula_in_dataset ( formula ) if avg_bandgap is not None : print ( f \"The averaged literature bandgap for { formula } is { avg_bandgap : .3f } eV (from dataset).\" ) else : print ( f \"The formula ' { formula } ' is not contained in the dataset.\" ) except Exception as e : print ( f \"Error: { e } \" ) predict_button . on_click ( on_click ) display ( VBox ([ HBox ([ formula_input , predict_button ]), output ]))","title":"Use the Model for Prediction from Individual Formulas"}]}